{"config":{"lang":["en"],"separator":"[\\s\\u200b\\-_,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"CANFAR Deployments","text":"<p>Welcome to the operational documentation for deploying and maintaining the CANFAR Science Platform. This repository contains Helm charts, Kubernetes configurations, CI/CD automation, and operations runbooks that power the platform infrastructure.</p>"},{"location":"#what-youll-find-here","title":"What You'll Find Here","text":"<p>This documentation is designed for platform operators, DevOps engineers, and infrastructure maintainers who deploy, configure, and manage CANFAR services.</p> <ul> <li> <p> Helm Charts</p> <p>Reusable deployment templates for CANFAR services with configurable values and environment overlays.</p> </li> <li> <p> Release Automation</p> <p>Automated CI/CD pipelines using GitHub Actions and Release Please for Helm chart versioning.</p> </li> <li> <p> Operations Runbooks</p> <p>Step-by-step procedures for releases, rollbacks, monitoring, and troubleshooting production deployments.</p> </li> <li> <p> Configuration Management</p> <p>Environment-specific overlays, secrets management, and Kubernetes resource definitions for staging and production.</p> </li> <li> <p> Documentation</p> <p>Automated MkDocs site deployment with operations guides and platform runbooks.</p> </li> <li> <p> Code Quality</p> <p>Pre-commit hooks, linting, and security scanning for infrastructure-as-code.</p> </li> </ul>"},{"location":"#quick-links","title":"Quick Links","text":"<ul> <li>CI/CD Pipelines - Understand GitHub Actions workflows for documentation and code quality</li> <li>Release Process - Follow the Helm chart release process and CANFAR platform schedule</li> <li>GitHub Repository - Browse source code, Helm charts, and configurations</li> </ul>"},{"location":"#getting-started","title":"Getting Started","text":"<p>If you're new to CANFAR deployments:</p> <ol> <li>Review the Release Process to understand our deployment workflow</li> <li>Explore the CI/CD Pipelines documentation to see how automation works</li> <li>Check the GitHub repository for Helm charts and configuration files</li> </ol>"},{"location":"#contributing","title":"Contributing","text":"<p>This is an operational repository for CANFAR platform infrastructure. Changes follow the standard pull request workflow with Release Please automation for versioning and changelog generation.</p> <p>For questions or support, contact the CADC operations team or visit the main CANFAR documentation.</p>"},{"location":"helm/base/","title":"Helm Chart for base objects of the CANFAR Science Platform","text":""},{"location":"helm/base/#install","title":"Install","text":""},{"location":"helm/base/#dependencies","title":"Dependencies","text":"<p>Kubernetes 1.27 and up are supported.</p>"},{"location":"helm/base/#from-source","title":"From source","text":"<p>Installation depends on a working Kubernetes cluster version 1.23 or greater.</p> <p>The base install also installs the Traefik proxy, which is needed by the Ingress when the Science Platform services are installed.</p> <pre><code>$ git clone https://github.com/opencadc/science-platform.git\n$ cd science-platform/deployment/helm\n$ helm install --dependency-update --values ./base/values.yaml &lt;name&gt; ./base\n</code></pre> <p>Where <code>&lt;name&gt;</code> is the name of this installation.  Example: <pre><code>$ helm install --dependency-update --values ./base/values.yaml base ./base\n</code></pre> This will create the core namespace (<code>skaha-system</code>), and install the Traefik proxy dependency.  Expected output: <pre><code>NAME: base\nLAST DEPLOYED: &lt;Timestamp e.g. Fri Jun 30 10:39:04 2023&gt;\nNAMESPACE: skaha-system\nSTATUS: deployed\nREVISION: 1\nTEST SUITE: None\n</code></pre></p>"},{"location":"helm/base/#from-the-canfar-repository","title":"From the CANFAR repository","text":"<p>The Helm repository contains the current stable version as well.</p> <pre><code>$ helm repo add canfar-skaha-system https://images.canfar.net/chartrepo/skaha-system\n$ helm repo update\n$ helm install --dependency-update --values canfar-skaha-system/base/values.yaml canfar-science-platform-base canfar-skaha-system/base\n</code></pre>"},{"location":"helm/base/#verification","title":"Verification","text":"<p>After the install, there should exist the necessary Namespaces and Objects.  See the Namespaces:</p> <pre><code>$ kubectl get namespaces\nNAME                   STATUS   AGE\n...\nskaha-system           Active   28m\nskaha-workload         Active   28m\n</code></pre>"},{"location":"helm/base/#proxy-using-traefik","title":"Proxy using Traefik","text":"<p>The Traefik proxy server is also installed as a dependency, which handles SSL termination.  Helm options are under the <code>traefik</code> key in the <code>values.yaml</code> file.</p> <p>You can create your own secrets to contain your self-signed server certificates to be used by the SSL termination.  See the <code>values.yaml</code> file for more, and don't forget to <code>base64</code> encode the values.</p>"},{"location":"helm/base/#shared-storage","title":"Shared Storage","text":"<p>Shared Storage is handled by the <code>local</code> Persistent Volume types.</p> <pre><code>...\n  volumeMode: Filesystem\n  accessModes:\n    - ReadWriteMany\n  persistentVolumeReclaimPolicy: Delete\n  storageClassName: local-storage\n  local:\n    path: /data/skaha-storage\n...\n</code></pre>"},{"location":"helm/base/#dns-on-macos","title":"DNS on macOS","text":"<p>The Docker VM on macOS cannot mount the NFS by default as it cannot do name resolution in the cluster.  It first needs to know about the <code>kube-dns</code> IP.  e.g.:</p> <pre><code>$ kubectl -n kube-system get service kube-dns\nNAME       TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)                  AGE\nkube-dns   ClusterIP   10.96.0.10   &lt;none&gt;        53/UDP,53/TCP,9153/TCP   4d23h\n</code></pre> <p>The <code>ClusterIP</code> needs to be known to the Docker VM's name resolution.  A simple way to do this is to mount the Docker VM root and modify it.  It will take effect immediately:</p> <pre><code>$ docker run --rm -it -v /:/vm-root alpine sh\n$ echo \"nameserver 10.96.0.10\" &gt;&gt; /vm-root/etc/resolv.conf\n$ cat /vm-root/etc/resolv.conf\n# DNS requests are forwarded to the host. DHCP DNS options are ignored.\nnameserver 192.168.65.7\nnameserver 10.96.0.10\n</code></pre>"},{"location":"helm/canfar/","title":"Deployment Guide","text":"<ul> <li>Dependencies</li> <li>Quick Start</li> <li>Base install</li> <li>Persistent Volumes</li> <li>POSIX Mapper install</li> <li>Kueue install</li> <li>Skaha install</li> <li>Science Portal install</li> <li>Cavern install</li> <li>Storage User Interface install</li> <li>Helm</li> <li>Obtaining a bearer token</li> <li>Flow</li> <li>Structure</li> </ul>"},{"location":"helm/canfar/#dependencies","title":"Dependencies","text":"<ul> <li>An existing Kubernetes cluster.</li> <li>An Service Registry deployment</li> </ul>"},{"location":"helm/canfar/#quick-start","title":"Quick Start","text":"<pre><code>helm repo add science-platform https://images.opencadc.org/chartrepo/platform\nhelm repo add science-platform-client https://images.opencadc.org/chartrepo/client\nhelm repo update\n\nhelm install --values my-base-local-values-file.yaml base science-platform/base\nhelm install -n skaha-system --values my-posix-mapper-local-values-file.yaml posixmapper science-platform/posixmapper\nhelm install -n skaha-system --values my-skaha-local-values-file.yaml skaha science-platform/skaha\nhelm install -n skaha-system --values my-scienceportal-local-values-file.yaml scienceportal science-platform/scienceportal\nhelm install -n skaha-system --values my-cavern-local-values-file.yaml cavern science-platform/cavern\nhelm install -n skaha-system --values my-storage-ui-local-values-file.yaml storage-ui science-platform-client/storageui\n</code></pre>"},{"location":"helm/canfar/#helm","title":"Helm","text":"<p>Add the Helm repository:</p> <pre><code>helm repo add science-platform https://images.opencadc.org/chartrepo/platform\nhelm repo update\n</code></pre>"},{"location":"helm/canfar/#base-install","title":"Base install","text":"<p>The Base install will create ServiceAccount, Role, Namespace, and RBAC objects needed to place the Skaha service.</p> <p>Create a <code>my-base-local-values-file.yaml</code> file to override Values from the main template <code>values.yaml</code> file.  Mainly the Traefik Default Server certificate (optional if needed):</p> <p><code>my-base-local-values-file.yaml</code> <pre><code>secrets:\n    default-certificate:\n        tls.crt: &lt;base64 encoded server certificate&gt;\n        tls.key: &lt;base64 encoded server key&gt;\n\n# Settings passed to Traefik.  The install flag is used by Helm to proceed to install Traefik or not.  If false, ensure v2.11.0 is at minimum installed.\ntraefik:\n  install: true\n  tlsStore:\n    default:\n      defaultCertificate:\n        # See default-certificate secret(s) above\n        secretName: default-certificate\n</code></pre></p> <pre><code>helm install --values my-base-local-values-file.yaml base science-platform/base\n\nNAME: base\nLAST DEPLOYED: Thu Sep 14 07:28:45 2025\nNAMESPACE: default\nSTATUS: deployed\nREVISION: 1\n</code></pre>"},{"location":"helm/canfar/#persistent-volumes-and-persistent-volume-claims","title":"Persistent Volumes and Persistent Volume Claims","text":"<p>Note The <code>base</code> MUST be installed first as it creates the necessary Namespaces for the Persistent Volume Claims!</p> <p>Important There are two (2) Persistent Volume Claims that are used in the system, due to the fact that there are two (2) Namespaces (<code>skaha-system</code> and <code>skaha-workload</code>).  These PVCs, while having potentially different configurations, SHOULD point to the same storage.  For example, if two <code>hostPath</code> PVCs are created, the <code>hostPath.path</code> MUST point to the same folder in order to have shared content between the Cavern Service (<code>cavern</code>) and the User Sessions (Notebooks, CARTA, etc.).</p> <p>It is expected that the deployer, or an Administrator, will create the necessary Persistent Volumes (if needed), and the required Persistent Volume Claims at this point.  There are sample Local Storage Persistent Volume examples in the <code>base/volumes</code> folder.</p> <p>Two (2) Persistent Volume Claims are required.  While both point to the same underlying storage, they are in different Namespaces.  This leads to somewhat duplicated effort, but it is necessary to ensure that both the <code>skaha-system</code> and <code>skaha-workload</code> namespaces have access to the required storage resources. See this short explanation for more information.</p>"},{"location":"helm/canfar/#posix-mapper-install","title":"POSIX Mapper install","text":"<p>The POSIX Mapper Service is required to provide a UID to Username mapping, and a GID to Group Name mapping so that any Terminal access properly showed System Users in User Sessions.  It will generate UIDs when a user is requested, or a GID when a Group is requested, and then keep track of them.</p> <p>This service is required to be installed before the Skaha service.</p> <p>Create a <code>my-posix-mapper-local-values-file.yaml</code> file to override Values from the main template <code>values.yaml</code> file.</p> <p><code>my-posix-mapper-local-values-file.yaml</code> <pre><code># POSIX Mapper web service deployment\ndeployment:\n  hostname: example.org\n  posixMapper:\n    # Optionally set the DEBUG port.\n    # extraEnv:\n    # - name: CATALINA_OPTS\n    #   value: \"-agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=0.0.0.0:5555\"\n    # - name: JAVA_OPTS\n    #   value: \"-agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=0.0.0.0:5555\"\n\n    # Uncomment to debug.  Requires options above as well as service port exposure below.\n    # extraPorts:\n    # - containerPort: 5555\n    #   protocol: TCP\n\n    # Resources provided to the Skaha service.\n    resources:\n      requests:\n        memory: \"1Gi\"\n        cpu: \"500m\"\n      limits:\n        memory: \"1Gi\"\n        cpu: \"500m\"\n\n    # Used to set the minimum UID.  Useful to avoid conflicts.\n    minUID: 10000\n\n    # Used to set the minimum GID.  Keep this much higher than the minUID so that default Groups can be set for new users.\n    minGID: 900000\n\n    # The URL of the IVOA Registry\n    registryURL: https://example.org/reg\n\n    # Optionally mount a custom CA certificate\n    # extraVolumeMounts:\n    # - mountPath: \"/config/cacerts\"\n    #   name: cacert-volume\n\n    # Create the CA certificate volume to be mounted in extraVolumeMounts\n    # extraVolumes:\n    # - name: cacert-volume\n    #   secret:\n    #     defaultMode: 420\n    #     secretName: posix-manager-cacert-secret\n\n  # Specify extra hostnames that will be added to the Pod's /etc/hosts file.  Note that this is in the\n  # deployment object, not the posixMapper one.\n  #\n  # These entries get added as hostAliases entries to the Deployment.\n  #\n  # Example:\n  # extraHosts:\n  #   - ip: 127.3.34.5\n  #     hostname: myhost.example.org\n  #\n  # extraHosts: []\n\n# Declare the storage for the skaha service to use.\nstorage:\n  service:\n    spec:\n      persistentVolumeClaim:\n        claimName: skaha-pvc # Match this label up with whatever was installed in the base install, or the desired PVC, or create dynamically provisioned storage.\n\nsecrets:\n  # Uncomment to enable local or self-signed CA certificates for your domain to be trusted.\n#   posix-manager-cacert-secret:\n#     ca.crt: &lt;base64 encoded ca crt&gt;\n\n# These values are preset in the catalina.properties, and this default database only exists beside this service.\n# It's usually safe to leave these as-is.\n# postgresql:\n#   maxActive: 4\n#   schema: mapping\n#   url: jdbc:postgresql://db.host:5432/mapping\n#   auth:\n#     username: posixmapperuser\n#     password: posixmapperpwd\n</code></pre></p> <pre><code>helm install -n skaha-system  --values my-posix-mapper-local-values-file.yaml posixmapper science-platform/posixmapper\n\nNAME: posixmapper\nLAST DEPLOYED: Thu Sep 28 07:28:45 2023\nNAMESPACE: skaha-system\nSTATUS: deployed\nREVISION: 1\n</code></pre> <p>Test it. <pre><code># See below for tokens\nexport SKA_TOKEN=...\ncurl -SsL --header \"Authorization: Bearer ${SKA_TOKEN}\" https://example.host.com/posix-mapper/uid\n[]%\n\ncurl -SsL --header \"Authorization: Bearer ${SKA_TOKEN}\" \"https://example.host.com/posix-mapper/uid?user=mynewuser\"\nmynewuser:x:1000:1000:::\n</code></pre></p>"},{"location":"helm/canfar/#kueue-install","title":"Kueue install","text":"<p>Kueue is a Kubernetes-native job queuing and scheduling system that enhances the management of workloads in a Kubernetes cluster. It provides advanced features for job scheduling, resource management, and workload prioritization, making it particularly useful for environments where efficient resource utilization and job handling are critical. Kueue is optional, but highly recommended for production deployments.</p> <p>See https://kueue.sigs.k8s.io/docs/installation/#install-a-released-version for details.</p>"},{"location":"helm/canfar/#helm_1","title":"Helm","text":"<p>Install CRDs and the various components: <pre><code>helm -n kueue-system install kueue oci://registry.k8s.io/kueue/charts/kueue --create-namespace\n</code></pre></p>"},{"location":"helm/canfar/#clusterqueue","title":"ClusterQueue","text":"<p>A <code>ClusterQueue</code> is a global resource in Kueue that defines a set of resources and policies for managing workloads across the entire Kubernetes cluster. It serves as a template for creating and managing <code>LocalQueues</code>, which are specific to namespaces.</p> <p>An example <code>ClusterQueue</code> has been added to the Skaha Example Kueue ClusterQueue file.  Adjust as needed.</p> <p>Create the <code>ClusterQueue</code>, with the associated <code>ResourceFlavor</code> and <code>WorkloadPriorityClass</code> objects: <pre><code>cp ../skaha/kueue/examples/clusterQueue.config.yaml ./\n# Edit as needed.\n\nkubectl apply -f ./clusterQueue.config.yaml\n</code></pre></p>"},{"location":"helm/canfar/#localqueue","title":"LocalQueue","text":"<p>A <code>LocalQueue</code> is a namespace-specific resource in Kueue that allows for the management of workloads within a particular Kubernetes namespace. It provides a way to define how jobs are queued, prioritized, and scheduled based on the policies set in the associated <code>ClusterQueue</code>.</p> <p>An example <code>LocalQueue</code> has been added to the Skaha Example Kueue LocalQueue file.  Adjust as needed, but it needs to exist in the workload namespace (<code>skaha-workload</code> by default).</p> <p>Create the <code>LocalQueue</code> in the <code>skaha-workload</code> namespace: <pre><code>cp ../skaha/kueue/examples/localQueue.config.yaml ./\n# Edit as needed.\n\nkubectl apply -f ./localQueue.config.yaml\n</code></pre></p>"},{"location":"helm/canfar/#rbac","title":"RBAC","text":"<p>The Science Platform (<code>skaha</code>) requires certain RBAC permissions to operate correctly within the Kubernetes cluster. These permissions allow Kueue to manage resources, schedule jobs, and interact with other components in the cluster.</p> <p>An example <code>RBAC</code> configuration has been added to the Skaha Example Kueue RBAC file.  Adjust as needed. The example files contains rules to allow both the <code>skaha</code> system to query for the existence of any configured LocalQueues to ensure integrity, as well as permitting Jobs to be scheduled into it from the Workload Namespace.</p> <p>Create the <code>RBAC</code> objects in the <code>skaha-system</code> namespace: <pre><code>cp ../skaha/kueue/examples/rbac.yaml ./\nkubectl apply -f ./rbac.yaml\n</code></pre></p>"},{"location":"helm/canfar/#skaha-install","title":"Skaha install","text":"<p>The Skaha service will manage User Sessions.  It relies on the POSIX Mapper being deployed, and available to be found via the IVOA Registry:</p> <p><code>/reg/resource-caps</code> <pre><code>...\n# Ensure the hostname matches the deployment hostname.\nivo://example.org/posix-mapper = https://example.host.com/posix-mapper/capabilities\n...\n</code></pre></p> <p>Create a <code>my-skaha-local-values-file.yaml</code> file to override Values from the main template <code>values.yaml</code> file.</p> <p><code>my-skaha-local-values-file.yaml</code> <pre><code># Skaha web service deployment\ndeployment:\n  hostname: example.org\n  skaha:\n    # Optionally set the DEBUG port.\n    # extraEnv:\n    # - name: CATALINA_OPTS\n    #   value: \"-agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=0.0.0.0:5555\"\n    # - name: JAVA_OPTS\n    #   value: \"-agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=0.0.0.0:5555\"\n\n    # Uncomment to debug.  Requires options above as well as service port exposure below.\n    # extraPorts:\n    # - containerPort: 5555\n    #   protocol: TCP\n\n    # Set the top-level-directory name that gets mounted at the root.\n    # skahaTld: \"/cavern\"\n\n    defaultQuotaGB: \"10\"\n\n    # Space delimited list of allowed Image Registry hosts.  These hosts should match the hosts in the User Session images.\n    registryHosts: \"images.canfar.net\"\n\n    # The IVOA GMS Group URI to verify users against for permission to use the Science Platform.\n    # See https://www.ivoa.net/documents/GMS/20220222/REC-GMS-1.0.html#tth_sEc3.2\n    usersGroup: \"ivo://example.org/gms?skaha-platform-users\"\n\n    # The IVOA GMS Group URI to verify images without contacting Harbor.\n    # See https://www.ivoa.net/documents/GMS/20220222/REC-GMS-1.0.html#tth_sEc3.2\n    adminsGroup: \"ivo://example.org/gms?skaha-admin-users\"\n\n    # The IVOA GMS Group URI to verify users against for permission to run headless jobs.\n    # See https://www.ivoa.net/documents/GMS/20220222/REC-GMS-1.0.html#tth_sEc3.2\n    headlessGroup: \"ivo://example.org/gms?skaha-headless-users\"\n\n    # The IVOA GMS Group URI to verify users against that have priority for their headless jobs.\n    # See https://www.ivoa.net/documents/GMS/20220222/REC-GMS-1.0.html#tth_sEc3.2\n    headlessPriorityGroup: \"ivo://example.org/gms?skaha-priority-headless-users\"\n\n    # Class name to set for priority headless jobs.\n    headlessPriorityClass: \"uber-user-vip\"\n\n    # Array of GMS Group URIs allowed to set the logging level.  If none set, then nobody can change the log level.\n    # See https://www.ivoa.net/documents/GMS/20220222/REC-GMS-1.0.html#tth_sEc3.2 for GMS Group URIs\n    # See https://github.com/opencadc/core/tree/main/cadc-log for Logging control\n    loggingGroups:\n      - \"ivo://example.org/gms?skaha-logging-admin-users\"\n\n    # The Resource ID (URI) of the Service that contains the Posix Mapping information\n    posixMapperResourceID: \"ivo://example.org/posix-mapper\"\n\n    # URI or URL of the OIDC (IAM) server.  Used to validate incoming tokens.\n    oidcURI: https://iam.example.org/\n\n    # The Resource ID (URI) of the GMS Service.\n    gmsID: ivo://example.org/gms\n\n    # The absolute URL of the IVOA Registry where services are registered\n    registryURL: https://example.org/reg\n\n    # Optionally describe how this Pod will be scheduled using the nodeAffinity clause. This applies to Skaha itself.\n    # Note the different indentation level compared to the sessions.nodeAffinity.\n    # See https://kubernetes.io/docs/tasks/configure-pod-container/assign-pods-nodes-using-node-affinity/\n    # See the [Sample Skaha Values file](skaha/sample-local-values.yaml).\n    # Example:\n    nodeAffinity:\n      # Only allow Skaha to run on specific Nodes.\n      requiredDuringSchedulingIgnoredDuringExecution:\n        nodeSelectorTerms:\n        - matchExpressions:\n          - key: kubernetes.io/hostname\n            operator: In\n            values:\n            - my-special-node-host\n\n    # Settings for User Sessions.  Sensible defaults supplied, but can be overridden.\n    # For units of storage, see https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#meaning-of-memory.\n    sessions:\n      expirySeconds: \"345600\"   # Duration, in seconds, until they expire and are shut down.\n      maxCount: \"3\"  # Max number of sessions per user.\n      minEphemeralStorage: \"20Gi\"   # The initial requested amount of ephemeral (local) storage.  Does NOT apply to Desktop sessions.\n      maxEphemeralStorage: \"200Gi\"  # The maximum amount of ephemeral (local) storage to allow a Session to extend to.  Does NOT apply to Desktop sessions.\n\n      # Optionally setup a separate host for User Sessions for Skaha to redirect to.  The HTTPS scheme is assumed.  Defaults to the Skaha hostname (.Values.deployment.hostname).\n      # Example:\n      #   hostname: myhost.example.org\n      hostname: sessions.example.org\n\n      # When set to 'true' this flag will enable GPU node scheduling.  Don't forget to declare any related GPU configurations, if appropriate, in the nodeAffinity below!\n      # gpuEnabled: false\n\n      # Set the YAML that will go into the \"affinity.nodeAffinity\" stanza for Pod Spec in User Sessions.  This can be used to enable GPU scheduling, for example,\n      # or to control how and where User Session Pods are scheduled.  This can be potentially dangerous unless you know what you are doing.\n      # See https://kubernetes.io/docs/tasks/configure-pod-container/assign-pods-nodes-using-node-affinity\n      # nodeAffinity: {}\n\n      # Mount CVMFS from the Node's mounted path into all User Sessions.\n      extraVolumes:\n      - name: cvmfs-mount\n        volume:\n          type: HOST_PATH     # HOST_PATH is for host path\n          hostPath: \"/cvmfs\"  # Path on the Node to look for a source folder\n          hostPathType: Directory\n        volumeMount:\n          mountPath: \"/cvmfs\"   # Path to mount on the User Sesssion Pod.\n          readOnly: false\n          mountPropagation: HostToContainer\n\n      # Kueue configurations for User Sessions\n      kueue:\n        default:\n          # Ensure this name matches whatever was created as the LocalQueue in the workload namespace.\n          queueName: canfar-science-platform-local-queue\n          priorityClass: low\n\n    # Optionally mount a custom CA certificate as an extra mount in Skaha (*not* user sessions)\n    # extraVolumeMounts:\n    # - mountPath: \"/config/cacerts\"\n    #   name: cacert-volume\n\n    # Create the CA certificate volume to be mounted in extraVolumeMounts\n    # extraVolumes:\n    # - name: cacert-volume\n    #   secret:\n    #     defaultMode: 420\n    #     secretName: skaha-cacert-secret\n\n    # Other data to be included in the main ConfigMap of this deployment.\n    # Of note, files that end in .key are special and base64 decoded.\n    #\n    # extraConfigData:\n\n    # Resources provided to the Skaha service.\n    # For units of storage, see https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#meaning-of-memory.\n    resources:\n      requests:\n        memory: \"1Gi\"\n        cpu: \"500m\"\n      limits:\n        memory: \"1500Mi\"\n        cpu: \"750m\"\n\n  # Specify extra hostnames that will be added to the Pod's /etc/hosts file.  Note that this is in the\n  # deployment object, not the skaha one.\n  #\n  # These entries get added as hostAliases entries to the Deployment.\n  #\n  # Example:\n  # extraHosts:\n  #   - ip: 127.3.34.5\n  #     hostname: myhost.example.org\n  #\n  # extraHosts: []\n\n# Set these labels appropriately to match your Persistent Volume labels.\n# The storage.service.spec can be anything that supports ACLs, such as CephFS or Local.\n# The CephFS Volume can be dynamically allocated here for the storage.service.spec:\n# Example:\n# storage:\n#   service:\n#     spec:\n#       cephfs:\n#         mons:\n#           ...\n# Default is a PersistentVolumeClaim to the Local Storage.\nstorage:\n  service:\n    spec:\n      persistentVolumeClaim:\n        claimName: skaha-pvc # Match this label up with whatever was installed in the base install, or the desired PVC, or create dynamically provisioned storage.\n\nsecrets:\n  # Uncomment to enable local or self-signed CA certificates for your domain to be trusted.\n#   skaha-cacert-secret:\n#     ca.crt: &lt;base64 encoded ca crt&gt;\n</code></pre></p> <pre><code>helm install -n skaha-system --values my-skaha-local-values-file.yaml skaha science-platform/skaha\n\nNAME: skaha\nLAST DEPLOYED: Thu Sep 28 07:31:10 2023\nNAMESPACE: skaha-system\nSTATUS: deployed\nREVISION: 1\n</code></pre> <p>Test it. <pre><code># See below for tokens\nexport SKA_TOKEN=...\ncurl -SsL --header \"Authorization: Bearer ${SKA_TOKEN}\" https://example.host.com/skaha/v1/session\n[]%\n\n# xxxxxx is the returned session ID.\ncurl -SsL --header \"Authorization: Bearer ${SKA_TOKEN}\" -d \"ram=1\" -d \"cores=1\" -d \"image=images.canfar.net/canucs/canucs:1.2.5\" -d \"name=myjupyternotebook\" \"https://example.host.com/skaha/v1/session\"\n</code></pre></p>"},{"location":"helm/canfar/#science-portal-user-interface-install","title":"Science Portal User Interface install","text":"<p>The Science Portal service will manage User Sessions.  It relies on the Skaha service being deployed, and available to be found via the IVOA Registry:</p> <p><code>/reg/resource-caps</code> <pre><code>...\n# Ensure the hostname matches the deployment hostname.\nivo://example.org/skaha = https://example.host.com/skaha/capabilities\n...\n</code></pre></p> <p>Create a <code>my-science-portal-local-values-file.yaml</code> file to override Values from the main template <code>values.yaml</code> file.</p> <p><code>my-science-portal-local-values-file.yaml</code> <pre><code>deployment:\n  hostname: example.org\n  sciencePortal:\n    # The Resource ID of the Service that contains the URL of the Skaha service in the IVOA Registry\n    skahaResourceID: ivo://example.org/skaha\n\n    # OIDC (IAM) server configuration.  These are required\n    # oidc:\n    #\n    # Location of the OpenID Provider (OIdP), and where users will login\n    #   uri: https://iam.example.org/\n\n      # The Client ID as listed on the OIdP.  Create one at the uri above.\n    #   clientID: my-client-id\n\n      # The Client Secret, which should be generated by the OIdP.\n    #   clientSecret: my-client-secret\n\n      # Where the OIdP should send the User after successful authentication.  This is also known as the redirect_uri in OpenID.\n    #   redirectURI: https://example.com/science-portal/oidc-callback\n\n      # Where to redirect to after the redirectURI callback has completed.  This will almost always be the URL to the /science-portal main page (https://example.com/science-portal).\n    #   callbackURI: https://example.com/science-portal/\n\n      # The standard OpenID scopes for token requests.  This is required, and if using the SKAO IAM, can be left as-is.\n    #   scope: \"openid profile offline_access\"\n\n    # Optionally mount a custom CA certificate\n    # extraVolumeMounts:\n    # - mountPath: \"/config/cacerts\"\n    #   name: cacert-volume\n\n    # Create the CA certificate volume to be mounted in extraVolumeMounts\n    # extraVolumes:\n    # - name: cacert-volume\n    #   secret:\n    #     defaultMode: 420\n    #     secretName: science-portal-cacert-secret\n\n    # The theme name for styling.\n    # src: The SRCNet theme\n    # canfar: The CANFAR theme for internal CADC deployment\n    # themeName: {src | canfar}\n\n    # Labels on the tabs\n    # Default:\n    # tabLabels:\n    #  - Public\n    #  - Advanced\n    # tabLabels: []\n\n    # Other data to be included in the main ConfigMap of this deployment.\n    # Of note, files that end in .key are special and base64 decoded.\n    #\n    # extraConfigData:\n\n    # Resources provided to the Science Portal service.\n    resources:\n      requests:\n        memory: \"1Gi\"\n        cpu: \"500m\"\n      limits:\n        memory: \"1500Mi\"\n        cpu: \"750m\"\n\n    # Optionally describe how this Pod will be scheduled using the nodeAffinity clause. This applies to Science Portal itself.\n    # See https://kubernetes.io/docs/tasks/configure-pod-container/assign-pods-nodes-using-node-affinity/\n    # Example:\n    nodeAffinity:\n      # Only allow Science Portal to run on specific Nodes.\n      requiredDuringSchedulingIgnoredDuringExecution:\n        nodeSelectorTerms:\n        - matchExpressions:\n          - key: kubernetes.io/hostname\n            operator: In\n            values:\n            - my-special-ui-host\n\n  # Specify extra hostnames that will be added to the Pod's /etc/hosts file.  Note that this is in the\n  # deployment object, not the sciencePortal one.\n  #\n  # These entries get added as hostAliases entries to the Deployment.\n  #\n  # Example:\n  # extraHosts:\n  #   - ip: 127.3.34.5\n  #     hostname: myhost.example.org\n  #\n  # extraHosts: []\n\n# secrets:\n  # Uncomment to enable local or self-signed CA certificates for your domain to be trusted.\n  # science-portal-cacert-secret:\n    # ca.crt: &lt;base64 encoded ca.crt blob&gt;\n</code></pre></p>"},{"location":"helm/canfar/#cavern-user-storage-api-install","title":"Cavern (User Storage API) install","text":"<p>The Cavern API provides access to the User Storage which is shared between Skaha and all of the User Sessions.  A Bearer token is required when trying to read private access, or any writing.</p> <p>[!NOTE] The <code>/home</code> and <code>/projects</code> folders will be created if not present during install.  Do not include them with your configuration!</p> <p>Create a <code>my-cavern-local-values-file.yaml</code> file to override Values from the main template <code>values.yaml</code> file.</p> <p><code>my-cavern-local-values-file.yaml</code> <pre><code># Cavern web service deployment\ndeployment:\n  hostname: example.org\n  cavern:\n    # How cavern identifies itself.  Required.\n    resourceID: \"ivo://example.org/cavern\"\n\n    # Set the Registry URL pointing to the desired registry.  Required\n    registryURL: \"https://example.org/reg\"\n\n    # How to find the POSIX Mapper API.  URI (ivo://) or URL (https://).  Required.\n    posixMapperResourceID: \"ivo://example.org/posix-mapper\"\n\n    # User Allocation settings.  This is used to create new folders under the main root allocation folders, namely /home and /projects.\n    allocations:\n      # Required.  The default size, in GB, of an allocation.  This is used in the absence of the Quota VOSpace property.  Can be a floating point number.\n      # Provided value is 10 (10 GiB) by default.\n      # Example:\n      #   defaultSizeGB: 25.5\n      defaultSizeGB: 25\n\n    filesystem:\n      # persistent data directory in container\n      dataDir: # e.g. \"/data\"\n\n      # RELATIVE path to the node/file content that could be mounted in other containers\n      subPath: # e.g. \"cavern\"\n\n      # See https://github.com/opencadc/vos/tree/master/cavern for documentation.  For deployments using OpenID Connect,\n      # the rootOwner MUST be an object with the following properties set.\n      rootOwner:\n        # The adminUsername is required to be set whomever has admin access over the filesystem.dataDir above.\n        adminUsername:\n\n        # The username of the root owner.\n        username:\n\n        # The UID of the root owner.\n        uid:\n\n        # The GID of the root owner.\n        gid:\n\n    # The API keys object that will be permitted to perform administrative tasks.  These will be passed as authorization headers to the Cavern API.\n    # The token values will be used by client applications, and each client matching a clientApplicationName should be configured with the matching token.\n    # Format is &lt;clientApplicationName&gt;: &lt;apiKeyToken&gt;\n    # Example:\n    #   adminAPIKeys:\n    #     skaha: \"token-value\"\n    #     prepareData: \"another-token-value\"\n    adminAPIKeys:\n      skaha: \"secret-for-skaha\"\n\n    # Further UWS settings for the Tomcat Pool setup.\n    uws:\n      install: true\n      schema: uws\n      maxActive: 2\n\n    # Optional rename of the application from the default \"cavern\"\n    # applicationName: \"cavern\"\n\n    # The endpoint to serve this from.  Defaults to /cavern.  If the applicationName is changed, then this should match.\n    # Don't forget to update your registry entries!\n    #\n    # endpoint: \"/cavern\"\n\n    # Simple Class name of the QuotaPlugin to use.  This is used to request quota and folder size information\n    # from the underlying storage system.  Optional, defaults to NoQuotaPlugin.\n    #\n    # - For CephFS deployments: CephFSQuotaPlugin\n    # - Default: NoQuotaPlugin\n    #\n    # quotaPlugin: {NoQuotaPlugin | CephFSQuotaPlug}\n\n    # Optionally set the DEBUG port.\n    #\n    # Example:\n    # extraEnv:\n    # - name: CATALINA_OPTS\n    #   value: \"-agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=0.0.0.0:5555\"\n    # - name: JAVA_OPTS\n    #   value: \"-agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=0.0.0.0:5555\"\n    #\n    # extraEnv:\n\n    # Optionally mount a custom CA certificate\n    # Example:\n    # extraVolumeMounts:\n    # - mountPath: \"/config/cacerts\"\n    #   name: cacert-volume\n    #\n    # extraVolumeMounts:\n\n    # Create the CA certificate volume to be mounted in extraVolumeMounts\n    # Example:\n    # extraVolumes:\n    # - name: cacert-volume\n    #   secret:\n    #     defaultMode: 420\n    #     secretName: cavern-cacert-secret\n    #\n    # extraVolumes:\n\n    # Other data to be included in the main ConfigMap of this deployment.\n    # Of note, files that end in .key are special and base64 decoded.\n    #\n    # extraConfigData:\n\n    # Resources provided to the Cavern service.\n    resources:\n      requests:\n        memory: \"1Gi\"\n        cpu: \"500m\"\n      limits:\n        memory: \"1Gi\"\n        cpu: \"500m\"\n\n    # Optionally describe how this Pod will be scheduled using the nodeAffinity clause. This applies to Cavern.\n    # See https://kubernetes.io/docs/tasks/configure-pod-container/assign-pods-nodes-using-node-affinity/\n    # Example:\n    nodeAffinity:\n      # Only allow Cavern to run on specific Nodes.\n      requiredDuringSchedulingIgnoredDuringExecution:\n        nodeSelectorTerms:\n        - matchExpressions:\n          - key: kubernetes.io/hostname\n            operator: In\n            values:\n            - my-special-api-host\n\n  # Specify extra hostnames that will be added to the Pod's /etc/hosts file.  Note that this is in the\n  # deployment object, not the cavern one.\n  #\n  # These entries get added as hostAliases entries to the Deployment.\n  #\n  # Example:\n  # extraHosts:\n  #   - ip: 127.3.34.5\n  #     hostname: myhost.example.org\n  #\n  # extraHosts: []\n\n# secrets:\n  # Uncomment to enable local or self-signed CA certificates for your domain to be trusted.\n  # cavern-cacert-secret:\n  #   ca.crt: &lt;base64 encoded CA crt&gt;\n\n# Set these appropriately to match your Persistent Volume Claim labels.\nstorage:\n  service:\n    spec:\n      # YAML for service mounted storage.\n      # Example is the persistentVolumeClaim below.  This should match whatever Skaha used.\n      # persistentVolumeClaim:\n      #   claimName: skaha-pvc\n\n# UWS Database\npostgresql:\n  install: true  # To run your own database, set this to false and override auth settings.\n</code></pre></p>"},{"location":"helm/canfar/#user-storage-ui-installation","title":"User Storage UI installation","text":"<p>Create a <code>my-storage-ui-local-values-file.yaml</code> file to override Values from the main template <code>values.yaml</code> file.</p> <p><code>my-storage-ui-local-values-file.yaml</code> <pre><code>deployment:\n  hostname: example.org\n  storageUI:\n    # OIDC (IAM) server configuration.  These are required\n    oidc:\n      # Location of the OpenID Provider (OIdP), and where users will login\n      uri: https://iam.example.org/\n\n      # The Client ID as listed on the OIdP.  Create one at the uri above.\n      clientID: my-client-id\n\n      # The Client Secret, which should be generated by the OIdP.\n      clientSecret:  my-client-secret\n\n      # Where the OIdP should send the User after successful authentication.  This is also known as the redirect_uri in OpenID.\n      redirectURI: https://example.com/science-portal/oidc-callback\n\n      # Where to redirect to after the redirectURI callback has completed.  This will almost always be the URL to the /science-portal main page (https://example.com/science-portal).\n      callbackURI: https://example.com/science-portal/\n\n      # The standard OpenID scopes for token requests.  This is required, and if using the SKAO IAM, can be left as-is.\n      scope: \"openid profile offline_access\"\n\n    # ID (URI) of the GMS Service.\n    gmsID: ivo://example.org/gms\n\n    # Dictionary of all VOSpace APIs (Services) available that will be visible on the UI.\n    # Format is:\n    backend:\n      defaultService: cavern\n      services:\n        cavern:\n          resourceID: \"ivo://example.org/cavern\"\n          nodeURIPrefix: \"vos://example.org~cavern\"\n          userHomeDir: \"/home\"\n          # Some VOSpace services support these features.  Cavern does not, but it needs to be explicitly declared here.\n          features:\n            batchDownload: false\n            batchUpload: false\n            externalLinks: false\n            paging: false\n\n    # Optionally mount a custom CA certificate\n    # extraVolumeMounts:\n    # - mountPath: \"/config/cacerts\"\n    #   name: cacert-volume\n\n    # Create the CA certificate volume to be mounted in extraVolumeMounts\n    # extraVolumes:\n    # - name: cacert-volume\n    #   secret:\n    #     defaultMode: 420\n    #     secretName: storage-ui-cacert-secret\n\n    # Other data to be included in the main ConfigMap of this deployment.\n    # Of note, files that end in .key are special and base64 decoded.\n    #\n    # extraConfigData:\n\n    # Resources provided to the StorageUI service.\n    resources:\n      requests:\n        memory: \"1Gi\"\n        cpu: \"500m\"\n      limits:\n        memory: \"1500Mi\"\n        cpu: \"750m\"\n\n    # Optionally describe how this Pod will be scheduled using the nodeAffinity clause. This applies to the Storage UI Pod(s).\n    # See https://kubernetes.io/docs/tasks/configure-pod-container/assign-pods-nodes-using-node-affinity/\n    # Example:\n    nodeAffinity:\n      # Only allow Storage UI to run on specific Nodes.\n      requiredDuringSchedulingIgnoredDuringExecution:\n        nodeSelectorTerms:\n        - matchExpressions:\n          - key: kubernetes.io/hostname\n            operator: In\n            values:\n            - my-special-ui-host\n\n  # Specify extra hostnames that will be added to the Pod's /etc/hosts file.  Note that this is in the\n  # deployment object, not the storageUI one.\n  #\n  # These entries get added as hostAliases entries to the Deployment.\n  #\n  # Example:\n  # extraHosts:\n  #   - ip: 127.3.34.5\n  #     hostname: myhost.example.org\n  #\n  # extraHosts: []\n\n# secrets:\n  # Uncomment to enable local or self-signed CA certificates for your domain to be trusted.\n  # storage-ui-cacert-secret:\n    # ca.crt: &lt;base64 encoded ca.crt blob&gt;\n</code></pre></p>"},{"location":"helm/canfar/#obtaining-a-bearer-token","title":"Obtaining a Bearer Token","text":"<p>See the JIRA Confluence page on obtaining a Bearer Token.</p>"},{"location":"helm/canfar/#flow","title":"Flow","text":"<p>The Skaha service depends on several installations being in place.</p>"},{"location":"helm/canfar/#structure","title":"Structure","text":""},{"location":"helm/cavern/","title":"Helm Chart for the Cavern User Storage API","text":"<p>See the Deployment Guide for a better idea of a full system.</p>"},{"location":"helm/cavern/#install","title":"Install","text":"<p>This <code>README</code> will focus on a basic install using a new <code>values-local.yaml</code> file.</p> <p>A working Science Platform is not required, but the Persistent Volume Claims are needed.  Those PVs and PVCs will provide the underlying storage for the Services and User Sessions.</p>"},{"location":"helm/cavern/#from-source","title":"From source","text":"<p>Installation depends on a working Kubernetes cluster version 1.23 or greater.</p> <p>The base install also installs the Traefik proxy, which is needed by the Ingress when the Science Platform services are installed.</p> <pre><code>$ git clone https://github.com/opencadc/science-platform.git\n$ cd science-platform/deployment/helm\n$ helm install -n skaha-system --dependency-update --values my-values-local.yaml &lt;name&gt; ./cavern\n</code></pre> <p>Where <code>&lt;name&gt;</code> is the name of this installation.  Example: <pre><code>$ helm install -n skaha-system --dependency-update --values my-values-local.yaml cavern ./cavern\n</code></pre> This will install Skaha service dependency, as well as the Skaha webservice and any necessary Ingress. <pre><code>NAME: cavern\nLAST DEPLOYED: &lt;Timestamp e.g. Fri Nov 07 04:19:04 2023&gt;\nSTATUS: deployed\nREVISION: 1\nTEST SUITE: None\n</code></pre></p>"},{"location":"helm/cavern/#verification","title":"Verification","text":"<p>After the install, there should exist the necessary Service.  See the Namespaces:</p> <pre><code>$ kubectl -n skaha-system get services\nNAME                   STATUS   AGE\n...\nskaha-system   cavern-tomcat-svc             ClusterIP      10.108.202.148   &lt;none&gt;        8080/TCP            1m\n</code></pre> <p>The IVOA VOSI availability endpoint can be used to check that the Skaha service has started properly.  It may take a few moments to start up.</p> <pre><code>$ curl https://myhost.example.com/cavern/availability\n\n&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n&lt;vosi:availability xmlns:vosi=\"http://www.ivoa.net/xml/VOSIAvailability/v1.0\"&gt;\n  &lt;vosi:available&gt;true&lt;/vosi:available&gt;\n  &lt;vosi:note&gt;service is accepting requests.&lt;/vosi:note&gt;\n  &lt;!--&lt;clientip&gt;192.1.1.4&lt;/clientip&gt;--&gt;\n&lt;/vosi:availability&gt;\n</code></pre>"},{"location":"helm/cavern/#configuration","title":"Configuration","text":"Parameter Description Default <code>kubernetesClusterDomain</code> Kubernetes cluster domain used to find internal hosts <code>cluster.local</code> <code>replicaCount</code> Number of Cavern replicas to deploy <code>1</code> <code>tolerations</code> Array of tolerations to pass to Kubernetes for fine-grained Node targeting of the Cavern API <code>[]</code> <code>deployment.hostname</code> Hostname for the Cavern deployment <code>\"\"</code> <code>deployment.cavern.loggingGroups</code> List of groups permitted to adjust logging levels for the Cavern service. <code>[]</code> <code>deployment.cavern.image</code> Cavern Docker image <code>images.opencadc.org/platform/cavern:&lt;current release version&gt;</code> <code>deployment.cavern.imagePullPolicy</code> Image pull policy for the Cavern container <code>IfNotPresent</code> <code>deployment.cavern.resourceID</code> Resource ID (URI) for this Cavern service <code>\"\"</code> <code>deployment.cavern.oidcURI</code> URI (or URL) for the OIDC service <code>\"\"</code> <code>deployment.cavern.gmsID</code> Resource ID (URI) for the IVOA Group Management Service <code>\"\"</code> <code>deployment.cavern.adminAPIKeys</code> API keys for client applications that can create new allocations <code>{}</code> <code>deployment.cavern.allocations.defaultSizeGB</code> Default size of user allocations in GB <code>10</code> <code>deployment.cavern.allocations.parentFolders</code> List of parent folders to create for user allocations.  Best to leave this alone. <code>[\"/home\", \"/projects\"]</code> <code>deployment.cavern.filesystem.dataDir</code> Persistent data directory in the Cavern container <code>\"\"</code> <code>deployment.cavern.filesystem.subPath</code> Relative path to the node/file content that could be mounted in other containers <code>\"\"</code> <code>deployment.cavern.filesystem.rootOwner.username</code> Username of the root owner of the filesystem data (parent of allocations) directory <code>\"\"</code> <code>deployment.cavern.filesystem.rootOwner.uid</code> UID of the root owner of the filesystem data (parent of allocations) directory <code>\"\"</code> <code>deployment.cavern.filesystem.rootOwner.gid</code> GID of the root owner of the filesystem data (parent of allocations) directory <code>\"\"</code> <code>deployment.cavern.filesystem.rootOwner.adminUsername</code> Admin username for the filesystem data (parent of allocations) directory <code>deployment.cavern.identityManagerClass</code> Class name for the identity manager used by Cavern <code>org.opencadc.auth.StandardIdentityManager</code> <code>deployment.cavern.uws.db.install</code> Whether to deploy a local PostgreSQL database for UWS <code>true</code> <code>deployment.cavern.uws.db.image</code> PostgreSQL image to use for UWS <code>postgres:15.12</code> <code>deployment.cavern.uws.db.runUID</code> UID for the PostgreSQL user in the UWS database <code>999</code> <code>deployment.cavern.uws.db.database</code> Name of the UWS database <code>uws</code> <code>deployment.cavern.uws.db.url</code> JDBC URL for the UWS database.  Use instead of <code>database</code>. <code>jdbc:postgresql://cavern-uws-db:5432/uws</code> <code>deployment.cavern.uws.db.username</code> Username for the UWS database <code>uwsuser</code> <code>deployment.cavern.uws.db.password</code> Password for the UWS database <code>uwspwd</code> <code>deployment.cavern.uws.db.schema</code> Schema name for the UWS database <code>uws</code> <code>deployment.cavern.uws.db.maxActive</code> Maximum number of active connections to the UWS database <code>2</code> <code>deployment.applicationName</code> Optional rename of the application from the default \"cavern\" <code>cavern</code> <code>deployment.endpoint</code> Endpoint to serve the Cavern service from <code>/cavern</code> <code>deployment.extraEnv</code> Extra environment variables to set in the Cavern container <code>[]</code> <code>deployment.extraVolumeMounts</code> Extra volume mounts for the Cavern container <code>[]</code> <code>deployment.extraVolumes</code> Extra volumes to mount in the Cavern container <code>[]</code> <code>deployment.resources.requests.memory</code> Memory request for the Cavern container <code>1Gi</code> <code>deployment.resources.requests.cpu</code> CPU request for the Cavern container <code>500m</code> <code>deployment.resources.limits.memory</code> Memory limit for the Cavern container <code>1Gi</code> <code>deployment.resources.limits.cpu</code> CPU limit for the Cavern container <code>500m</code> <code>deployment.cavern.registryURL</code> (list OR string) <code>[]</code> IVOA Registry array of IVOA Registry locations or single IVOA Registry location <code>livenessProbe</code> Configure the liveness probe check <code>{}</code> <code>readinessProbe</code> Configure the readiness probe check <code>{}</code> <code>tolerations</code> Tolerations to apply to the Cavern Pod <code>[]</code> <code>secrets</code> Secrets to create for the Cavern service, such as CA certificates <code>{}</code> <code>service.cavern.extraPorts</code> Extra ports to expose for the Cavern service <code>[]</code> <code>storage.service.spec</code> Storage specification for the Cavern service <code>{}</code>"},{"location":"helm/cavern/#user-allocations-with-special-access","title":"User Allocations with special access","text":""},{"location":"helm/cavern/#note","title":"Note","text":"<p>The <code>admin-api-key</code> has admin level permissions.  Rotate them regularly, or keep the values file safe.</p> <p>Cavern typically accepts user allocation requests from the Administrative user, but it can be configured to allow other users to request allocations as well. This is done by adding the user's API key to the <code>deployment.cavern.adminAPIKeys</code> configuration: <pre><code>deployment:\n  cavern:\n    adminAPIKeys:\n      skaha: \"skahasecretkey1234567890\"\n      prepareData: \"preparedatasecretkey1234567890\"\n</code></pre></p> <p>With this configuration, listed clients can request new user allocations using the <code>admin-api-key</code> challenge type in the <code>Authorization</code> header.  This <code>admin-api-key</code> represents a trusted client application to act on behalf of the Administrative user: <pre><code>$ curl -Lv --header \"Authorization: admin-api-key prepareData:preparedatasecretkey1234567890\" --header \"content-type: text/xml\" --upload-file user-alloc-upload-jwt.xml https://example.org/cavern/nodes/home/new-user\n</code></pre></p> <p>Where the upload XML file would look like this: <pre><code>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n&lt;vos:node xmlns:vos=\"http://www.ivoa.net/xml/VOSpace/v2.0\"\n          xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n          uri=\"vos://exmaple.org~cavern/home/new-user\" xsi:type=\"vos:ContainerNode\"&gt;\n  &lt;vos:properties&gt;\n    &lt;vos:property uri=\"ivo://opencadc.org/vospace/core#creatorJWT\"&gt;JWT_TOKEN_REPLACE_ME&lt;/vos:property&gt; &lt;!-- JWT token of the new user --&gt;\n    &lt;vos:property uri=\"ivo://cadc.nrc.ca/vospace/core#inheritPermissions\"&gt;true&lt;/vos:property&gt;\n    &lt;vos:property uri=\"ivo://ivoa.net/vospace/core#quota\"&gt;524288000&lt;/vos:property&gt;. &lt;!-- 500MB quota example, adjust as needed --&gt;\n  &lt;/vos:properties&gt;\n  &lt;vos:nodes /&gt;\n&lt;/vos:node&gt;\n</code></pre> Where <code>JWT_TOKEN_REPLACE_ME</code> is replaced with a valid JWT token of the new user (i.e. the one making the request to be added).  Don't forget to set the <code>vos:property:uri</code> to the correct value for your service and the path of the new user.</p>"},{"location":"helm/posix-mapper/","title":"Deployment","text":"<p>This Helm chart deploys the POSIX Mapper application, which is designed to map POSIX file system operations to a cloud-native environment.</p>"},{"location":"helm/posix-mapper/#prerequisites","title":"Prerequisites","text":"<ul> <li>Kubernetes 1.27+</li> <li>Helm 3.0+</li> <li>Deployed PostgreSQL database for application data storage</li> </ul>"},{"location":"helm/posix-mapper/#postgresql-database","title":"PostgreSQL Database","text":"<p>The POSIX Mapper requires a PostgreSQL database to store UID/GID mappings.  As this is a critical component, ensure that your database is properly configured and accessible from the POSIX Mapper application.  Use some persistent storage solution (like a Persistent Volume Claim) to ensure that the database data is not lost if deploying PostgreSQL in Kubernetes, or install a dedicated instance outside of the cluster.</p>"},{"location":"helm/posix-mapper/#sample-postgresql-installation-in-kubernetes","title":"Sample PostgreSQL Installation (in Kubernetes)","text":"<p>You can deploy a PostgreSQL database using the following Helm chart, with a PVC to ensure data persistence (Using <code>skaha-system</code> namespace as an example):</p>"},{"location":"helm/posix-mapper/#persistent-volume-claim-pvc","title":"Persistent Volume Claim (PVC)","text":"<p>Create a Persistent Volume Claim (PVC) for PostgreSQL: <pre><code>apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: posix-mapper-postgres-pvc\n  namespace: skaha-system\nspec:\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 2Gi\n  storageClassName: \"\"\n  selector:\n    matchLabels:\n      storage: posix-mapper-postgres-storage\n</code></pre></p> <p>This will need to match to a Persistent Volume (PV) that is available in your Kubernetes cluster.  An example PV could look like this for a CephFS instance in an OpenStack Share:</p> <pre><code>---\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: posix-mapper-postgres-pv\n  labels:\n    storage: posix-mapper-postgres-storage\nspec:\n  capacity:\n    storage: 2Gi\n  volumeMode: Filesystem\n  accessModes:\n    - ReadWriteMany\n  persistentVolumeReclaimPolicy: Delete\n  storageClassName: \"\"\n  cephfs:\n    monitors:\n    - 10.0.0.1:6789\n    - 10.0.0.2:6789\n    path: /volumes/myvolume\n    user: posix-mapper-postgres\n    readOnly: false\n    secretRef:\n      name: posix-mapper-postgres-secret\n      namespace: skaha-system\n</code></pre> <p>Ultimately, it will be up to the deployment to ensure that the PVC is bound to a suitable PV, and that the PV is available in the cluster.</p>"},{"location":"helm/posix-mapper/#install-postgresql-using-helm","title":"Install PostgreSQL using Helm","text":"<pre><code>helm repo add bitnami https://charts.bitnami.com/bitnami\nhelm repo update\n</code></pre> <p>Use a Helm Values file to customize the installation.  This will initialize the database schema and set up the required user credentials.  The schema should match what the POSIX Mapper expects in its configuration. Create a file named <code>my-postgresql-values.yaml</code> with the following content: <pre><code>auth:\n  username: posixmapperuser\n  password: posixmapperpwd\n  database: posixmapper\nprimary:\n  initdb:\n    scripts:\n      init_schema.sql: |\n          create schema mapping;\n  persistence:\n    enabled: true\n    existingClaim: posix-mapper-postgres-pvc\n</code></pre> <pre><code>helm install posix-mapper-postgres bitnami/postgresql \\\n  --namespace skaha-system \\\n  --values my-postgresql-values.yaml\n</code></pre></p>"},{"location":"helm/posix-mapper/#posix-mapper-installation","title":"POSIX Mapper Installation","text":"<p>To deploy the POSIX Mapper application using the Helm chart, follow these steps:</p> <ol> <li> <p>Add the Helm Repository <pre><code>helm repo add science-platform-repo https://images.opencadc.org/chartrepo/platform\nhelm repo update\n</code></pre></p> </li> <li> <p>Install the POSIX Mapper Chart: <pre><code>helm -n skaha-system --values &lt;myvalues.yaml&gt; install posix-mapper science-platform-repo/posix-mapper\n</code></pre></p> </li> </ol>"},{"location":"helm/posix-mapper/#configuration","title":"Configuration","text":"<p>The POSIX Mapper Helm chart comes with some default configuration suitable for most deployments. However, you can customize the installation by providing your own <code>values.yaml</code> file. This allows you to override default settings such as resource allocations, environment variables, and other parameters, as well as set required parameters such as the PostgreSQL database configuration.</p> <p>To customize the installation:</p> <ul> <li>Create a <code>local-values.yaml</code> File: Define your custom configurations in this file.</li> <li>Install the Chart with Custom Values: <pre><code>helm -n skaha-system upgrade --install --values local-values.yaml posix-mapper science-platform-repo/posix-mapper\n</code></pre></li> </ul>"},{"location":"helm/posix-mapper/#supported-configuration-options","title":"Supported Configuration Options","text":"<p>See the values.yaml file for a complete list of configuration options. Below are some of the key parameters you can configure:</p> Parameter Description Default <code>kubernetesClusterDomain</code> Kubernetes cluster domain used to find internal hosts <code>cluster.local</code> <code>replicaCount</code> Number of POSIX Mapper replicas to deploy <code>1</code> <code>tolerations</code> Array of tolerations to pass to Kubernetes for fine-grained Node targeting of the <code>posix-mapper</code> API <code>[]</code> <code>deployment.hostname</code> Hostname for the POSIX Mapper deployment <code>\"\"</code> <code>deployment.posixMapper.loggingGroups</code> List of groups permitted to adjust logging levels for the POSIX Mapper service. <code>[]</code> <code>deployment.posixMapper.image</code> POSIX Mapper Docker image <code>images.opencadc.org/platform/posix-mapper:&lt;current release version&gt;</code> <code>deployment.posixMapper.imagePullPolicy</code> Image pull policy for the POSIX Mapper container <code>IfNotPresent</code> <code>deployment.posixMapper.resourceID</code> Resource ID (URI) for this POSIX Mapper service <code>\"\"</code> <code>deployment.posixMapper.oidcURI</code> URI (or URL) for the OIDC service <code>\"\"</code> <code>deployment.posixMapper.gmsID</code> Resource ID (URI) for the IVOA Group Management Service <code>\"\"</code> <code>deployment.posixMapper.minUID</code> Minimum UID for POSIX Mapper operations.  High to avoid conflicts. <code>10000</code> <code>deployment.posixMapper.minGID</code> Minimum GID for POSIX Mapper operations.  High to avoid conflicts. <code>900000</code> <code>deployment.posixMapper.registryURL</code> URL for the IVOA registry containing service locations <code>\"\"</code> <code>deployment.posixMapper.nodeAffinity</code> Kubernetes Node affinity for the POSIX Mapper API Pod <code>{}</code> <code>deployment.posixMapper.extraPorts</code> List of extra ports to expose in the POSIX Mapper service.  See the <code>values.yaml</code> file for examples. <code>[]</code> <code>deployment.posixMapper.extraVolumeMounts</code> List of extra volume mounts to be mounted in the POSIX Mapper deployment.  See the <code>values.yaml</code> file for examples. <code>[]</code> <code>deployment.posixMapper.extraVolumes</code> List of extra volumes to be mounted in the POSIX Mapper deployment.  See the <code>values.yaml</code> file for examples. <code>[]</code> <code>deployment.posixMapper.extraHosts</code> List of extra hosts to be added to the POSIX Mapper deployment.  See the <code>values.yaml</code> file for examples. <code>[]</code> <code>deployment.posixMapper.extraEnv</code> List of extra environment variables to be set in the POSIX Mapper service.  See the <code>values.yaml</code> file for examples. <code>[]</code> <code>deployment.posixMapper.resources</code> Resource requests and limits for the POSIX Mapper API <code>{}</code> <code>deployment.posixMapper.registryURL</code> (list OR string) <code>[]</code> IVOA Registry array of IVOA Registry locations or single IVOA Registry location <code>postgresql.maxActive</code> Maximum number of active connections to the PostgreSQL database <code>8</code> <code>postgresql.url</code> Required JDBC URL for the PostgreSQL database <code>\"\"</code> <code>postgresql.schema</code> Required Database schema to use for the POSIX Mapper <code>\"\"</code> <code>postgresql.auth.username</code> Username for the PostgreSQL database <code>\"\"</code> <code>postgresql.auth.password</code> Password for the PostgreSQL database <code>\"\"</code>"},{"location":"helm/science-portal/","title":"Helm Chart for the Science Portal user interface","text":"<p>See the Deployment Guide for a better idea of the underlying APIs.</p>"},{"location":"helm/science-portal/#dependencies","title":"Dependencies","text":"<ul> <li>An existing Kubernetes cluster.</li> <li>An IVOA Registry (See the Current SKAO Registry)</li> <li>A working Science Platform system</li> </ul>"},{"location":"helm/science-portal/#install","title":"Install","text":"<p>The Science Portal is a Single Page Application (SPA) with a rich Javascript client and DOM manager.  It uses React to power the various Dashboard elements, and is configurable for different OpenID Providers (OIdP).</p>"},{"location":"helm/science-portal/#minimum-helm-configuration","title":"Minimum Helm configuration","text":"<p>See the full set of options in the values.yaml.  The deployed Redirect URI (<code>redirect_uri</code>) is <code>/science-portal/oidc-callback</code>, which handles receiving the <code>code</code> as part of the authorization code flow, and obtaining a token to put into a cookie.</p> <p><code>my-science-portal-local-values-file.yaml</code> <pre><code># @param securityContext - Optional security context for the container.  This is a security feature to restrict system calls.\n# securityContext: {}\n#\n# Example:\n# securityContext:\n#   seccompProfile:\n#     type: RuntimeDefault\n\n# @param applicationName - The name of the application.  This will rename the underlying WAR file, thus changing the endpoint.  Defaults to science-portal.\n# applicationName: science-portal\n\ndeployment:\n  hostname: example.com # Change this!\n  sciencePortal:\n    # OIDC (IAM) server configuration.  These are required\n    oidc:\n      # Location of the OpenID Provider (OIdP), and where users will login\n      uri: https://ska-iam.stfc.ac.uk/\n\n      # The Client ID as listed on the OIdP.  Create one at the uri above.\n      clientID:\n\n      # The Client Secret, which should be generated by the OIdP.\n      clientSecret:\n\n      # Where the OIdP should send the User after successful authentication.  This is also known as the redirect_uri in OpenID.  This URI NEEDS\n      redirectURI: https://example.com/science-portal/oidc-callback\n\n      # Where to redirect to after the redirectURI callback has completed.  This will almost always be the URL to the /science-portal main page (https://example.com/science-portal).\n      callbackURI: https://example.com/science-portal/\n\n      # The standard OpenID scopes for token requests.  This is required, and if using the SKAO IAM, can be left as-is.\n      scope: \"openid profile offline_access\"\n\n    # Optionally mount a custom CA certificate\n    # extraVolumeMounts:\n    # - mountPath: \"/config/cacerts\"\n    #   name: cacert-volume\n\n    # Create the CA certificate volume to be mounted in extraVolumeMounts\n    # extraVolumes:\n    # - name: cacert-volume\n    #   secret:\n    #     defaultMode: 420\n    #     secretName: science-portal-cacert-secret\n\n    # The Resource ID of the Service that contains the URL of the Skaha service in the IVOA Registry\n    skahaResourceID: ivo://example.org/skaha\n\n    # Array of tab labels from left to right.  There are two supported tabs currently: Public (Standard) and Private (Advanced)\n    # Recommended is Standard and Advanced, but you do you.\n    # Example:\n    #\n    # tabLabels:\n    #   - Standard\n    #   - Advanced\n    #\n    tabLabels: []\n\n    # Theme to use\n    themeName: src\n\n    # The logo in the top left.  No link associated, just the image.  This can be relative, or absolute.\n    # Default is the SRCNet Logo.\n    # logoURL: /science-portal/images/SRCNetLogo.png\n\n# secrets:\n  # Uncomment to enable local or self-signed CA certificates for your domain to be trusted.\n  # science-portal-cacert-secret:\n    # ca.crt: &lt;base64 encoded ca.crt blob&gt;\n</code></pre></p>"},{"location":"helm/science-portal/#run-with-configured-values","title":"Run with configured values","text":"<pre><code>helm repo update\n\nhelm install -n skaha-system --values my-science-portal-local-values-file.yaml scienceportal science-platform/scienceportal\n\nRelease \"scienceportal\" has been installed. Happy Helming!\nNAME: scienceportal\nLAST DEPLOYED: Thu Oct 19 11:59:15 2023\nNAMESPACE: skaha-system\nSTATUS: deployed\nREVISION: 1\nTEST SUITE: None\n</code></pre>"},{"location":"helm/science-portal/#authentication-authorization","title":"Authentication &amp; Authorization","text":"<p>A&amp;A is handle by caching the Token Set server side and issuing a cookie to the browser to enable secure retrieval.  See the Application Authentication Documentation.</p>"},{"location":"helm/science-portal/#endpoints","title":"Endpoints","text":"<p>The system will be available at the <code>/science-portal</code> endpoint, (i.e. https://example.com/science-portal).  Authenticating to the system is mandatory.</p>"},{"location":"helm/skaha/","title":"Skaha Helm Chart","text":"<p>See the Deployment Guide for a better idea of a full system.</p> <p>The Skaha Helm chart facilitates the deployment of the Skaha application within a Kubernetes cluster. This chart is designed to streamline the installation and management of Skaha, ensuring a seamless integration into your Kubernetes environment.</p>"},{"location":"helm/skaha/#prerequisites","title":"Prerequisites","text":"<p>Before deploying the Skaha Helm chart, ensure that the following conditions are met:</p> <ul> <li>Kubernetes Cluster: A running Kubernetes cluster, version 1.27 or higher.</li> <li>Helm: Helm package manager, version 3, installed on your machine. Refer to the official Helm documentation for installation instructions.</li> <li>Kueue: Kueue is recommended to be installed in your cluster, as Skaha optionally integrates with Kueue for job queueing. Follow the Kueue installation guide to set it up.</li> </ul>"},{"location":"helm/skaha/#installation","title":"Installation","text":"<p>To deploy the Skaha application using the Helm chart, follow these steps:</p> <ol> <li> <p>Add the Skaha Helm Repository: <pre><code>helm repo add skaha-repo https://images.opencadc.org/chartrepo/platform\n</code></pre></p> </li> <li> <p>Update Helm Repositories: <pre><code>helm repo update\n</code></pre></p> </li> <li> <p>Install the Skaha Chart: <pre><code>helm install skaha-release skaha-repo/skaha\n</code></pre></p> </li> </ol> <p>Replace <code>skaha-release</code> with your desired release name.</p>"},{"location":"helm/skaha/#configuration","title":"Configuration","text":"<p>The Skaha Helm chart comes with a default configuration suitable for most deployments. However, you can customize the installation by providing your own <code>values.yaml</code> file. This allows you to override default settings such as resource allocations, environment variables, and other parameters.</p> <p>To customize the installation:</p> <ul> <li>Create a <code>values.yaml</code> File: Define your custom configurations in this file.</li> <li>Install the Chart with Custom Values: <pre><code>helm install skaha-release skaha-repo/skaha -f values.yaml\n</code></pre></li> </ul>"},{"location":"helm/skaha/#supported-configuration-options","title":"Supported Configuration Options","text":"<p>The following table lists the configurable parameters for the Skaha Helm chart:</p> Parameter Description Default <code>kubernetesClusterDomain</code> Kubernetes cluster domain used to find internal hosts <code>cluster.local</code> <code>replicaCount</code> Number of Skaha replicas to deploy <code>1</code> <code>tolerations</code> Array of tolerations to pass to Kubernetes for fine-grained Node targeting of the <code>skaha</code> API <code>[]</code> <code>skaha.namespace</code> Namespace where Skaha is deployed <code>skaha-system</code> <code>skahaWorkload.namespace</code> Namespace where Skaha Workload (User Sesssion space) is deployed <code>skaha-workload</code> <code>deployment.hostname</code> Hostname for the Skaha deployment <code>\"\"</code> <code>deployment.skaha.image</code> Skaha Docker image <code>images.opencadc.org/platform/skaha:&lt;current release version&gt;</code> <code>deployment.skaha.imagePullPolicy</code> Image pull policy for the Skaha container <code>IfNotPresent</code> <code>deployment.skaha.imageCache.refreshSchedule</code> Schedule for refreshing the Skaha image cache in <code>cron</code> format <code>@daily</code> <code>deployment.skaha.skahaTld</code> Top-level directory for Skaha <code>/cavern</code> <code>deployment.skaha.defaultQuotaGB</code> Default quota for Skaha in GB.  Used when allocating first-time users into the system. <code>10</code> <code>deployment.skaha.registryHosts</code> Space delimited list of Docker (Harbor) registry hosts <code>images.canfar.net</code> <code>deployment.skaha.usersGroup</code> GMS style Group URI for Skaha users to belong to <code>\"\"</code> <code>deployment.skaha.adminsGroup</code> GMS style Group URI for Skaha admins to belong to <code>\"\"</code> <code>deployment.skaha.headlessGroup</code> GMS style Group URI whose members can submit headless jobs <code>\"\"</code> <code>deployment.skaha.headlessPriorityGroup</code> GMS style Group URI whose member's headless jobs can pre-empt other's.  Useful fortight deadlines in processing <code>\"\"</code> <code>deployment.skaha.headlessPriorityClass</code> Name of the <code>priorityClass</code> for headless jobs to allow some pre-emption <code>\"\"</code> <code>deployment.skaha.loggingGroups</code> List of GMS style Group URIs whose members can alter the log level.  See cadc-log regarding the <code>/logControl</code> endpoint. <code>[]</code> <code>deployment.skaha.posixMapperResourceID</code> Resource ID (URI) for the POSIX Mapper service containing the UIDs and GIDs <code>\"\"</code> <code>deployment.skaha.oidcURI</code> URI (or URL) for the OIDC service <code>\"\"</code> <code>deployment.skaha.gmsID</code> Resource ID (URI) for the IVOA Group Management Service <code>\"\"</code> <code>deployment.skaha.registryURL</code> URL for the IVOA registry containing service locations <code>\"\"</code> <code>deployment.skaha.nodeAffinity</code> Kubernetes Node affinity for the Skaha API Pod <code>{}</code> <code>deployment.skaha.extraEnv</code> List of extra environment variables to be set in the Skaha service.  See the <code>values.yaml</code> file for examples. <code>[]</code> <code>deployment.skaha.resources</code> Resource requests and limits for the Skaha API <code>{}</code> <code>deployment.skaha.extraPorts</code> List of extra ports to expose in the Skaha service.  See the <code>values.yaml</code> file for examples. <code>[]</code> <code>deployment.skaha.extraVolumeMounts</code> List of extra volume mounts to be mounted in the Skaha deployment.  See the <code>values.yaml</code> file for examples. <code>[]</code> <code>deployment.skaha.extraVolumes</code> List of extra volumes to be mounted in the Skaha deployment.  See the <code>values.yaml</code> file for examples. <code>[]</code> <code>deployment.skaha.priorityClassName</code> Name of the <code>priorityClass</code> for the Skaha API Pod used for pre-emption <code>\"\"</code> <code>deployment.skaha.serviceAccountName</code> Name of the Service Account for the Skaha API Pod <code>\"skaha\"</code> <code>deployment.skaha.identityManagerClass</code> Java Class name for the IdentityManager to use.  Defaults to <code>org.opencadc.auth.StandardIdentityManager</code> for use with bearer tokens (OIDC) <code>\"org.opencadc.auth.StandardIdentityManager\"</code> <code>deployment.skaha.apiVersion</code> API version used to match the Ingress path (e.g. <code>/skaha/v0</code>) <code>\"v0\"</code> <code>deployment.skaha.registryURL</code> (list OR string) <code>[]</code> IVOA Registry array of IVOA Registry locations or single IVOA Registry location <code>deployment.skaha.sessions.expirySeconds</code> Expiry time, in seconds, for interactive sessions.  Defaults to four (4) days. <code>\"345600\"</code> <code>deployment.skaha.sessions.imagePullPolicy</code> Image pull policy for all User Sessions. <code>\"Always\"</code> <code>deployment.skaha.sessions.maxCount</code> Maximum number of interactive sessions per user.  Defaults to three (3). <code>\"3\"</code> <code>deployment.skaha.sessions.minEphemeralStorage</code> Minimum ephemeral storage, in Kubernetes quantity, for interactive sessions.  Defaults to 20Gi. <code>\"20Gi\"</code> <code>deployment.skaha.sessions.maxEphemeralStorage</code> Maximum ephemeral storage, in Kubernetes quantity, for interactive sessions.  Defaults to 200Gi. <code>\"200Gi\"</code> <code>deployment.skaha.sessions.initContainerImage</code> Init container image for Skaha User Sessions. <code>redis-7.4.2-alpine3.21</code> <code>deployment.skaha.sessions.kueue.default.queueName</code> Name of the default <code>LocalQueue</code> instance from Kueue for all types <code>\"\"</code> <code>deployment.skaha.sessions.kueue.default.priorityClass</code> Name of the <code>priorityClass</code> for the all types to allow some pre-emption <code>\"\"</code> <code>deployment.skaha.sessions.kueue.&lt;typename&gt;.queueName</code> Name of the <code>LocalQueue</code> instance from Kueue for the given type <code>\"\"</code> <code>deployment.skaha.sessions.kueue.&lt;typename&gt;.priorityClass</code> Name of the <code>priorityClass</code> for the given type to allow some pre-emption <code>\"\"</code> <code>deployment.skaha.sessions.hostname</code> Hostname to access user sessions on.  Defaults to <code>deployment.hostname</code> <code>deployment.hostname</code> <code>deployment.skaha.sessions.tls</code> TLS configuration for the User Sessions IngressRoute. <code>{}</code> <code>deployment.skaha.sessions.extraVolumes</code> List of extra <code>volume</code> and <code>volumeMount</code> to be mounted in User Sessions.  See the <code>values.yaml</code> file for examples. <code>[]</code> <code>deployment.skaha.sessions.gpuEnabled</code> Enable GPU support for User Sessions.  Defaults to <code>false</code> <code>false</code> <code>deployment.skaha.sessions.nodeAffinity</code> Kubernetes Node affinity for the Skaha User Session Pods <code>{}</code> <code>deployment.skaha.sessions.tolerations</code> Array of tolerations to pass to Kubernetes for fine-grained Node targeting of the <code>skaha</code> User Sessions <code>[]</code> <code>secrets</code> List of secrets to be mounted in the Skaha API defined as objects (i.e <code>secretName: {cert.pem: xxx}</code>) <code>[]</code> <code>storage.service.spec</code> Storage class specification for the Skaha API.  Can be <code>persistentVolumeClaim</code> or a dynamic instantiation like <code>hostPath</code>.  See Volumes. <code>{}</code> <code>redis</code> Redis sub-chart configuration for Skaha's caching of Harbor Docker image metadata. See <code>values.yaml</code> for available configuration values. <code>kueue</code> Kueue sub-chart configuration for Skaha's Kueue integration See <code>values.yaml</code> for available configuration values."},{"location":"helm/skaha/#notes-on-tolerations-and-nodeaffinity","title":"Notes on tolerations and nodeAffinity","text":"<p>Ensure that <code>tolerations</code> and <code>nodeAffinity</code> are at the expected indentation!  These are YAML configurations passed directly to Kubernetes, and the base <code>.tolerations</code> and <code>.deployment.skaha.nodeAffinity</code> values apply to the <code>skaha</code> API only, whereas the <code>.deployment.skaha.sessions.tolerations</code> and <code>.deployment.skaha.sessions.nodeAffinity</code> apply to all User Session Pods.</p>"},{"location":"helm/skaha/#kueue","title":"Kueue","text":"<p>Skaha leverages Kueue for efficient job queueing and management when properly installed and configured in your cluster. For detailed information on Kueue's features and setup, refer to the Kueue documentation.</p>"},{"location":"helm/skaha/#installation_1","title":"Installation","text":"<p>https://kueue.sigs.k8s.io/docs/installation/#install-a-released-version</p> <p>Will install the Kueue Chart, with a default <code>ClusterQueue</code>, and whatever defined <code>LocalQueues</code> were declared in the <code>deployment.skaha.sessions.kueue</code> section: <pre><code>deployment:\n  skaha:\n    sessions:\n      kueue:\n        notebook:\n          queueName: some-local-queue\n          priorityClass: med\n</code></pre></p> <p>To determine your cluster's allocatable resources, checkout a small Python utility (requires <code>uv</code>): https://github.com/opencadc/deployments/tree/main/configs/kueue/kueuer</p> <p>Then run: <pre><code>git clone https://github.com/opencadc/deployments/tree/main/configs/kueue/kueuer\ncd kueuer\n# if not using the default ~/.kube/config\nexport KUBECONFIG=/home/user/.kube/my-config\n\n# 60% of cluster resources\nuv run kr cluster resources -f allocatable -s 0.6\n\n# 80% of cluster resources\nuv run kr cluster resources -f allocatable -s 0.8\n</code></pre></p>"},{"location":"helm/skaha/#uninstallation","title":"Uninstallation","text":"<p>To remove the Skaha application from your cluster:</p> <pre><code>helm uninstall skaha-release\n</code></pre> <p>This command will delete all resources associated with the Skaha release.</p>"},{"location":"helm/skaha/#license","title":"License","text":"<p>This project is licensed under the MIT License. For more information, refer to the LICENSE file in the repository.</p>"},{"location":"helm/sshd/","title":"Deployment Guilde","text":"<ul> <li>Dependencies</li> <li>Helm</li> </ul>"},{"location":"helm/sshd/#dependencies","title":"Dependencies","text":"<ul> <li>An existing Kubernetes cluster, version 1.26 or greater.</li> <li>A working <code>base</code> Helm Chart install.  If using Traefik, add a port (entry point) that this SSHD service will expose, which will be declared in the <code>traefik.ports</code> section.  Example:   <pre><code># Install Traefik by default.  Set to true to add it in.  Omitting it defaults to true, so beware.\ntraefik:\n  install: true\n  ports:\n    sshd:\n      port: 64022  # Expose port 64022.\n      expose: true\n</code></pre></li> <li>A <code>PersistentVolumeClaim</code> claiming storage that contains the root of the User Storage.  This will be the same <code>PersistentVolumeClaim</code> that Cavern uses (if installed).  See</li> <li>A Kubernetes secret called <code>sssd-ldap-secret</code> in the Skaha Namespace (defaults to <code>skaha-system</code>) with a single key of <code>ldap-password</code> whose value is the password of the LDAP bind user as configured in the <code>values.yaml</code> file for (<code>deployment.sshd.ldap.bindDN</code>):</li> <li><code>kubectl -n skaha-system create secret generic sssd-ldap-secret --from-literal=\"ldap-password=my-super-secret-passwd\"</code></li> </ul>"},{"location":"helm/sshd/#sample-values-file","title":"Sample Values file","text":"<pre><code>deployment:\n  sshd:\n    entryPoint: sshd\n    rootPath: \"/cavern\"  # If Cavern is installed, this will point to the same location as deployment.cavern.filesystem.subPath.\n\n    # LDAP configuration information.  Authentication is handled by the secret/sssd.conf file.\n    ldap:\n      url: \"ldaps://my-ldap-host.example.org\"\n      searchBase: \"dc=exmaple,dc=org\"\n      userSearchBase: \"ou=users,ou=ds,dc=example,dc=org\"\n      groupSearchBase: \"ou=groups,ou=ds,dc=example,dc=org\"\n      bindDN: \"uid=superuser,ou=Admins,dc=example,dc=org\"\n\nstorage:\n  service:\n    spec:\n      persistentVolumeClaim:\n        claimName: skaha-pvc # Match this label up with whatever was installed in the base install, or the desired PVC, or create dynamically provisioned storage.\n</code></pre>"},{"location":"helm/storage-ui/","title":"Helm Chart for the User Storage user interface","text":"<p>See the Deployment Guide for a better idea of the underlying APIs.</p>"},{"location":"helm/storage-ui/#dependencies","title":"Dependencies","text":"<ul> <li>An existing Kubernetes cluster.</li> <li>An IVOA Registry (See the Current SKAO Registry)</li> <li>A working Cavern (User Storage) system</li> </ul>"},{"location":"helm/storage-ui/#install","title":"Install","text":"<p>The Science Portal is a Single Page Application (SPA) with a rich Javascript client and DOM manager.  It uses React to power the various Dashboard elements, and is configurable for different OpenID Providers (OIdP).</p>"},{"location":"helm/storage-ui/#minimum-helm-configuration","title":"Minimum Helm configuration","text":"<p>See the full set of options in the values.yaml.  The deployed Redirect URI (<code>redirect_uri</code>) is <code>/storage-ui/oidc-callback</code>, which handles receiving the <code>code</code> as part of the authorization code flow, and obtaining a token to put into a cookie.</p> <p><code>my-storage-ui-local-values-file.yaml</code> <pre><code># @param securityContext - Optional security context for the container.  This is a security feature to restrict system calls.\n# securityContext: {}\n#\n# Example:\n# securityContext:\n#   seccompProfile:\n#     type: RuntimeDefault\n\n# @param applicationName - The name of the application.  This will rename the underlying WAR file, thus changing the endpoint.  Defaults to storage.\n# applicationName: storage\n\ndeployment:\n  hostname: example.com # Change this!\n  storageUI:\n    # OIDC (IAM) server configuration.  These are required\n    oidc:\n      # Location of the OpenID Provider (OIdP), and where users will login\n      uri: https://ska-iam.stfc.ac.uk/\n\n      # The Client ID as listed on the OIdP.  Create one at the uri above.\n      clientID: &lt;myclientid&gt;\n\n      # The Client Secret, which should be generated by the OIdP.\n      clientSecret: &lt;myclientsecret&gt;\n\n      # Used instead of clientSecret above.  Useful to avoid setting the clientSecret explicitly.\n      existingSecretName: &lt;kubernetes-secret-name&gt;\n\n      # Where the OIdP should send the User after successful authentication.  This is also known as the redirect_uri in OpenID.  This URI NEEDS\n      redirectURI: https://example.com/storage/oidc-callback\n\n      # Where to redirect to after the redirectURI callback has completed.  This will almost always be the URL to the /storage-ui main page (https://example.com/storage-ui).\n      callbackURI: https://example.com/storage/list\n\n      # The standard OpenID scopes for token requests.  This is required, and if using the SKAO IAM, can be left as-is.\n      scope: \"openid profile offline_access\"\n\n    # ID (URI) of the GMS Service.\n    gmsID: ivo://skao.int/gms\n\n    # Backend services\n    backend:\n      defaultService: cavern\n      services:\n        cavern:\n          resourceID: \"ivo://example.org/cavern\"\n          nodeURIPrefix: \"vos://example.org~cavern\" # How VOSpace URIs will be prefixed\n          userHomeDir: \"/home\"\n\n          # Link to Manage Groups link\n          groupManagementURI: \"https://example.org/groups\"\n\n          # Some VOSpace services support these features.  Cavern does not, but it needs to be explicitly declared here.\n          features:\n            batchDownload: false\n            batchUpload: false\n            externalLinks: false\n            paging: false\n            directDownload: false\n\n    # Specify the SRC theme.\n    themeName: src\n\n    # Optionally mount a custom CA certificate\n    # extraVolumeMounts:\n    # - mountPath: \"/config/cacerts\"\n    #   name: cacert-volume\n\n    # Create the CA certificate volume to be mounted in extraVolumeMounts\n    # extraVolumes:\n    # - name: cacert-volume\n    #   secret:\n    #     defaultMode: 420\n    #     secretName: storage-ui-cacert-secret\n\n# secrets:\n  # Uncomment to enable local or self-signed CA certificates for your domain to be trusted.\n  # storage-ui-cacert-secret:\n    # ca.crt: &lt;base64 encoded ca.crt blob&gt;\n</code></pre></p>"},{"location":"helm/storage-ui/#run-with-configured-values","title":"Run with configured values","text":"<pre><code>helm repo add science-platform-client https://images.opencadc.org/chartrepo/client\nhelm repo update\n\nhelm install -n skaha-system --values my-storage-ui-local-values-file.yaml storageui science-platform-client/storageui\n\nRelease \"storageui\" has been installed. Happy Helming!\nNAME: storageui\nLAST DEPLOYED: Thu Jan 12 17:01:07 2024\nNAMESPACE: skaha-system\nSTATUS: deployed\nREVISION: 1\nTEST SUITE: None\n</code></pre>"},{"location":"helm/storage-ui/#authentication-authorization","title":"Authentication &amp; Authorization","text":"<p>A&amp;A is handle by caching the Token Set server side and issuing a cookie to the browser to enable secure retrieval.  See the Application Authentication Documentation.</p>"},{"location":"helm/storage-ui/#endpoints","title":"Endpoints","text":"<p>The system will be available at the <code>/storage</code> endpoint, (i.e. https://example.com/storage/list).  Authenticating to the system is optional.</p>"},{"location":"helm/utils/","title":"Science Platform Helm Utility library (0.1.0)","text":"<p>A small Helm Library Chart to provide common utility functions to other Charts.</p>"},{"location":"helm/utils/#install","title":"Install","text":"<p>Add to your Chart dependencies from within the <code>helm</code> folder:</p> <pre><code>  - name: \"utils\"\n    version: \"^0.1.0\"\n    repository: \"file://../utils\"\n</code></pre>"},{"location":"helm/utils/#functions","title":"Functions","text":""},{"location":"helm/utils/#getsecretkeyvalue","title":"getSecretKeyValue","text":"<p>The <code>getSecretKeyValue</code> function in the _get-secret-key-value.yaml file contains a function to read a Kubernetes Secret, and extract a value for the given <code>key</code>.</p>"},{"location":"helm/utils/#example","title":"Example","text":"<pre><code>{{- $clientSecret := include \"getSecretKeyValue\" (list $existingSecretName \"clientSecret\" $namespace) -}}\n</code></pre>"},{"location":"operations/","title":"Operations","text":"<p>Operational guides and runbooks for managing CANFAR platform infrastructure, releases, and deployments.</p>"},{"location":"operations/#overview","title":"Overview","text":"<p>This section provides comprehensive documentation for platform operators managing CANFAR deployments. Whether you're releasing new versions, troubleshooting issues, or maintaining infrastructure, these guides will help you follow best practices and ensure reliable operations.</p> <ul> <li> CI/CD Pipelines GitHub Actions workflows</li> <li> Release Process Release playbook and procedures</li> </ul>"},{"location":"operations/#key-responsibilities","title":"Key Responsibilities","text":""},{"location":"operations/#release-management","title":"Release Management","text":"<ul> <li>Coordinate releases using Release Please automation</li> <li>Review and merge release PRs with proper approvals</li> <li>Monitor post-release workflows and verify deployments</li> <li>Manage hotfixes and rollback procedures when needed</li> </ul>"},{"location":"operations/#infrastructure-operations","title":"Infrastructure Operations","text":"<ul> <li>Deploy Helm charts and configuration overlays</li> <li>Manage environment-specific configurations (staging, production)</li> <li>Monitor platform health and respond to incidents</li> <li>Maintain secrets and access controls</li> </ul>"},{"location":"operations/#cicd-maintenance","title":"CI/CD Maintenance","text":"<ul> <li>Keep GitHub Actions workflows up to date</li> <li>Monitor workflow runs and troubleshoot failures</li> <li>Update documentation and configuration files</li> </ul>"},{"location":"operations/#tools-technologies","title":"Tools &amp; Technologies","text":"<p>The CANFAR deployment infrastructure relies on:</p> <ul> <li>Kubernetes - Container orchestration platform</li> <li>Helm - Package manager for Kubernetes applications</li> <li>GitHub Actions - CI/CD automation and workflows</li> <li>Release Please - Automated release management and changelog generation</li> <li>MkDocs Material - Documentation site generation</li> <li>uv - Python package and dependency management</li> </ul>"},{"location":"operations/#getting-help","title":"Getting Help","text":"<p>For operational support or questions:</p> <ul> <li>Check the relevant runbook in this documentation</li> <li>Review recent GitHub Actions workflow runs for error logs</li> <li>Contact the CADC operations team</li> <li>Consult the main CANFAR documentation</li> </ul>"},{"location":"operations/#best-practices","title":"Best Practices","text":"<ol> <li>Always follow the release checklist - Skip no steps to ensure consistent, reliable releases</li> <li>Test in staging first - Validate changes in staging before promoting to production</li> <li>Monitor post-deployment - Watch metrics and logs after every deployment</li> <li>Document incidents - Capture lessons learned and update runbooks</li> <li>Keep secrets secure - Rotate credentials regularly and limit access</li> <li>Maintain audit trails - All changes go through pull requests with proper reviews</li> </ol>"},{"location":"operations/ci-cd/","title":"CI/CD Pipelines","text":"<p>The deployments repository uses GitHub Actions to automate documentation and code quality tasks.</p>"},{"location":"operations/ci-cd/#documentation-deployment-docsyml","title":"Documentation Deployment (<code>docs.yml</code>)","text":"<p>Automatically builds and deploys the MkDocs documentation site to GitHub Pages.</p>"},{"location":"operations/ci-cd/#triggers","title":"Triggers","text":"<ul> <li>Pushes to <code>main</code> that modify <code>docs/**</code>, <code>mkdocs.yml</code>, or <code>pyproject.toml</code></li> <li>Manual dispatch with required reason field</li> </ul>"},{"location":"operations/ci-cd/#workflow-steps","title":"Workflow Steps","text":"<ol> <li>Checkout: Fetches full git history (required for git-revision-date plugin)</li> <li>Install uv: Sets up the uv package manager</li> <li>Setup Python: Installs Python using uv</li> <li>Install dependencies: Runs <code>uv sync</code> to install all dependencies from <code>pyproject.toml</code></li> <li>Deploy: Runs <code>uv run mkdocs gh-deploy --force</code> to build and publish to <code>gh-pages</code> branch</li> </ol>"},{"location":"operations/ci-cd/#requirements","title":"Requirements","text":"<ul> <li><code>contents: write</code> permission for pushing to <code>gh-pages</code> branch</li> <li>Dependencies managed in <code>pyproject.toml</code>:</li> <li><code>mkdocs-material</code> - Material theme for MkDocs</li> <li><code>mkdocs-git-revision-date-localized-plugin</code> - Git revision dates in docs</li> </ul>"},{"location":"operations/ci-cd/#pre-commit-checks-pre-commityml","title":"Pre-commit Checks (<code>pre-commit.yml</code>)","text":"<p>Runs pre-commit hooks on all files to ensure code quality and consistency.</p>"},{"location":"operations/ci-cd/#triggers_1","title":"Triggers","text":"<ul> <li>Pull requests to <code>main</code></li> <li>Manual dispatch</li> </ul>"},{"location":"operations/ci-cd/#what-it-checks","title":"What it checks","text":"<ul> <li>YAML syntax and formatting</li> <li>JSON formatting</li> <li>File permissions and naming</li> <li>Security scanning for hardcoded secrets</li> <li>Python code quality (if applicable)</li> </ul>"},{"location":"operations/release-process/","title":"Release Process","text":"<p>Documentation for releasing Helm charts and platform configuration for the CANFAR Science Platform.</p>"},{"location":"operations/release-process/#canfar-release-cycle","title":"CANFAR Release Cycle","text":"<p>The CANFAR Science Platform uses a version format <code>YYYY.Q</code> for quarterly releases:</p> <ul> <li>2025.1 - Q1 2025 release</li> <li>2025.2 - Q2 2025 release</li> </ul> <p>Between releases, hotfix patches may be released as needed for critical issues.</p>"},{"location":"operations/release-process/#helm-chart-releases","title":"Helm Chart Releases","text":"<p>Each Helm chart in this repository is versioned independently:</p> <ul> <li>Charts follow semantic versioning (<code>MAJOR.MINOR.PATCH</code>)</li> <li>Release Please automation manages changelog and version bumps</li> <li>Each chart has its own release cycle and <code>CHANGELOG.md</code></li> </ul>"},{"location":"operations/release-process/#branching-model","title":"Branching Model","text":"<ul> <li><code>main</code> is the integration branch - all work merges via pull requests</li> <li>Use conventional commits for automatic changelog generation</li> <li>Hotfixes branch from the latest release tag and merge back to <code>main</code></li> </ul>"},{"location":"operations/release-process/#release-workflow","title":"Release Workflow","text":""},{"location":"operations/release-process/#1-making-changes","title":"1. Making Changes","text":"<ul> <li>Create a pull request to <code>main</code></li> <li>Use conventional commit messages (e.g., <code>feat:</code>, <code>fix:</code>, <code>docs:</code>)</li> <li>Add appropriate labels for changelog categorization</li> <li>Ensure all CI checks pass</li> </ul>"},{"location":"operations/release-process/#2-release-please-automation","title":"2. Release Please Automation","text":"<p>Release Please automatically:</p> <ul> <li>Detects changes to Helm charts</li> <li>Determines version bump based on conventional commits</li> <li>Updates <code>CHANGELOG.md</code> and <code>Chart.yaml</code></li> <li>Creates a release PR for each affected chart</li> </ul>"},{"location":"operations/release-process/#3-review-and-merge","title":"3. Review and Merge","text":"<ul> <li>Review the generated changelog and version bump</li> <li>Verify Helm chart values and configuration</li> <li>Obtain required approvals</li> <li>Merge the release PR to create tags and GitHub releases</li> </ul>"},{"location":"operations/release-process/#4-deployment","title":"4. Deployment","text":"<p>After release PR is merged:</p> <ul> <li>Git tag is created automatically</li> <li>GitHub release is published with changelog</li> <li>Deploy to staging environment first</li> <li>Run validation and smoke tests</li> <li>Deploy to production after successful validation</li> </ul>"},{"location":"operations/release-process/#hotfix-process","title":"Hotfix Process","text":"<p>For critical issues requiring immediate patches:</p> <ol> <li>Create <code>hotfix/&lt;issue&gt;</code> branch from affected release tag</li> <li>Apply fix and create PR to <code>main</code></li> <li>Release Please generates patch release PR</li> <li>Follow standard review and merge process</li> <li>Deploy hotfix after testing</li> </ol>"},{"location":"operations/release-process/#pre-deployment-checklist","title":"Pre-deployment Checklist","text":"<p>Before deploying a Helm chart release:</p> <ul> <li>\u2705 Review CHANGELOG for all changes</li> <li>\u2705 Verify chart values match intended configuration</li> <li>\u2705 Check for breaking changes or migrations</li> <li>\u2705 Ensure dependent services are compatible</li> <li>\u2705 Prepare rollback plan</li> </ul>"},{"location":"operations/release-process/#rollback-strategy","title":"Rollback Strategy","text":"<p>If issues are detected after deployment:</p> <ol> <li>Roll back to previous Helm chart version</li> <li>Document issue and root cause</li> <li>Create hotfix branch to address problem</li> <li>Follow hotfix process for patch release</li> </ol>"}]}