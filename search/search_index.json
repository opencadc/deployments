{"config":{"lang":["en"],"separator":"[\\s\\u200b\\-_,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"CANFAR Deployments","text":"<p>Welcome to the operational documentation for deploying and maintaining the CANFAR Science Platform. This repository contains Helm charts, Kubernetes configurations, CI/CD automation, and operations runbooks that power the platform infrastructure.</p>"},{"location":"#what-youll-find-here","title":"What You'll Find Here","text":"<p>This documentation is designed for platform operators, DevOps engineers, and infrastructure maintainers who deploy, configure, and manage CANFAR services.</p> <ul> <li> <p> Helm Charts \u2192</p> <p>Reusable deployment templates for CANFAR services with configurable values and environment overlays.</p> </li> <li> <p> Release Automation \u2192</p> <p>Automated CI/CD pipelines using GitHub Actions and Release Please for Helm chart versioning.</p> </li> <li> <p> Operations Runbooks \u2192</p> <p>Step-by-step procedures for releases, rollbacks, monitoring, and troubleshooting production deployments.</p> </li> <li> <p> Configuration Management</p> <p>Environment-specific overlays, secrets management, and Kubernetes resource definitions for staging and production.</p> </li> <li> <p> Documentation</p> <p>Automated MkDocs site deployment with operations guides and platform runbooks.</p> </li> <li> <p> Code Quality</p> <p>Pre-commit hooks, linting, and security scanning for infrastructure-as-code.</p> </li> </ul>"},{"location":"#quick-links","title":"Quick Links","text":"<ul> <li>Helm Charts Documentation - Explore reusable Helm charts for CANFAR services</li> <li>CI/CD Pipelines - Understand GitHub Actions workflows for documentation and code quality</li> <li>Release Process - Follow the Helm chart release process and CANFAR platform schedule</li> <li>GitHub Repository - Browse source code, Helm charts, and configurations</li> </ul>"},{"location":"#getting-started","title":"Getting Started","text":"<p>If you're new to CANFAR deployments:</p> <ol> <li>Familiarize yourself with the Helm Charts Documentation to understand service deployments</li> <li>Review the Release Process to understand our deployment workflow</li> <li>Explore the CI/CD Pipelines documentation to see how automation works</li> <li>Check the GitHub repository for Helm charts and configuration files</li> </ol>"},{"location":"#contributing","title":"Contributing","text":"<p>This is an operational repository for CANFAR platform infrastructure. Changes follow the standard pull request workflow with Release Please automation for versioning and changelog generation.</p> <p>For questions or support, contact the CADC operations team or visit the main CANFAR documentation.</p>"},{"location":"helm/","title":"Helm Charts Documentation","text":"<p>This section provides comprehensive documentation for deploying and managing Helm charts for software services provided by the Candan Astronomy Data Centre (CADC).</p>"},{"location":"helm/#canfar-science-platform","title":"CANFAR Science Platform","text":"<p>The CANFAR Science Platform Helm charts enable the deployment of a scalable, user-friendly environment for astronomers to analyze and visualize data. The platform includes components for user session management, data storage, and interactive analysis tools.  Get started now \u2192</p>"},{"location":"helm/core/","title":"CADC/CANFAR Core Helm Charts","text":"<p>A compilation of common services and components that are shared across the CADC and CANFAR platforms. These Helm charts provide the foundational infrastructure and services required for deploying and managing the various applications within the platform.</p>"},{"location":"helm/core/#explore-the-core-services","title":"Explore the Core Services","text":"Discover the configurable values for each Helm Chart <ul> <li> Registry (<code>reg</code>) to install an IVOA Registry service for service discovery</li> </ul>"},{"location":"helm/core/reg/","title":"IVOA Registry Helm Chart","text":"<p>The Registry (<code>reg</code>) Helm chart facilitates the deployment of the Registry application within a Kubernetes cluster. This chart is designed to streamline the installation and management of the Registry service, ensuring a seamless integration into your Kubernetes environment.</p>"},{"location":"helm/core/reg/#prerequisites","title":"Prerequisites","text":"<p>Before deploying the Registry Helm chart, ensure that the following conditions are met:</p> <ul> <li>Kubernetes Cluster: A running Kubernetes cluster, version 1.29 or higher.</li> <li>Helm: Helm package manager, version 3, installed on your machine. Refer to the official Helm documentation for installation instructions.</li> </ul>"},{"location":"helm/core/reg/#installation","title":"Installation","text":"<p>To deploy the Registry application using the Helm chart, follow these steps:</p> <ol> <li> <p>Add the Registry Helm Repository: <pre><code>helm repo add registry-repo https://images.opencadc.org/chartrepo/core\n</code></pre></p> </li> <li> <p>Update Helm Repositories: <pre><code>helm repo update\n</code></pre></p> </li> <li> <p>Install the Registry Chart: <pre><code>helm --namespace cadc-core upgrade --install --values &lt;your-registry-values.yaml&gt; registry-release registry-repo/reg\n</code></pre> Replace <code>registry-release</code> with your desired release name.</p> </li> </ol>"},{"location":"helm/core/reg/#configuration","title":"Configuration","text":"<p>The Registry Helm chart comes with a default configuration suitable for most deployments. However, you can customize the installation by providing your own <code>values.yaml</code> file. This allows you to override default settings such as resource allocations, environment variables, and other parameters.</p> <p>To customize the installation:</p> <ul> <li>Create a <code>values.yaml</code> File: Define your custom configurations in this file.</li> <li>Install the Chart with Custom Values: <pre><code>helm --namespace cadc-core upgrade --install --values values.yaml registry-release registry-repo/reg\n</code></pre></li> </ul> <p>This Helm Chart supports both Pod and Container level security contexts. You can enable or disable these features based on your cluster's security requirements.  The Registry service need not run as root, so it is recommended to enable these security contexts for enhanced security.</p>"},{"location":"helm/core/reg/#example-valuesyaml-configuration","title":"Example <code>values.yaml</code> Configuration","text":"<pre><code>podSecurityContext:\n  runAsNonRoot: true\n  seccompProfile:\n    type: RuntimeDefault\n\nsecurityContext:\n  runAsUser: 10000\n  runAsGroup: 10000\n  allowPrivilegeEscalation: false\n  seccompProfile:\n    type: RuntimeDefault\n\nglobal:\n  hostname: example.org\n\napplication:\n  serviceEntries:\n    - id: ivo://example.org/services/service-1\n      url: https://example.org/services/service-1/capabilities\n    - id: ivo://example.org/services/service-2\n      url: https://example.org/services/service-2/capabilities\n\n  authority: ivo://example.org/authority\n\n# This section builds out the service account more information can be found here: https://kubernetes.io/docs/concepts/security/service-accounts/\nserviceAccount:\n  # Specifies whether a service account should be created\n  create: false\n  # Automatically mount a ServiceAccount's API credentials?\n  automount: false\n  # Annotations to add to the service account\n  annotations: {}\n  # The name of the service account to use.\n  # If not set and create is true, a name is generated using the fullname template\n  name: \"example-registry-service-account\"\n\n# Ingress is an quick way to get up and running, but httpRoute is preferred to make use of the Kubernetes Gateway API.  \n# If using Ingress, ensure that your cluster has an Ingress controller installed and configured to handle the specified className.\ningress:\n  enabled: true\n  className: traefik\n  hosts:\n  - host: example.org\n    paths:\n      - backend:\n          service:\n            name: reg\n            port:\n              number: 8080\n        path: /reg\n        pathType: Prefix\n</code></pre>"},{"location":"helm/core/reg/#uninstallation","title":"Uninstallation","text":"<p>To remove the Registry application from your cluster:</p> <pre><code>helm --namespace cadc-core uninstall registry-release\n</code></pre> <p>This command will delete all resources associated with the Registry release.</p>"},{"location":"helm/core/reg/#license","title":"License","text":"<p>This project is licensed under the MIT License. For more information, refer to the LICENSE file in the repository.</p>"},{"location":"helm/core/reg/#values-reference","title":"Values Reference","text":""},{"location":"helm/science-platform/","title":"CANFAR Science Platform Helm Chart Deployment","text":"<p>Guides and documentation for deploying and managing the CANFAR Science Platform using Helm charts. This section covers the structure, configuration options, and deployment procedures for the platform's Helm charts.</p>"},{"location":"helm/science-platform/#up-and-running","title":"Up and Running","text":"<p>To deploy the CANFAR Science Platform using Helm charts, follow the deployment guide which provides step-by-step instructions for setting up the platform in your Kubernetes environment.</p> <ul> <li> Get Started with the Deployment Guide Step-by-step deployment instructions \u2192</li> </ul>"},{"location":"helm/science-platform/#explore-the-science-platform-components","title":"Explore the Science Platform Components","text":"Discover the configurable values for each Helm Chart <ul> <li> Base to install core objects like an Ingress Controller and Service Accounts</li> <li> POSIX Mapper for UID and GID mapping</li> <li> Cavern for IVOA VOSpace storage</li> <li> Skaha to manage user sessions and workloads</li> <li> Science Portal interactive user sessions</li> <li> Storage UI manage Cavern storage from the Browser</li> </ul>"},{"location":"helm/science-platform/base/","title":"Base Helm Chart","text":""},{"location":"helm/science-platform/base/#install","title":"Install","text":""},{"location":"helm/science-platform/base/#dependencies","title":"Dependencies","text":"<p>Kubernetes 1.29 and up are supported.</p>"},{"location":"helm/science-platform/base/#from-source","title":"From source","text":"<p>The base install also installs the Traefik proxy, which is needed by the Ingress when the Science Platform services are installed.</p> <pre><code>$ git clone https://github.com/opencadc/science-platform.git\n$ cd science-platform/deployment/helm\n$ helm --namespace traefik upgrade --install --dependency-update --values ./base/values.yaml &lt;name&gt; ./base\n</code></pre> <p>Where <code>&lt;name&gt;</code> is the name of this installation.  Example: <pre><code>$ helm --namespace traefik upgrade --install --dependency-update --values ./base/values.yaml base ./base\n</code></pre> This will create the core namespace (<code>skaha-system</code>), and install the Traefik proxy dependency.  Expected output: <pre><code>NAME: base\nLAST DEPLOYED: &lt;Timestamp e.g. Mon Jun 30 10:39:04 2025&gt;\nNAMESPACE: skaha-system\nSTATUS: deployed\nREVISION: 4\nTEST SUITE: None\n</code></pre></p>"},{"location":"helm/science-platform/base/#from-the-canfar-repository","title":"From the CANFAR repository","text":"<p>The Helm repository contains the current stable version as well.</p> <pre><code>$ helm repo add canfar-skaha-system https://images.opencadc.org/chartrepo/platform\n$ helm repo update\n$ helm --namespace traefik upgrade --install --dependency-update --values canfar-skaha-system/base/values.yaml canfar-science-platform-base canfar-skaha-system/base\n</code></pre>"},{"location":"helm/science-platform/base/#verification","title":"Verification","text":"<p>After the install, there should exist the necessary Namespaces and Objects.  See the Namespaces:</p> <pre><code>$ kubectl get namespaces\nNAME                   STATUS   AGE\n...\nskaha-system           Active   28m\nskaha-workload         Active   28m\n</code></pre>"},{"location":"helm/science-platform/base/#proxy-using-traefik","title":"Proxy using Traefik","text":"<p>The Traefik proxy server is also installed as a dependency, which handles SSL termination.  Helm options are under the <code>traefik</code> key in the <code>values.yaml</code> file.</p> <p>You can create your own secrets to contain your self-signed server certificates to be used by the SSL termination.  See the <code>values.yaml</code> file for more, and don't forget to <code>base64</code> encode the values.</p>"},{"location":"helm/science-platform/base/#shared-storage","title":"Shared Storage","text":"<p>Shared Storage is handled by the <code>local</code> Persistent Volume types.</p> <pre><code>...\n  volumeMode: Filesystem\n  accessModes:\n    - ReadWriteMany\n  persistentVolumeReclaimPolicy: Delete\n  storageClassName: local-storage\n  local:\n    path: /data/skaha-storage\n...\n</code></pre>"},{"location":"helm/science-platform/base/#dns-on-macos","title":"DNS on macOS","text":"<p>The Docker VM on macOS cannot mount the NFS by default as it cannot do name resolution in the cluster.  It first needs to know about the <code>kube-dns</code> IP.  e.g.:</p> <pre><code>$ kubectl --namespace kube-system get service kube-dns\nNAME       TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)                  AGE\nkube-dns   ClusterIP   10.96.0.10   &lt;none&gt;        53/UDP,53/TCP,9153/TCP   4d23h\n</code></pre> <p>The <code>ClusterIP</code> needs to be known to the Docker VM's name resolution.  A simple way to do this is to mount the Docker VM root and modify it.  It will take effect immediately:</p> <pre><code>$ docker run --rm --interactive --tty --volume /:/vm-root alpine sh\n$ echo \"nameserver 10.96.0.10\" &gt;&gt; /vm-root/etc/resolv.conf\n$ cat /vm-root/etc/resolv.conf\n# DNS requests are forwarded to the host. DHCP DNS options are ignored.\nnameserver 192.168.65.7\nnameserver 10.96.0.10\n</code></pre>"},{"location":"helm/science-platform/base/#configuration","title":"Configuration","text":"Parameter Description Default <code>kubernetesClusterDomain</code> The DNS cluster domain <code>cluster.local</code> <code>skaha.namespace</code> The Namespace for the APIs (UIs and APIs) <code>skaha-system</code> <code>skahaWorkload.namespace</code> The Namespace for the User Sessions <code>skaha-workload</code> <code>secrets</code> Any secrets to be created at install. <code>{}</code> <code>traefik.install</code> Whether to install Traefik <code>false</code> <code>traefik</code> Traefik configuration options See Traefik Chart"},{"location":"helm/science-platform/browser-authentication/","title":"Authentication in browser based applications (Science Portal &amp; Storage User Interface)","text":"<ul> <li>OpenID Connect</li> <li>Login</li> <li>BFF Pattern</li> <li>After Successful Login</li> <li>Authenticated Requests<ul> <li>Flow</li> </ul> </li> </ul>"},{"location":"helm/science-platform/browser-authentication/#openid-connect","title":"OpenID Connect","text":"<p>Browser based applications use the Authorization Code flow to authenticate users to the OpenId Provider (OIdP).  A good example of how that works is shown at Medium.com, with <code>openid</code> included in the <code>scope</code> parameter.</p> <p>Once that flow succeeds, the OpenID Connect Client (the application) will have an Access Token (and Refresh Token) to use to make authenticated calls on behalf of the user to an API, such as Cavern or Skaha.</p> <p>As this flow is inefficient to use each time a request is made, the Access Token and Refresh Tokens are stored for the user, and retrieved when an authenticated call is necessary.  Access Tokens cannot be securely stored in the browser however, so a secure way of doing it is to implement the Backend For Frontend (BFF) pattern.</p>"},{"location":"helm/science-platform/browser-authentication/#login","title":"Login","text":"<p>Login is supplied by the OpenID Connect Provider, and the endpoint can be looked up using the JSON document at the <code>.well-known/openid-configuration</code> endpoint, and looking up the <code>authorization_endpoint</code> key.  To start the Authorization Code flow, redirect the user to the <code>authorization_endpoint</code> with the following <code>properties</code>:</p> Property Value <code>scope</code> <code>openid profile offline_access</code> <code>redirect_uri</code> <code>https://example.com/myapplication/oidc-callback</code> <code>response_type</code> <code>code</code> <code>client_id</code> <code>myclient_identifier</code> <p>Example:</p> <p>https://ska-iam.stfc.ac.uk/authorize?client_id=asfaslkfjlkj3-asdfdsdflkj&amp;scope=openid%20profile%20offline_access&amp;response_type=code&amp;redirect_uri=https%3A%2F%2Fexample.com%2Fmyapplication%2Foidc-callback</p> <p>If successful, this will call the URL at <code>redirect_uri</code> with a <code>code</code> parameter, containing a very short lived string value:</p> <p>https://example.com/myapplication/oidc-callback?code=sdfue887hdyr</p> <p>The <code>redirect_uri</code> endpoint can then pull the <code>code</code> query parameter, and use the <code>token_endpoint</code> from the <code>.well-known/openid-configuration</code> endpoint to exchange that <code>code</code> for tokens.  In order to do that, the client must authenticate with the same <code>client_id</code> used in the Login, as well as the <code>client_secret</code>, and POST the values.  The <code>client_secret</code> is typically generated by the client on registration.</p> <p>Example: <pre><code>final String codeFromCallbackURI = request.getParameter(\"code\");\nfinal ClientID clientID = new ClientID(this.clientID);\nfinal Secret clientSecret = new Secret(this.clientSecret);\nfinal AuthorizationCodeGrant codeGrant = new AuthorizationCodeGrant(codeFromCallbackURI);\n\n// Basic Authentication to obtain a Token from the IAM service.\nfinal ClientAuthentication clientAuth = new ClientSecretBasic(clientID, clientSecret);\n\nfinal URI tokenEndpoint = URI.create(Client.getTokenEndpoint().toExternalForm());\nfinal TokenRequest tokenRequest = new TokenRequest(tokenEndpoint, clientAuth, codeGrant);\n\n// Send the request for the token...\nfinal TokenResponse tokenResponse = sendTokenRequest(tokenRequest);\nfinal AccessTokenResponse tokenSuccessResponse = tokenResponse.toSuccessResponse();\n\n// We now have the Assets (Access Token, Refresh Token, and Expiry Time of Access Token)\nfinal Assets assets = new Assets(new JSONObject(tokenSuccessResponse.toJSONObject().toJSONString()));\n</code></pre></p> <p>Document returned from the <code>TokenRequest</code>: <pre><code>{\n  \"access_token\": \"MTQ0NjJkZmQ5OTM2NDE1ZTZjNGZmZjI3\",\n  \"expires_in\": 3600,\n  \"refresh_token\": \"IwOGYzYTlmM2YxOTQ5MGE3YmNmMDFkNTVk\",\n\n  \"id_token\": \"asklIILLdnsf9sdjsdfhkjhjh\" // Not actually used, but there if needed.\n}\n</code></pre></p> <p>The application now has what it needs to make authenticated calls to the API(s).  Let's look at how they're stored and used with the BFF Pattern.</p>"},{"location":"helm/science-platform/browser-authentication/#bff-pattern","title":"BFF Pattern","text":"<p>The UI applications use the Backend For Frontend (BFF) pattern to securely store tokens in a server-side cache, and can only be retrieved with an encrypted, HTTP-Only, and Secure, first-party cookie from the browser.  First-party cookies are obtained from a direct visit to the site, such as from a redirect, rather than from a request made from the page using JavaScript (third-party).  As browsers tighten security on cookies, this helps to future proof it.</p> <p>All OpenID Connect (OIDC) interaction is handled by the Nimbus OAuth2 Java Library.</p>"},{"location":"helm/science-platform/browser-authentication/#after-successful-login","title":"After Successful Login","text":"<p>The JSON document with a token set represents the Assets.  These Assets are stored in a Redis cache on the server, and a key is issued to retrieve them.  Each application has its own Token Cache.  The Assets are made up of the <code>access_token</code>, <code>refresh_token</code>, and the <code>expires_in</code> values.</p> <p>That returned Assets key is SHA-256 encrypted, and set in the browser in a secure cookie.  That cookie is only good for this application, and cannot be read by JavaScript (<code>http-only</code>).</p>"},{"location":"helm/science-platform/browser-authentication/#authenticated-requests","title":"Authenticated Requests","text":"<p>That encrypted cookie can now be used with the application to make authenticated requests.  The browser will send the cookie with each request, and follow the path as laid out in the diagram.</p> <p></p>"},{"location":"helm/science-platform/browser-authentication/#general-steps","title":"General Steps","text":"<ol> <li>User makes a request for a resource from a browser application</li> <li>If there is no first-party cookie, then proceed as though anonymous.  If the resource is protected, then a 401 or 403 status code is returned.</li> <li>For the Science Portal, this means denying access with a modal login box as authentication is required.</li> <li>For the Storage UI, this means producing a button to optionally authenticate, as public browsing is allowed for Public items.</li> <li>Decrypt the cookie if present, then use the key to look up the Assets in the Redis cache.</li> <li>Use the Access Token from the Assets as a Bearer token in the request header to the API.</li> </ol> <p>If the Access Token is valid (and the user is granted access to the resource), then the resource is returned.  The system will check the <code>expires_in</code> value to determine if the Access Token will soon expire, and if so, will request a refresh.</p> <ol> <li>If no Refresh Token is present in the Assets, the user needs to re-authenticate.</li> <li>If a Refresh Token is present in the Assets, then request a new Access Token from the token endpoint.</li> <li>If the Refresh Token is expired (i.e. 401 is returned from the OIdP), then the user needs to re-authenticate.</li> <li>Use the refreshed Access Token and return the resource to the user.</li> </ol>"},{"location":"helm/science-platform/cavern/","title":"Cavern Helm Chart","text":""},{"location":"helm/science-platform/cavern/#install","title":"Install","text":"<p>This <code>README</code> will focus on a basic install using a new <code>values-local.yaml</code> file.</p> <p>A working Science Platform is not required, but the Persistent Volume Claims are needed.  Those PVs and PVCs will provide the underlying storage for the Services and User Sessions.</p>"},{"location":"helm/science-platform/cavern/#from-source","title":"From source","text":"<p>Installation depends on a working Kubernetes cluster version 1.23 or greater.</p> <p>The base install also installs the Traefik proxy, which is needed by the Ingress when the Science Platform services are installed.</p> <pre><code>$ git clone https://github.com/opencadc/science-platform.git\n$ cd science-platform/deployment/helm\n$ helm install -n skaha-system --dependency-update --values my-values-local.yaml &lt;name&gt; ./cavern\n</code></pre> <p>Where <code>&lt;name&gt;</code> is the name of this installation.  Example: <pre><code>$ helm install -n skaha-system --dependency-update --values my-values-local.yaml cavern ./cavern\n</code></pre> This will install Skaha service dependency, as well as the Skaha webservice and any necessary Ingress. <pre><code>NAME: cavern\nLAST DEPLOYED: &lt;Timestamp e.g. Tue Nov 25 04:19:04 2025&gt;\nSTATUS: deployed\nREVISION: 1\nTEST SUITE: None\n</code></pre></p>"},{"location":"helm/science-platform/cavern/#verification","title":"Verification","text":"<p>After the install, there should exist the necessary Service.  See the Namespaces:</p> <pre><code>$ kubectl -n skaha-system get services\nNAME                   STATUS   AGE\n...\nskaha-system   cavern-tomcat-svc             ClusterIP      10.108.202.148   &lt;none&gt;        8080/TCP            1m\n</code></pre> <p>The IVOA VOSI availability endpoint can be used to check that the Skaha service has started properly.  It may take a few moments to start up.</p> <pre><code>$ curl https://myhost.example.com/cavern/availability\n\n&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n&lt;vosi:availability xmlns:vosi=\"http://www.ivoa.net/xml/VOSIAvailability/v1.0\"&gt;\n  &lt;vosi:available&gt;true&lt;/vosi:available&gt;\n  &lt;vosi:note&gt;service is accepting requests.&lt;/vosi:note&gt;\n  &lt;!--&lt;clientip&gt;192.1.1.4&lt;/clientip&gt;--&gt;\n&lt;/vosi:availability&gt;\n</code></pre>"},{"location":"helm/science-platform/cavern/#configuration","title":"Configuration","text":"Parameter Description Default <code>kubernetesClusterDomain</code> Kubernetes cluster domain used to find internal hosts <code>cluster.local</code> <code>replicaCount</code> Number of Cavern replicas to deploy <code>1</code> <code>tolerations</code> Array of tolerations to pass to Kubernetes for fine-grained Node targeting of the Cavern API <code>[]</code> <code>deployment.hostname</code> Hostname for the Cavern deployment <code>\"\"</code> <code>deployment.cavern.loggingGroups</code> List of groups permitted to adjust logging levels for the Cavern service. <code>[]</code> <code>deployment.cavern.image</code> Cavern Docker image <code>images.opencadc.org/platform/cavern:&lt;current release version&gt;</code> <code>deployment.cavern.imagePullPolicy</code> Image pull policy for the Cavern container <code>IfNotPresent</code> <code>deployment.cavern.resourceID</code> Resource ID (URI) for this Cavern service <code>\"\"</code> <code>deployment.cavern.oidcURI</code> URI (or URL) for the OIDC service <code>\"\"</code> <code>deployment.cavern.gmsID</code> Resource ID (URI) for the IVOA Group Management Service <code>\"\"</code> <code>deployment.cavern.adminAPIKeys</code> API keys for client applications that can create new allocations <code>{}</code> <code>deployment.cavern.allocations.defaultSizeGB</code> Default size of user allocations in GB <code>10</code> <code>deployment.cavern.allocations.parentFolders</code> List of parent folders to create for user allocations.  Best to leave this alone. <code>[\"/home\", \"/projects\"]</code> <code>deployment.cavern.filesystem.dataDir</code> Persistent data directory in the Cavern container <code>\"\"</code> <code>deployment.cavern.filesystem.subPath</code> Relative path to the node/file content that could be mounted in other containers <code>\"\"</code> <code>deployment.cavern.filesystem.rootOwner.username</code> Username of the root owner of the filesystem data (parent of allocations) directory <code>\"\"</code> <code>deployment.cavern.filesystem.rootOwner.uid</code> UID of the root owner of the filesystem data (parent of allocations) directory <code>\"\"</code> <code>deployment.cavern.filesystem.rootOwner.gid</code> GID of the root owner of the filesystem data (parent of allocations) directory <code>\"\"</code> <code>deployment.cavern.filesystem.rootOwner.adminUsername</code> Admin username for the filesystem data (parent of allocations) directory <code>deployment.cavern.identityManagerClass</code> Class name for the identity manager used by Cavern <code>org.opencadc.auth.StandardIdentityManager</code> <code>deployment.cavern.uws.db.install</code> Whether to deploy a local PostgreSQL database for UWS <code>true</code> <code>deployment.cavern.uws.db.image</code> PostgreSQL image to use for UWS <code>postgres:15.12</code> <code>deployment.cavern.uws.db.runUID</code> UID for the PostgreSQL user in the UWS database <code>999</code> <code>deployment.cavern.uws.db.database</code> Name of the UWS database <code>uws</code> <code>deployment.cavern.uws.db.url</code> JDBC URL for the UWS database.  Use instead of <code>database</code>. <code>jdbc:postgresql://cavern-uws-db:5432/uws</code> <code>deployment.cavern.uws.db.username</code> Username for the UWS database <code>uwsuser</code> <code>deployment.cavern.uws.db.password</code> Password for the UWS database <code>uwspwd</code> <code>deployment.cavern.uws.db.schema</code> Schema name for the UWS database <code>uws</code> <code>deployment.cavern.uws.db.maxActive</code> Maximum number of active connections to the UWS database <code>2</code> <code>deployment.applicationName</code> Optional rename of the application from the default \"cavern\" <code>cavern</code> <code>deployment.endpoint</code> Endpoint to serve the Cavern service from <code>/cavern</code> <code>deployment.extraEnv</code> Extra environment variables to set in the Cavern container <code>[]</code> <code>deployment.extraVolumeMounts</code> Extra volume mounts for the Cavern container <code>[]</code> <code>deployment.extraVolumes</code> Extra volumes to mount in the Cavern container <code>[]</code> <code>deployment.resources.requests.memory</code> Memory request for the Cavern container <code>1Gi</code> <code>deployment.resources.requests.cpu</code> CPU request for the Cavern container <code>500m</code> <code>deployment.resources.limits.memory</code> Memory limit for the Cavern container <code>1Gi</code> <code>deployment.resources.limits.cpu</code> CPU limit for the Cavern container <code>500m</code> <code>deployment.cavern.registryURL</code> (list OR string) <code>[]</code> IVOA Registry array of IVOA Registry locations or single IVOA Registry location <code>livenessProbe</code> Configure the liveness probe check <code>{}</code> <code>readinessProbe</code> Configure the readiness probe check <code>{}</code> <code>tolerations</code> Tolerations to apply to the Cavern Pod <code>[]</code> <code>secrets</code> Secrets to create for the Cavern service, such as CA certificates <code>{}</code> <code>service.cavern.extraPorts</code> Extra ports to expose for the Cavern service <code>[]</code> <code>storage.service.spec</code> Storage specification for the Cavern service <code>{}</code>"},{"location":"helm/science-platform/cavern/#user-allocations-with-special-access","title":"User Allocations with special access","text":""},{"location":"helm/science-platform/cavern/#note","title":"Note","text":"<p>The <code>admin-api-key</code> has admin level permissions.  Rotate them regularly, or keep the values file safe.</p> <p>Cavern typically accepts user allocation requests from the Administrative user, but it can be configured to allow other users to request allocations as well. This is done by adding the user's API key to the <code>deployment.cavern.adminAPIKeys</code> configuration: <pre><code>deployment:\n  cavern:\n    adminAPIKeys:\n      skaha: \"skahasecretkey1234567890\"\n      prepareData: \"preparedatasecretkey1234567890\"\n</code></pre></p> <p>With this configuration, listed clients can request new user allocations using the <code>admin-api-key</code> challenge type in the <code>Authorization</code> header.  This <code>admin-api-key</code> represents a trusted client application to act on behalf of the Administrative user: <pre><code>$ curl -Lv --header \"Authorization: admin-api-key prepareData:preparedatasecretkey1234567890\" --header \"content-type: text/xml\" --upload-file user-alloc-upload-jwt.xml https://example.org/cavern/nodes/home/new-user\n</code></pre></p> <p>Where the upload XML file would look like this: <pre><code>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n&lt;vos:node xmlns:vos=\"http://www.ivoa.net/xml/VOSpace/v2.0\"\n          xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n          uri=\"vos://exmaple.org~cavern/home/new-user\" xsi:type=\"vos:ContainerNode\"&gt;\n  &lt;vos:properties&gt;\n    &lt;vos:property uri=\"ivo://opencadc.org/vospace/core#creatorJWT\"&gt;JWT_TOKEN_REPLACE_ME&lt;/vos:property&gt; &lt;!-- JWT token of the new user --&gt;\n    &lt;vos:property uri=\"ivo://cadc.nrc.ca/vospace/core#inheritPermissions\"&gt;true&lt;/vos:property&gt;\n    &lt;vos:property uri=\"ivo://ivoa.net/vospace/core#quota\"&gt;524288000&lt;/vos:property&gt;. &lt;!-- 500MB quota example, adjust as needed --&gt;\n  &lt;/vos:properties&gt;\n  &lt;vos:nodes /&gt;\n&lt;/vos:node&gt;\n</code></pre> Where <code>JWT_TOKEN_REPLACE_ME</code> is replaced with a valid JWT token of the new user (i.e. the one making the request to be added).  Don't forget to set the <code>vos:property:uri</code> to the correct value for your service and the path of the new user.</p>"},{"location":"helm/science-platform/deployment/","title":"Deployment Guide","text":"<ul> <li>Deployment Guide<ul> <li>Dependencies</li> <li>Quick Start<ul> <li>Base install</li> <li>Persistent Volumes and Persistent Volume Claims</li> <li>POSIX Mapper install</li> <li>Cavern (User Storage API) install</li> <li>Kueue install</li> <li>Skaha install</li> <li>Science Portal User Interface install</li> <li>User Storage UI installation</li> </ul> </li> <li>Browser Authentication</li> <li>Obtaining a Bearer Token</li> <li>Flow</li> <li>Structure</li> </ul> </li> </ul>"},{"location":"helm/science-platform/deployment/#dependencies","title":"Dependencies","text":"<ul> <li>An existing Kubernetes cluster.</li> <li>An IVOA Service Registry deployment</li> </ul>"},{"location":"helm/science-platform/deployment/#quick-start","title":"Quick Start","text":"<pre><code>helm repo add science-platform https://images.opencadc.org/chartrepo/platform\nhelm repo add science-platform-client https://images.opencadc.org/chartrepo/client\nhelm repo update\n\nhelm upgrade --install -n traefik --create-namespace --values my-base-local-values-file.yaml base science-platform/base\nhelm upgrade --install -n skaha-system --values my-posix-mapper-local-values-file.yaml posix-mapper science-platform/posixmapper\nhelm upgrade --install -n skaha-system --values my-cavern-local-values-file.yaml cavern science-platform/cavern\nhelm upgrade --install -n skaha-system --values my-skaha-local-values-file.yaml skaha science-platform/skaha\nhelm upgrade --install -n skaha-system --values my-scienceportal-local-values-file.yaml science-portal science-platform/scienceportal\nhelm upgrade --install -n skaha-system --values my-storage-ui-local-values-file.yaml storage-ui science-platform-client/storageui\n</code></pre>"},{"location":"helm/science-platform/deployment/#base-install","title":"Base install","text":"<p>The Base install will create ServiceAccount, Role, Namespace, and RBAC objects needed to deploy the Skaha service, as such it requires cluster-admin privileges.</p> <p>Important</p> <p> </p> <p>The <code>base</code> chart itself is optional, but the objects it creates are required for the other services to operate correctly.  If you have already created the necessary Namespaces and RBAC objects, or if they have been created for you by your Cluster Administrator, you can skip this step.</p> <p>Create a <code>my-base-local-values-file.yaml</code> file to override Values from the main template <code>values.yaml</code> file.  Mainly the Traefik Default Server certificate (optional if needed):</p> <p><code>my-base-local-values-file.yaml</code> <pre><code>secrets:\n    default-certificate:\n        tls.crt: &lt;base64 encoded server certificate&gt;\n        tls.key: &lt;base64 encoded server key&gt;\n\n# Settings passed to Traefik.  The install flag is used by Helm to proceed to install Traefik or not.  If false, ensure v2.11.0 is at minimum installed.\ntraefik:\n  install: true\n  tlsStore:\n    default:\n      defaultCertificate:\n        # See default-certificate secret(s) above\n        secretName: default-certificate\n</code></pre></p> <pre><code>helm upgrade --install -n traefik --create-namespace --values my-base-local-values-file.yaml base science-platform/base\n\nNAME: base\nLAST DEPLOYED: Thu Nov 11 07:28:45 2025\nNAMESPACE: traefik\nSTATUS: deployed\nREVISION: 4\n</code></pre>"},{"location":"helm/science-platform/deployment/#persistent-volumes-and-persistent-volume-claims","title":"Persistent Volumes and Persistent Volume Claims","text":"<p>Important</p> <p>The <code>base</code> Helm Chart must be installed first as it creates the necessary Namespaces for the Persistent Volume Claims!</p> <p>There are two (2) Persistent Volume Claims that are used in the system, due to the fact that there are two (2) Namespaces (<code>skaha-system</code> and <code>skaha-workload</code>).  These PVCs, while having potentially different configurations, must point to the same storage.  For example, if two <code>hostPath</code> PVCs are created, the <code>hostPath.path</code> must point to the same folder in order to have shared content between the Cavern Service (<code>cavern</code>) and the User Sessions (Notebooks, CARTA, etc.).</p> <p>It is expected that the deployer, or an Administrator, will create the necessary Persistent Volumes (if needed), and the required Persistent Volume Claims at this point.  There are sample Local Storage Persistent Volume examples in the <code>base/volumes</code> folder.</p> <p>Two (2) Persistent Volume Claims are required.  While both point to the same underlying storage, they are in different Namespaces.  This leads to somewhat duplicated effort, but it is necessary to ensure that both the <code>skaha-system</code> and <code>skaha-workload</code> namespaces have access to the required storage resources. See this short explanation for more information.</p>"},{"location":"helm/science-platform/deployment/#posix-mapper-install","title":"POSIX Mapper install","text":"<p>The POSIX Mapper Service is required to provide a UID to Username mapping, and a GID to Group Name mapping so that any Terminal access properly showed System Users in User Sessions.  It will generate UIDs when a user is requested, or a GID when a Group is requested, and then keep track of them.</p> <p>This service is required to be installed before the Skaha service.</p> <p>Create a <code>my-posix-mapper-local-values-file.yaml</code> file to override Values from the main template <code>values.yaml</code> file.</p> <p><code>my-posix-mapper-local-values-file.yaml</code> <pre><code># POSIX Mapper web service deployment\ndeployment:\n  hostname: example.org\n  posixMapper:\n    # Optionally set the DEBUG port.\n    # extraEnv:\n    # - name: CATALINA_OPTS\n    #   value: \"-agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=0.0.0.0:5555\"\n    # - name: JAVA_OPTS\n    #   value: \"-agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=0.0.0.0:5555\"\n\n    # Uncomment to debug.  Requires options above as well as service port exposure below.\n    # extraPorts:\n    # - containerPort: 5555\n    #   protocol: TCP\n\n    # Resources provided to the Skaha service.\n    resources:\n      requests:\n        memory: \"1Gi\"\n        cpu: \"500m\"\n      limits:\n        memory: \"1Gi\"\n        cpu: \"500m\"\n\n    # Used to set the minimum UID.  Useful to avoid conflicts.\n    minUID: 10000\n\n    # Used to set the minimum GID.  Keep this much higher than the minUID so that default Groups can be set for new users.\n    minGID: 900000\n\n    # The URL of the IVOA Registry\n    registryURL: https://example.org/reg\n\n    # Optionally mount a custom CA certificate\n    # extraVolumeMounts:\n    # - mountPath: \"/config/cacerts\"\n    #   name: cacert-volume\n\n    # Create the CA certificate volume to be mounted in extraVolumeMounts\n    # extraVolumes:\n    # - name: cacert-volume\n    #   secret:\n    #     defaultMode: 420\n    #     secretName: posix-manager-cacert-secret\n\n  # Specify extra hostnames that will be added to the Pod's /etc/hosts file.  Note that this is in the\n  # deployment object, not the posixMapper one.\n  #\n  # These entries get added as hostAliases entries to the Deployment.\n  #\n  # Example:\n  # extraHosts:\n  #   - ip: 127.3.34.5\n  #     hostname: myhost.example.org\n  #\n  # extraHosts: []\n\nsecrets:\n  # Uncomment to enable local or self-signed CA certificates for your domain to be trusted.\n#   posix-manager-cacert-secret:\n#     ca.crt: &lt;base64 encoded ca crt&gt;\n\n# These values are preset in the catalina.properties, and this default database only exists beside this service.\npostgresql:\n#   maxActive: 4\n#   schema: mapping\n#   url: jdbc:postgresql://db.host:5432/mapping\n#   auth:\n#     username: posixmapperuser\n#     password: posixmapperpwd\n</code></pre></p> <pre><code>helm -n skaha-system upgrade --install  --values my-posix-mapper-local-values-file.yaml posix-mapper science-platform/posixmapper\n\nNAME: posix-mapper\nLAST DEPLOYED: Thu Oct 28 07:28:45 2025\nNAMESPACE: skaha-system\nSTATUS: deployed\nREVISION: 1\n</code></pre> <p>Test it. <pre><code># See below for tokens\nexport SKA_TOKEN=...\ncurl -SsL --header \"Authorization: Bearer ${SKA_TOKEN}\" https://example.host.com/posix-mapper/uid\n[]%\n\ncurl -SsL --header \"Authorization: Bearer ${SKA_TOKEN}\" \"https://example.host.com/posix-mapper/uid?user=mynewuser\"\nmynewuser:x:10000:10000:::\n</code></pre></p>"},{"location":"helm/science-platform/deployment/#cavern-user-storage-api-install","title":"Cavern (User Storage API) install","text":"<p>The Cavern API provides access to the User Storage which is shared between Skaha and all of the User Sessions.  A Bearer token is required when trying to read private access, or any writing.</p> <p>Create a <code>my-cavern-local-values-file.yaml</code> file to override Values from the main template <code>values.yaml</code> file.</p> <p><code>my-cavern-local-values-file.yaml</code> <pre><code># Cavern web service deployment\ndeployment:\n  hostname: example.org\n  cavern:\n    # How cavern identifies itself.  Required.\n    resourceID: \"ivo://example.org/cavern\"\n\n    # Set the Registry URL pointing to the desired registry.  Required\n    registryURL: \"https://example.org/reg\"\n\n    # How to find the POSIX Mapper API.  URI (ivo://) or URL (https://).  Required.\n    posixMapperResourceID: \"ivo://example.org/posix-mapper\"\n\n    # User Allocation settings.  This is used to create new folders under the main root allocation folders, namely /home and /projects.\n    allocations:\n      # Required.  The default size, in GB, of an allocation.  This is used in the absence of the Quota VOSpace property.  Can be a floating point number.\n      # Provided value is 10 (10 GiB) by default.\n      # Example:\n      #   defaultSizeGB: 25.5\n      defaultSizeGB: 25\n\n    filesystem:\n      # persistent data directory in container\n      dataDir: # e.g. \"/data\"\n\n      # RELATIVE path to the node/file content that could be mounted in other containers\n      subPath: # e.g. \"cavern\"\n\n      # See https://github.com/opencadc/vos/tree/master/cavern for documentation.  For deployments using OpenID Connect,\n      # the rootOwner MUST be an object with the following properties set.\n      rootOwner:\n        # The adminUsername is required to be set whomever has admin access over the filesystem.dataDir above.\n        adminUsername:\n\n        # The username of the root owner.\n        username:\n\n        # The UID of the root owner.\n        uid:\n\n        # The GID of the root owner.\n        gid:\n\n    # The API keys object that will be permitted to perform administrative tasks.  These will be passed as authorization headers to the Cavern API.\n    # The token values will be used by client applications, and each client matching a clientApplicationName should be configured with the matching token.\n    # Format is &lt;clientApplicationName&gt;: &lt;apiKeyToken&gt;\n    # Example:\n    #   adminAPIKeys:\n    #     skaha: \"token-value\"\n    #     prepareData: \"another-token-value\"\n    adminAPIKeys:\n      prepareData: \"32fjd93jfn93n3nFjsl293jfn93jf=\"\n      skaha: \"88shdj3en1rBuMVSllWVuuz190HJpF=\"\n\n    # Further UWS settings for the Tomcat Pool setup.  Set uws.db.install to false and set the uws.db.url property, with authentication.\n    uws:\n      db:\n        install: true\n        schema: uws\n        maxActive: 2\n\n    # Optional rename of the application from the default \"cavern\"\n    # applicationName: \"cavern\"\n\n    # The endpoint to serve this from.  Defaults to /cavern.  If the applicationName is changed, then this should match.\n    # Don't forget to update your registry entries!\n    #\n    # endpoint: \"/cavern\"\n\n    # Simple Class name of the QuotaPlugin to use.  This is used to request quota and folder size information\n    # from the underlying storage system.  Optional, defaults to NoQuotaPlugin.\n    #\n    # - For CephFS deployments: CephFSQuotaPlugin\n    # - Default: NoQuotaPlugin\n    #\n    # quotaPlugin: {NoQuotaPlugin | CephFSQuotaPlug}\n\n    # Optionally set the DEBUG port.\n    #\n    # Example:\n    # extraEnv:\n    # - name: CATALINA_OPTS\n    #   value: \"-agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=0.0.0.0:5555\"\n    # - name: JAVA_OPTS\n    #   value: \"-agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=0.0.0.0:5555\"\n    #\n    # extraEnv:\n\n    # Optionally mount a custom CA certificate\n    # Example:\n    # extraVolumeMounts:\n    # - mountPath: \"/config/cacerts\"\n    #   name: cacert-volume\n    #\n    # extraVolumeMounts:\n\n    # Create the CA certificate volume to be mounted in extraVolumeMounts\n    # Example:\n    # extraVolumes:\n    # - name: cacert-volume\n    #   secret:\n    #     defaultMode: 420\n    #     secretName: cavern-cacert-secret\n    #\n    # extraVolumes:\n\n    # Other data to be included in the main ConfigMap of this deployment.\n    # Of note, files that end in .key are special and base64 decoded.\n    #\n    # extraConfigData:\n\n    # Resources provided to the Cavern service.\n    resources:\n      requests:\n        memory: \"1Gi\"\n        cpu: \"500m\"\n      limits:\n        memory: \"1Gi\"\n        cpu: \"500m\"\n\n    # Optionally describe how this Pod will be scheduled using the nodeAffinity clause. This applies to Cavern.\n    # See https://kubernetes.io/docs/tasks/configure-pod-container/assign-pods-nodes-using-node-affinity/\n    # Example:\n    nodeAffinity:\n      # Only allow Cavern to run on specific Nodes.\n      requiredDuringSchedulingIgnoredDuringExecution:\n        nodeSelectorTerms:\n        - matchExpressions:\n          - key: kubernetes.io/hostname\n            operator: In\n            values:\n            - my-special-api-host\n\n  # Specify extra hostnames that will be added to the Pod's /etc/hosts file.  Note that this is in the\n  # deployment object, not the cavern one.\n  #\n  # These entries get added as hostAliases entries to the Deployment.\n  #\n  # Example:\n  # extraHosts:\n  #   - ip: 127.3.34.5\n  #     hostname: myhost.example.org\n  #\n  # extraHosts: []\n\n# secrets:\n  # Uncomment to enable local or self-signed CA certificates for your domain to be trusted.\n  # cavern-cacert-secret:\n  #   ca.crt: &lt;base64 encoded CA crt&gt;\n\n# Set these appropriately to match your Persistent Volume Claim labels.\nstorage:\n  service:\n    spec:\n      # YAML for service mounted storage.\n      # Example is the persistentVolumeClaim below.  This should match whatever Skaha used.\n      # persistentVolumeClaim:\n      #   claimName: skaha-pvc\n</code></pre></p>"},{"location":"helm/science-platform/deployment/#kueue-install","title":"Kueue install","text":"<p>Kueue is a Kubernetes-native job queuing and scheduling system that enhances the management of workloads in a Kubernetes cluster. It provides advanced features for job scheduling, resource management, and workload prioritization, making it particularly useful for environments where efficient resource utilization and job handling are critical. Kueue is optional, but highly recommended for production deployments.</p> <p>See https://kueue.sigs.k8s.io/docs/installation/#install-a-released-version for details.</p>"},{"location":"helm/science-platform/deployment/#helm","title":"Helm","text":"<p>Install CRDs and the various components: <pre><code>helm -n kueue-system install kueue oci://registry.k8s.io/kueue/charts/kueue --create-namespace\n</code></pre></p>"},{"location":"helm/science-platform/deployment/#clusterqueue","title":"ClusterQueue","text":"<p>A <code>ClusterQueue</code> is a global resource in Kueue that defines a set of resources and policies for managing workloads across the entire Kubernetes cluster. It serves as a template for creating and managing <code>LocalQueues</code>, which are specific to namespaces.</p> <p>An example <code>ClusterQueue</code> has been added to the Skaha Example Kueue ClusterQueue file.  Adjust as needed.</p> <p>Create the <code>ClusterQueue</code>, with the associated <code>ResourceFlavor</code> and <code>WorkloadPriorityClass</code> objects: <pre><code>cp ../skaha/kueue/examples/clusterQueue.config.yaml ./\n# Edit as needed.\n\nkubectl apply -f ./clusterQueue.config.yaml\n</code></pre></p>"},{"location":"helm/science-platform/deployment/#localqueue","title":"LocalQueue","text":"<p>A <code>LocalQueue</code> is a namespace-specific resource in Kueue that allows for the management of workloads within a particular Kubernetes namespace. It provides a way to define how jobs are queued, prioritized, and scheduled based on the policies set in the associated <code>ClusterQueue</code>.</p> <p>An example <code>LocalQueue</code> has been added to the Skaha Example Kueue LocalQueue file.  Adjust as needed, but it needs to exist in the workload namespace (<code>skaha-workload</code> by default).</p> <p>Create the <code>LocalQueue</code> in the <code>skaha-workload</code> namespace: <pre><code>cp ../skaha/kueue/examples/localQueue.config.yaml ./\n# Edit as needed.\n\nkubectl apply -f ./localQueue.config.yaml\n</code></pre></p>"},{"location":"helm/science-platform/deployment/#rbac","title":"RBAC","text":"<p>The Science Platform (<code>skaha</code>) requires certain RBAC permissions to operate correctly within the Kubernetes cluster. These permissions allow Kueue to manage resources, schedule jobs, and interact with other components in the cluster.</p> <p>An example <code>RBAC</code> configuration has been added to the Skaha Example Kueue RBAC file.  Adjust as needed. The example files contains rules to allow both the <code>skaha</code> system to query for the existence of any configured LocalQueues to ensure integrity, as well as permitting Jobs to be scheduled into it from the Workload Namespace.</p> <p>Create the <code>RBAC</code> objects in the <code>skaha-system</code> namespace: <pre><code>cp ../skaha/kueue/examples/rbac.yaml ./\nkubectl apply -f ./rbac.yaml\n</code></pre></p>"},{"location":"helm/science-platform/deployment/#skaha-install","title":"Skaha install","text":"<p>The Skaha service will manage User Sessions.  It relies on the POSIX Mapper being deployed, and available to be found via the IVOA Registry:</p> <p><code>/reg/resource-caps</code> <pre><code>...\n# Ensure the hostname matches the deployment hostname.\nivo://example.org/posix-mapper = https://example.host.com/posix-mapper/capabilities\n...\n</code></pre></p> <p>Create a <code>my-skaha-local-values-file.yaml</code> file to override Values from the main template <code>values.yaml</code> file.</p> <p><code>my-skaha-local-values-file.yaml</code> <pre><code># Skaha web service deployment\ndeployment:\n  hostname: example.org\n  skaha:\n    # Optionally set the DEBUG port.\n    # extraEnv:\n    # - name: CATALINA_OPTS\n    #   value: \"-agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=0.0.0.0:5555\"\n    # - name: JAVA_OPTS\n    #   value: \"-agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=0.0.0.0:5555\"\n\n    # Uncomment to debug.  Requires options above as well as service port exposure below.\n    # extraPorts:\n    # - containerPort: 5555\n    #   protocol: TCP\n\n    defaultQuotaGB: \"10\"\n\n    # Space delimited list of allowed Image Registry hosts.  These hosts should match the hosts in the User Session images.\n    registryHosts: \"images.canfar.net\"\n\n    # The IVOA GMS Group URI to verify users against for permission to use the Science Platform.\n    # See https://www.ivoa.net/documents/GMS/20220222/REC-GMS-1.0.html#tth_sEc3.2\n    usersGroup: \"ivo://example.org/gms?skaha-platform-users\"\n\n    # The IVOA GMS Group URI to verify images without contacting Harbor.\n    # See https://www.ivoa.net/documents/GMS/20220222/REC-GMS-1.0.html#tth_sEc3.2\n    adminsGroup: \"ivo://example.org/gms?skaha-admin-users\"\n\n    # The IVOA GMS Group URI to verify users against for permission to run headless jobs.\n    # See https://www.ivoa.net/documents/GMS/20220222/REC-GMS-1.0.html#tth_sEc3.2\n    headlessGroup: \"ivo://example.org/gms?skaha-headless-users\"\n\n    # The IVOA GMS Group URI to verify users against that have priority for their headless jobs.\n    # See https://www.ivoa.net/documents/GMS/20220222/REC-GMS-1.0.html#tth_sEc3.2\n    headlessPriorityGroup: \"ivo://example.org/gms?skaha-priority-headless-users\"\n\n    # Class name to set for priority headless jobs.\n    headlessPriorityClass: \"uber-user-vip\"\n\n    # Array of GMS Group URIs allowed to set the logging level.  If none set, then nobody can change the log level.\n    # See https://www.ivoa.net/documents/GMS/20220222/REC-GMS-1.0.html#tth_sEc3.2 for GMS Group URIs\n    # See https://github.com/opencadc/core/tree/main/cadc-log for Logging control\n    loggingGroups:\n      - \"ivo://example.org/gms?skaha-logging-admin-users\"\n\n    # The Resource ID (URI) of the Service that contains the Posix Mapping information\n    posixMapperResourceID: \"ivo://example.org/posix-mapper\"\n\n    # URI or URL of the OIDC (IAM) server.  Used to validate incoming tokens.\n    oidcURI: https://iam.example.org/\n\n    # The Resource ID (URI) of the GMS Service.\n    gmsID: ivo://example.org/gms\n\n    # The absolute URL of the IVOA Registry where services are registered\n    registryURL: https://example.org/reg\n\n    # Optionally describe how this Pod will be scheduled using the nodeAffinity clause. This applies to Skaha itself.\n    # Note the different indentation level compared to the sessions.nodeAffinity.\n    # See https://kubernetes.io/docs/tasks/configure-pod-container/assign-pods-nodes-using-node-affinity/\n    # See the [Sample Skaha Values file](skaha/sample-local-values.yaml).\n    # Example:\n    nodeAffinity:\n      # Only allow Skaha to run on specific Nodes.\n      requiredDuringSchedulingIgnoredDuringExecution:\n        nodeSelectorTerms:\n        - matchExpressions:\n          - key: kubernetes.io/hostname\n            operator: In\n            values:\n            - my-special-node-host\n\n    # Settings for User Sessions.  Sensible defaults supplied, but can be overridden.\n    # For units of storage, see https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#meaning-of-memory.\n    sessions:\n      expirySeconds: \"345600\"   # Duration, in seconds, until they expire and are shut down.\n      maxCount: \"3\"  # Max number of sessions per user.\n      minEphemeralStorage: \"20Gi\"   # The initial requested amount of ephemeral (local) storage.  Does NOT apply to Desktop sessions.\n      maxEphemeralStorage: \"200Gi\"  # The maximum amount of ephemeral (local) storage to allow a Session to extend to.  Does NOT apply to Desktop sessions.\n\n      # Optionally setup a separate host for User Sessions for Skaha to redirect to.  The HTTPS scheme is assumed.  Defaults to the Skaha hostname (.Values.deployment.hostname).\n      # Example:\n      #   hostname: myhost.example.org\n      hostname: sessions.example.org\n\n      # When set to 'true' this flag will enable GPU node scheduling.  Don't forget to declare any related GPU configurations, if appropriate, in the nodeAffinity below!\n      # gpuEnabled: false\n\n      # Optionally set the node label selector to identify Kubernetes Worker Nodes.  This is used to accurately query for available\n      # resources from schedulable Nodes by eliminating, for example, Nodes that are only cordoned for Web APIs.\n      # Example:\n      #   nodeLabelSelector: \"node-role.kubernetes.io/node-type=worker\"\n      #\n      # Example (multiple labels ANDed):\n      #   nodeLabelSelector: \"node-role.kubernetes.io/node-type=worker,environment=production\"\n      #\n      #\n      # Example (multiple labels ORed):\n      #   nodeLabelSelector: \"node-role.kubernetes.io/node-type in (worker,worker-gpu)\"\n      nodeLabelSelector:\n\n      userStorage:\n        persistentVolumeClaimName: skaha-workload-cavern-pvc\n        nodeURIPrefix: \"vos://canfar.net~src~cavern\"\n        serviceURI: \"ivo://canfar.net/src/cavern\"\n        admin:\n          auth:\n            apiKey: \"88shdj3en1rBuMVSllWVuuz190HJpF=\"\n\n      # Set the YAML that will go into the \"affinity.nodeAffinity\" stanza for Pod Spec in User Sessions.  This can be used to enable GPU scheduling, for example,\n      # or to control how and where User Session Pods are scheduled.  This can be potentially dangerous unless you know what you are doing.\n      # See https://kubernetes.io/docs/tasks/configure-pod-container/assign-pods-nodes-using-node-affinity\n      # nodeAffinity: {}\n\n      # Mount CVMFS from the Node's mounted path into all User Sessions.\n      extraVolumes:\n      - name: cvmfs-mount\n        volume:\n          type: HOST_PATH     # HOST_PATH is for host path\n          hostPath: \"/cvmfs\"  # Path on the Node to look for a source folder\n          hostPathType: Directory\n        volumeMount:\n          mountPath: \"/cvmfs\"   # Path to mount on the User Sesssion Pod.\n          readOnly: false\n          mountPropagation: HostToContainer\n\n      # Kueue configurations for User Sessions\n      kueue:\n        default:\n          # Ensure this name matches whatever was created as the LocalQueue in the workload namespace.\n          queueName: canfar-science-platform-local-queue\n          priorityClass: low\n\n    # Optionally mount a custom CA certificate as an extra mount in Skaha (*not* user sessions)\n    # extraVolumeMounts:\n    # - mountPath: \"/config/cacerts\"\n    #   name: cacert-volume\n\n    # Create the CA certificate volume to be mounted in extraVolumeMounts\n    # extraVolumes:\n    # - name: cacert-volume\n    #   secret:\n    #     defaultMode: 420\n    #     secretName: skaha-cacert-secret\n\n    # Other data to be included in the main ConfigMap of this deployment.\n    # Of note, files that end in .key are special and base64 decoded.\n    #\n    # extraConfigData:\n\n    # Resources provided to the Skaha service.\n    # For units of storage, see https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#meaning-of-memory.\n    resources:\n      requests:\n        memory: \"1Gi\"\n        cpu: \"500m\"\n      limits:\n        memory: \"1500Mi\"\n        cpu: \"750m\"\n\n  # Specify extra hostnames that will be added to the Pod's /etc/hosts file.  Note that this is in the\n  # deployment object, not the skaha one.\n  #\n  # These entries get added as hostAliases entries to the Deployment.\n  #\n  # Example:\n  # extraHosts:\n  #   - ip: 127.3.34.5\n  #     hostname: myhost.example.org\n  #\n  # extraHosts: []\n\n# Enable experimental feature flags\nexperimentalFeatures:\n  enabled: true\n  sessionLimitRange:\n    enabled: true\n    rbac:\n      create: false\n    limitSpec:\n      max:  # maximum allowed requested\n        memory: \"192Gi\"\n        cpu: \"16\"\n        nvidia.com/gpu: \"1\"\n      default:  # actually refers to default limits\n        memory: \"24Gi\"\n        cpu: \"4\"\n        nvidia.com/gpu: \"0\"\n      defaultRequest:  # default requests\n        memory: \"4Gi\"\n        cpu: \"1\"\n        nvidia.com/gpu: \"0\"\n\nsecrets:\n  # Uncomment to enable local or self-signed CA certificates for your domain to be trusted.\n#   skaha-cacert-secret:\n#     ca.crt: &lt;base64 encoded ca crt&gt;\n</code></pre></p> <pre><code>helm -n skaha-system upgrade --install --values my-skaha-local-values-file.yaml skaha science-platform/skaha\n\nNAME: skaha\nLAST DEPLOYED: Thu Oct 31 02:01:10 2025\nNAMESPACE: skaha-system\nSTATUS: deployed\nREVISION: 8\n</code></pre> <p>Test it. <pre><code># See below for tokens\nexport SKA_TOKEN=...\ncurl -SsL --header \"Authorization: Bearer ${SKA_TOKEN}\" https://example.host.com/skaha/v1/session\n[]%\n\n# xxxxxx is the returned session ID.\ncurl -SsL --header \"Authorization: Bearer ${SKA_TOKEN}\" -d \"ram=1\" -d \"cores=1\" -d \"image=images.canfar.net/canucs/canucs:1.2.5\" -d \"name=myjupyternotebook\" \"https://example.host.com/skaha/v1/session\"\n</code></pre></p>"},{"location":"helm/science-platform/deployment/#science-portal-user-interface-install","title":"Science Portal User Interface install","text":"<p>The Science Portal service will manage User Sessions.  It relies on the Skaha service being deployed, and available to be found via the IVOA Registry:</p> <p><code>/reg/resource-caps</code> <pre><code>...\n# Ensure the hostname matches the deployment hostname.\nivo://example.org/skaha = https://example.host.com/skaha/capabilities\n...\n</code></pre></p> <p>Create a <code>my-science-portal-local-values-file.yaml</code> file to override Values from the main template <code>values.yaml</code> file.</p> <p><code>my-science-portal-local-values-file.yaml</code> <pre><code>deployment:\n  hostname: example.org\n  sciencePortal:\n    # The Resource ID of the Service that contains the URL of the Skaha service in the IVOA Registry\n    skahaResourceID: ivo://example.org/skaha\n\n    # OIDC (IAM) server configuration.  These are required\n    # oidc:\n    #\n    # Location of the OpenID Provider (OIdP), and where users will login\n    #   uri: https://iam.example.org/\n\n      # The Client ID as listed on the OIdP.  Create one at the uri above.\n    #   clientID: my-client-id\n\n      # The Client Secret, which should be generated by the OIdP.\n    #   clientSecret: my-client-secret\n\n      # Where the OIdP should send the User after successful authentication.  This is also known as the redirect_uri in OpenID.\n    #   redirectURI: https://example.com/science-portal/oidc-callback\n\n      # Where to redirect to after the redirectURI callback has completed.  This will almost always be the URL to the /science-portal main page (https://example.com/science-portal).\n    #   callbackURI: https://example.com/science-portal/\n\n      # The standard OpenID scopes for token requests.  This is required, and if using the SKAO IAM, can be left as-is.\n    #   scope: \"openid profile offline_access\"\n\n    # Optionally mount a custom CA certificate\n    # extraVolumeMounts:\n    # - mountPath: \"/config/cacerts\"\n    #   name: cacert-volume\n\n    # Create the CA certificate volume to be mounted in extraVolumeMounts\n    # extraVolumes:\n    # - name: cacert-volume\n    #   secret:\n    #     defaultMode: 420\n    #     secretName: science-portal-cacert-secret\n\n    # The theme name for styling.\n    # src: The SRCNet theme\n    # canfar: The CANFAR theme for internal CADC deployment\n    # themeName: {src | canfar}\n\n    # Labels on the tabs\n    # Default:\n    # tabLabels:\n    #  - Public\n    #  - Advanced\n    # tabLabels: []\n\n    # Other data to be included in the main ConfigMap of this deployment.\n    # Of note, files that end in .key are special and base64 decoded.\n    #\n    # extraConfigData:\n\n    # Resources provided to the Science Portal service.\n    resources:\n      requests:\n        memory: \"1Gi\"\n        cpu: \"500m\"\n      limits:\n        memory: \"1500Mi\"\n        cpu: \"750m\"\n\n    # Optionally describe how this Pod will be scheduled using the nodeAffinity clause. This applies to Science Portal itself.\n    # See https://kubernetes.io/docs/tasks/configure-pod-container/assign-pods-nodes-using-node-affinity/\n    # Example:\n    nodeAffinity:\n      # Only allow Science Portal to run on specific Nodes.\n      requiredDuringSchedulingIgnoredDuringExecution:\n        nodeSelectorTerms:\n        - matchExpressions:\n          - key: kubernetes.io/hostname\n            operator: In\n            values:\n            - my-special-ui-host\n\nexperimentalFeatures:\n  enabled: true\n  slider:\n    enabled: true\n\n  # Specify extra hostnames that will be added to the Pod's /etc/hosts file.  Note that this is in the\n  # deployment object, not the sciencePortal one.\n  #\n  # These entries get added as hostAliases entries to the Deployment.\n  #\n  # Example:\n  # extraHosts:\n  #   - ip: 127.3.34.5\n  #     hostname: myhost.example.org\n  #\n  # extraHosts: []\n\n# secrets:\n  # Uncomment to enable local or self-signed CA certificates for your domain to be trusted.\n  # science-portal-cacert-secret:\n    # ca.crt: &lt;base64 encoded ca.crt blob&gt;\n</code></pre></p>"},{"location":"helm/science-platform/deployment/#user-storage-ui-installation","title":"User Storage UI installation","text":"<p>Create a <code>my-storage-ui-local-values-file.yaml</code> file to override Values from the main template <code>values.yaml</code> file.</p> <p><code>my-storage-ui-local-values-file.yaml</code> <pre><code>deployment:\n  hostname: example.org\n  storageUI:\n    # OIDC (IAM) server configuration.  These are required\n    oidc:\n      # Location of the OpenID Provider (OIdP), and where users will login\n      uri: https://iam.example.org/\n\n      # The Client ID as listed on the OIdP.  Create one at the uri above.\n      clientID: my-client-id\n\n      # The Client Secret, which should be generated by the OIdP.\n      clientSecret:  my-client-secret\n\n      # Where the OIdP should send the User after successful authentication.  This is also known as the redirect_uri in OpenID.\n      redirectURI: https://example.com/science-portal/oidc-callback\n\n      # Where to redirect to after the redirectURI callback has completed.  This will almost always be the URL to the /science-portal main page (https://example.com/science-portal).\n      callbackURI: https://example.com/science-portal/\n\n      # The standard OpenID scopes for token requests.  This is required, and if using the SKAO IAM, can be left as-is.\n      scope: \"openid profile offline_access\"\n\n    # ID (URI) of the GMS Service.\n    gmsID: ivo://example.org/gms\n\n    # Dictionary of all VOSpace APIs (Services) available that will be visible on the UI.\n    # Format is:\n    backend:\n      defaultService: cavern\n      services:\n        cavern:\n          resourceID: \"ivo://example.org/cavern\"\n          nodeURIPrefix: \"vos://example.org~cavern\"\n          userHomeDir: \"/home\"\n          # Some VOSpace services support these features.  Cavern does not, but it needs to be explicitly declared here.\n          features:\n            batchDownload: false\n            batchUpload: false\n            externalLinks: false\n            paging: false\n\n    # Optionally mount a custom CA certificate\n    # extraVolumeMounts:\n    # - mountPath: \"/config/cacerts\"\n    #   name: cacert-volume\n\n    # Create the CA certificate volume to be mounted in extraVolumeMounts\n    # extraVolumes:\n    # - name: cacert-volume\n    #   secret:\n    #     defaultMode: 420\n    #     secretName: storage-ui-cacert-secret\n\n    # Other data to be included in the main ConfigMap of this deployment.\n    # Of note, files that end in .key are special and base64 decoded.\n    #\n    # extraConfigData:\n\n    # Resources provided to the StorageUI service.\n    resources:\n      requests:\n        memory: \"1Gi\"\n        cpu: \"500m\"\n      limits:\n        memory: \"1500Mi\"\n        cpu: \"750m\"\n\n    # Optionally describe how this Pod will be scheduled using the nodeAffinity clause. This applies to the Storage UI Pod(s).\n    # See https://kubernetes.io/docs/tasks/configure-pod-container/assign-pods-nodes-using-node-affinity/\n    # Example:\n    nodeAffinity:\n      # Only allow Storage UI to run on specific Nodes.\n      requiredDuringSchedulingIgnoredDuringExecution:\n        nodeSelectorTerms:\n        - matchExpressions:\n          - key: kubernetes.io/hostname\n            operator: In\n            values:\n            - my-special-ui-host\n\n  # Specify extra hostnames that will be added to the Pod's /etc/hosts file.  Note that this is in the\n  # deployment object, not the storageUI one.\n  #\n  # These entries get added as hostAliases entries to the Deployment.\n  #\n  # Example:\n  # extraHosts:\n  #   - ip: 127.3.34.5\n  #     hostname: myhost.example.org\n  #\n  # extraHosts: []\n\n# secrets:\n  # Uncomment to enable local or self-signed CA certificates for your domain to be trusted.\n  # storage-ui-cacert-secret:\n    # ca.crt: &lt;base64 encoded ca.crt blob&gt;\n</code></pre></p>"},{"location":"helm/science-platform/deployment/#browser-authentication","title":"Browser Authentication","text":"<p>Encrypted cookies are used to gain access to a Bearer Token for API access.  These cookies are managed by the browser based applications (Storage UI and Science Portal).  See the Browser Authentication documentation for details.</p>"},{"location":"helm/science-platform/deployment/#obtaining-a-bearer-token","title":"Obtaining a Bearer Token","text":"<p>See the JIRA Confluence page on obtaining a Bearer Token.</p>"},{"location":"helm/science-platform/deployment/#flow","title":"Flow","text":"<p>The Skaha service depends on several installations being in place.</p>"},{"location":"helm/science-platform/deployment/#structure","title":"Structure","text":""},{"location":"helm/science-platform/posix-mapper/","title":"POSIX Mapper Helm Chart","text":"<p>This Helm chart deploys the POSIX Mapper application, which is designed to map POSIX file system operations to a cloud-native environment.</p>"},{"location":"helm/science-platform/posix-mapper/#prerequisites","title":"Prerequisites","text":"<ul> <li>Kubernetes 1.29+</li> <li>Helm 3.0+</li> <li>Deployed PostgreSQL database for application data storage (Sample installation instructions provided below)</li> </ul>"},{"location":"helm/science-platform/posix-mapper/#postgresql-database","title":"PostgreSQL Database","text":"<p>The POSIX Mapper requires a PostgreSQL database to store UID/GID mappings.  As this is a critical component, ensure that your database is properly configured and accessible from the POSIX Mapper application.  Use some persistent storage solution (like a Persistent Volume Claim) to ensure that the database data is not lost if deploying PostgreSQL in Kubernetes, or install a dedicated instance outside of the cluster.</p>"},{"location":"helm/science-platform/posix-mapper/#sample-postgresql-installation-in-kubernetes","title":"Sample PostgreSQL Installation (in Kubernetes)","text":"<p>You can deploy a PostgreSQL database using the following Helm chart, with a PVC to ensure data persistence (Using <code>skaha-system</code> namespace as an example):</p>"},{"location":"helm/science-platform/posix-mapper/#persistent-volume-claim-pvc","title":"Persistent Volume Claim (PVC)","text":"<p>Create a Persistent Volume Claim (PVC) for PostgreSQL: <pre><code>apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: posix-mapper-postgres-pvc\n  namespace: skaha-system\nspec:\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 2Gi\n  storageClassName: \"\"\n  selector:\n    matchLabels:\n      storage: posix-mapper-postgres-storage\n</code></pre></p> <p>This will need to match to a Persistent Volume (PV) that is available in your Kubernetes cluster.  An example PV could look like this for a CephFS instance in an OpenStack Share:</p> <pre><code>---\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: posix-mapper-postgres-pv\n  labels:\n    storage: posix-mapper-postgres-storage\nspec:\n  capacity:\n    storage: 2Gi\n  volumeMode: Filesystem\n  accessModes:\n    - ReadWriteMany\n  persistentVolumeReclaimPolicy: Delete\n  storageClassName: \"\"\n  cephfs:\n    monitors:\n    - 10.0.0.1:6789\n    - 10.0.0.2:6789\n    path: /volumes/myvolume\n    user: posix-mapper-postgres\n    readOnly: false\n    secretRef:\n      name: posix-mapper-postgres-secret\n      namespace: skaha-system\n</code></pre> <p>Ultimately, it will be up to the deployment to ensure that the PVC is bound to a suitable PV, and that the PV is available in the cluster.</p>"},{"location":"helm/science-platform/posix-mapper/#install-postgresql-using-helm","title":"Install PostgreSQL using Helm","text":"<pre><code># Version 12.12.10 is used as an example as it's the final release supporting PostgreSQL 15.x, which has been tested with the POSIX Mapper.\nhelm -n postgresql upgrade --install --create-namespace posix-mapper-postgresql --version 12.12.10 oci://registry-1.docker.io/bitnamicharts/postgresql\n</code></pre> <p>Use a Helm Values file to customize the installation.  This will initialize the database schema and set up the required user credentials.  The schema should match what the POSIX Mapper expects in its configuration. Create a file named <code>my-postgresql-values.yaml</code> with the following content: <pre><code>auth:\n  username: posixmapperuser\n  password: posixmapperpwd\n  database: posixmapper\nprimary:\n  initdb:\n    scripts:\n      init_schema.sql: |\n          create schema mapping;\n  persistence:\n    enabled: true\n    existingClaim: posix-mapper-postgres-pvc\n</code></pre></p> <p>Then run the following command to install PostgreSQL with the specified values: <pre><code>helm -n postgresql upgrade --install --create-namespace posix-mapper-postgresql --values my-postgresql-values.yaml --version 12.12.10 oci://registry-1.docker.io/bitnamicharts/postgresql\n</code></pre></p>"},{"location":"helm/science-platform/posix-mapper/#posix-mapper-installation","title":"POSIX Mapper Installation","text":"<p>To deploy the POSIX Mapper application using the Helm chart, follow these steps:</p> <ol> <li> <p>Add the Helm Repository <pre><code>helm repo add science-platform-repo https://images.opencadc.org/chartrepo/platform\nhelm repo update\n</code></pre></p> </li> <li> <p>Install the POSIX Mapper Chart: <pre><code>helm -n skaha-system --values &lt;myvalues.yaml&gt; install posix-mapper science-platform-repo/posix-mapper\n</code></pre></p> </li> </ol>"},{"location":"helm/science-platform/posix-mapper/#configuration","title":"Configuration","text":"<p>The POSIX Mapper Helm chart comes with some default configuration suitable for most deployments. However, you can customize the installation by providing your own <code>values.yaml</code> file. This allows you to override default settings such as resource allocations, environment variables, and other parameters, as well as set required parameters such as the PostgreSQL database configuration.</p> <p>To customize the installation:</p> <ul> <li>Create a <code>local-values.yaml</code> File: Define your custom configurations in this file.</li> <li>Install the Chart with Custom Values: <pre><code>helm -n skaha-system upgrade --install --values local-values.yaml posix-mapper science-platform-repo/posix-mapper\n</code></pre></li> </ul>"},{"location":"helm/science-platform/posix-mapper/#supported-configuration-options","title":"Supported Configuration Options","text":"<p>See the values.yaml file for a complete list of configuration options. Below are some of the key parameters you can configure:</p> Parameter Description Default <code>kubernetesClusterDomain</code> Kubernetes cluster domain used to find internal hosts <code>cluster.local</code> <code>replicaCount</code> Number of POSIX Mapper replicas to deploy <code>1</code> <code>tolerations</code> Array of tolerations to pass to Kubernetes for fine-grained Node targeting of the <code>posix-mapper</code> API <code>[]</code> <code>deployment.hostname</code> Hostname for the POSIX Mapper deployment <code>\"\"</code> <code>deployment.posixMapper.loggingGroups</code> List of groups permitted to adjust logging levels for the POSIX Mapper service. <code>[]</code> <code>deployment.posixMapper.image</code> POSIX Mapper Docker image <code>images.opencadc.org/platform/posix-mapper:&lt;current release version&gt;</code> <code>deployment.posixMapper.imagePullPolicy</code> Image pull policy for the POSIX Mapper container <code>IfNotPresent</code> <code>deployment.posixMapper.resourceID</code> Resource ID (URI) for this POSIX Mapper service <code>\"\"</code> <code>deployment.posixMapper.oidcURI</code> URI (or URL) for the OIDC service <code>\"\"</code> <code>deployment.posixMapper.gmsID</code> Resource ID (URI) for the IVOA Group Management Service <code>\"\"</code> <code>deployment.posixMapper.minUID</code> Minimum UID for POSIX Mapper operations.  High to avoid conflicts. <code>10000</code> <code>deployment.posixMapper.minGID</code> Minimum GID for POSIX Mapper operations.  High to avoid conflicts. <code>900000</code> <code>deployment.posixMapper.nodeAffinity</code> Kubernetes Node affinity for the POSIX Mapper API Pod <code>{}</code> <code>deployment.posixMapper.extraPorts</code> List of extra ports to expose in the POSIX Mapper service.  See the <code>values.yaml</code> file for examples. <code>[]</code> <code>deployment.posixMapper.extraVolumeMounts</code> List of extra volume mounts to be mounted in the POSIX Mapper deployment.  See the <code>values.yaml</code> file for examples. <code>[]</code> <code>deployment.posixMapper.extraVolumes</code> List of extra volumes to be mounted in the POSIX Mapper deployment.  See the <code>values.yaml</code> file for examples. <code>[]</code> <code>deployment.posixMapper.extraHosts</code> List of extra hosts to be added to the POSIX Mapper deployment.  See the <code>values.yaml</code> file for examples. <code>[]</code> <code>deployment.posixMapper.extraEnv</code> List of extra environment variables to be set in the POSIX Mapper service.  See the <code>values.yaml</code> file for examples. <code>[]</code> <code>deployment.posixMapper.resources</code> Resource requests and limits for the POSIX Mapper API <code>{}</code> <code>deployment.posixMapper.registryURL</code> (list OR string) IVOA Registry array of IVOA Registry locations or single IVOA Registry location <code>[]</code> <code>postgresql.maxActive</code> Maximum number of active connections to the PostgreSQL database <code>8</code> <code>postgresql.url</code> Required JDBC URL for the PostgreSQL database <code>\"\"</code> <code>postgresql.schema</code> Required Database schema to use for the POSIX Mapper <code>\"\"</code> <code>postgresql.auth.username</code> Username for the PostgreSQL database <code>\"\"</code> <code>postgresql.auth.password</code> Password for the PostgreSQL database <code>\"\"</code>"},{"location":"helm/science-platform/science-portal/","title":"Science Portal Helm Chart","text":""},{"location":"helm/science-platform/science-portal/#dependencies","title":"Dependencies","text":"<ul> <li>An existing Kubernetes cluster (1.29+).</li> <li>An IVOA Registry (See the Current SKAO Registry)</li> <li>A working <code>skaha</code> web service deployed and registered in the IVOA Registry.</li> </ul>"},{"location":"helm/science-platform/science-portal/#install","title":"Install","text":"<p>The Science Portal is a Single Page Application (SPA) with a rich Javascript client and DOM manager.  It uses React to power the various Dashboard elements, and is configurable for different OpenID Providers (OIdP).</p>"},{"location":"helm/science-platform/science-portal/#minimum-helm-configuration","title":"Minimum Helm configuration","text":"<p>See the full set of options in the values.yaml.  The deployed Redirect URI (<code>redirect_uri</code>) is <code>/science-portal/oidc-callback</code>, which handles receiving the <code>code</code> as part of the authorization code flow, and obtaining a token to put into a cookie.</p>"},{"location":"helm/science-platform/science-portal/#run-with-configured-values","title":"Run with configured values","text":"<pre><code>helm repo add science-platform https://images.opencadc.org/chartrepo/science-platform\nhelm repo update\n\nhelm --namespace skaha-system upgrade --install --values my-science-portal-local-values-file.yaml science-portal science-platform/scienceportal\n\nRelease \"science-portal\" has been installed. Happy Helming!\nNAME: science-portal\nLAST DEPLOYED: Thu Oct 19 11:59:15 2024\nNAMESPACE: skaha-system\nSTATUS: deployed\nREVISION: 7\nTEST SUITE: None\n</code></pre>"},{"location":"helm/science-platform/science-portal/#configuration","title":"Configuration","text":""},{"location":"helm/science-platform/science-portal/#theme","title":"Theme","text":"<p>The Science Portal supports minimal theming via configuration options.  The following options are available under the <code>deployment.sciencePortal.theme</code> object: - <code>name</code>: The name of the theme to use.  Options are <code>canfar</code> and <code>src</code>.  These simply modify the header menu options. - <code>logoURL</code>: A URL to a logo image to use in the header (far left).  This overrides the default logo for the selected theme.</p> <p>The previous <code>deployment.sciencePortal.themeName</code> is deprecated in favour of the new <code>theme</code> object.</p>"},{"location":"helm/science-platform/science-portal/#authentication-authorization","title":"Authentication &amp; Authorization","text":"<p>A&amp;A is handle by caching the Token Set server side and issuing a cookie to the browser to enable secure retrieval.  See the Application Authentication Documentation.</p>"},{"location":"helm/science-platform/science-portal/#endpoints","title":"Endpoints","text":"<p>The system will be available at the <code>/science-portal</code> endpoint, (i.e. https://example.com/science-portal).  Authenticating to the system is mandatory.</p>"},{"location":"helm/science-platform/science-portal/#values-reference","title":"Values Reference","text":""},{"location":"helm/science-platform/science-portal/#scienceportal","title":"scienceportal","text":"<p>A Helm chart to install the Science Portal UI</p> Chart AppVersion Type 1.2.0 1.2.6 application"},{"location":"helm/science-platform/science-portal/#requirements","title":"Requirements","text":"Repository Name Version file://../utils utils ^0.1.0 oci://registry-1.docker.io/bitnamicharts redis ^18.19.0"},{"location":"helm/science-platform/science-portal/#values","title":"Values","text":"Key Type Default Description deployment.hostname string <code>\"example.host.com\"</code> deployment.sciencePortal.defaultProjectName string <code>\"skaha\"</code> deployment.sciencePortal.gmsID string <code>nil</code> deployment.sciencePortal.identityManagerClass string <code>\"org.opencadc.auth.StandardIdentityManager\"</code> deployment.sciencePortal.image string <code>\"images.opencadc.org/platform/science-portal:1.2.6\"</code> deployment.sciencePortal.imagePullPolicy string <code>\"Always\"</code> deployment.sciencePortal.resources.limits.cpu string <code>\"1\"</code> deployment.sciencePortal.resources.limits.memory string <code>\"1000Mi\"</code> deployment.sciencePortal.resources.requests.cpu string <code>\"500m\"</code> deployment.sciencePortal.resources.requests.memory string <code>\"750Mi\"</code> deployment.sciencePortal.skahaResourceID string <code>nil</code> deployment.sciencePortal.tabLabels[0] string <code>\"Standard\"</code> deployment.sciencePortal.tabLabels[1] string <code>\"Advanced\"</code> deployment.sciencePortal.theme object <code>{}</code> experimentalFeatures.enabled bool <code>false</code> kubernetesClusterDomain string <code>\"cluster.local\"</code> podSecurityContext object <code>{}</code> redis.architecture string <code>\"standalone\"</code> redis.auth.enabled bool <code>false</code> redis.image.repository string <code>\"redis\"</code> redis.image.tag string <code>\"8.2.2-bookworm\"</code> redis.master.persistence.enabled bool <code>false</code> replicaCount int <code>1</code> securityContext object <code>{}</code> tolerations list <code>[]</code>"},{"location":"helm/science-platform/skaha/","title":"Skaha Helm Chart","text":"<p>The Skaha Helm chart facilitates the deployment of the Skaha application within a Kubernetes cluster. This chart is designed to streamline the installation and management of Skaha, ensuring a seamless integration into your Kubernetes environment.</p>"},{"location":"helm/science-platform/skaha/#prerequisites","title":"Prerequisites","text":"<p>Before deploying the Skaha Helm chart, ensure that the following conditions are met:</p> <ul> <li>Kubernetes Cluster: A running Kubernetes cluster, version 1.29 or higher.</li> <li>Helm: Helm package manager, version 3, installed on your machine. Refer to the official Helm documentation for installation instructions.</li> <li>Kueue: Kueue is recommended to be installed in your cluster, as Skaha optionally integrates with Kueue for job queueing. Follow the Kueue installation guide to set it up.</li> </ul>"},{"location":"helm/science-platform/skaha/#installation","title":"Installation","text":"<p>To deploy the Skaha application using the Helm chart, follow these steps:</p> <ol> <li> <p>Add the Skaha Helm Repository: <pre><code>helm repo add skaha-repo https://images.opencadc.org/chartrepo/platform\n</code></pre></p> </li> <li> <p>Update Helm Repositories: <pre><code>helm repo update\n</code></pre></p> </li> <li> <p>Install the Skaha Chart: <pre><code>helm --namespace skaha-system upgrade --install --values &lt;your-skaha-values.yaml&gt; skaha-release skaha-repo/skaha\n</code></pre> Replace <code>skaha-release</code> with your desired release name.</p> </li> </ol>"},{"location":"helm/science-platform/skaha/#configuration","title":"Configuration","text":"<p>The Skaha Helm chart comes with a default configuration suitable for most deployments. However, you can customize the installation by providing your own <code>values.yaml</code> file. This allows you to override default settings such as resource allocations, environment variables, and other parameters.</p> <p>To customize the installation:</p> <ul> <li>Create a <code>values.yaml</code> File: Define your custom configurations in this file.</li> <li>Install the Chart with Custom Values: <pre><code>helm --namespace skaha-system upgrade --install --values values.yaml skaha-release skaha-repo/skaha\n</code></pre></li> </ul>"},{"location":"helm/science-platform/skaha/#limitrange","title":"LimitRange","text":"<p>You can define a <code>LimitRange</code> for User Session Pods by modifying the <code>deployment.skaha.sessions.limitRange</code> section in your <code>values.yaml</code> file. This configuration allows you to set resource limits and requests for different session types.  The <code>min</code> clause is ignored due to hard-coded resources for Desktop and Firefly sessions.  This will go directly into a <code>LimitRange</code> object created in the Skaha workload Namespace, and, as such, supports the Kubernetes units.</p> <p>The <code>rbac</code> section allows you to create the necessary Role and RoleBinding for the LimitRange object.  If your organization does not permit the creation of RBAC objects, you can set <code>create</code> to <code>false</code> and manage the RBAC externally.</p> <pre><code>deployment:\n  skaha:\n    sessions:\n      limitRange:\n        rbac:\n          create: true\n        limitSpec:\n          max:\n            # maximum resource limit to grow to, also used by the UI to limit selectable resources\n            memory: \"96Gi\"\n            cpu: \"12\"\n          default:\n            # actually refers to default limit\n            memory: \"32Gi\"\n            cpu: \"8\"\n          defaultRequest:\n            # default resource requests\n            memory: \"4Gi\"\n            cpu: \"1\"\n</code></pre>"},{"location":"helm/science-platform/skaha/#flexible-session-pods","title":"Flexible Session Pods","text":"<p>You can customize the User Session Pods by modifying the <code>deployment.skaha.sessions</code> section in your <code>values.yaml</code> file. This includes settings for resource requests, storage allocation, and more.</p> <p>Flexible User sessions are created with a small amount of CPU and memory by default and are allowed to grow to a specified limit in the <code>LimitRange</code> configuration. You can adjust these minimum (request) settings as needed: <pre><code>deployment:\n  skaha:\n    sessions:\n      flexResourceRequests:\n        # The headless session type resource requests get slightly more resources to start.\n        headless:\n          memoryInGB: \"2\"\n          cpuCores: \"1\"\n        notebook:\n          memoryInGB: \"2\"\n          cpuCores: \"0.5\"\n</code></pre></p> <p>The <code>headless</code>, <code>notebook</code>, <code>desktop</code>, <code>contributed</code>, and <code>firefly</code> session types can all be customized individually.  Any session type not specified will use the values defined in the <code>LimitRange</code> configuration.</p> <p>Note that Kubernetes resource units are not supported in this configuration; only floating point or integer numbers as strings are valid.  For example, for 100m of CPU, use <code>\"0.1\"</code>.</p>"},{"location":"helm/science-platform/skaha/#notes-on-tolerations-and-nodeaffinity","title":"Notes on tolerations and nodeAffinity","text":"<p>Ensure that <code>tolerations</code> and <code>nodeAffinity</code> are at the expected indentation!  These are YAML configurations passed directly to Kubernetes, and the base <code>.tolerations</code> and <code>.deployment.skaha.nodeAffinity</code> values apply to the <code>skaha</code> API only, whereas the <code>.deployment.skaha.sessions.tolerations</code> and <code>.deployment.skaha.sessions.nodeAffinity</code> apply to all User Session Pods.</p>"},{"location":"helm/science-platform/skaha/#kueue","title":"Kueue","text":"<p>Skaha leverages Kueue for efficient job queueing and management when properly installed and configured in your cluster. For detailed information on Kueue's features and setup, refer to the Kueue documentation.</p>"},{"location":"helm/science-platform/skaha/#installation_1","title":"Installation","text":"<p>https://kueue.sigs.k8s.io/docs/installation/#install-a-released-version</p> <p>Will install the Kueue Chart, with a default <code>ClusterQueue</code>, and whatever defined <code>LocalQueues</code> were declared in the <code>deployment.skaha.sessions.kueue</code> section: <pre><code>deployment:\n  skaha:\n    sessions:\n      kueue:\n        notebook:\n          queueName: some-local-queue\n          priorityClass: med\n</code></pre></p> <p>To determine your cluster's allocatable resources, checkout a small Python utility (requires <code>uv</code>): https://github.com/opencadc/deployments/tree/main/configs/kueue/kueuer</p> <p>Then run: <pre><code>git clone https://github.com/opencadc/deployments.git\ncd deployments/configs/kueue/kueuer\n# if not using the default ~/.kube/config\nexport KUBECONFIG=/home/user/.kube/my-config\n\n# 60% of cluster resources\nuv run kr cluster resources -f allocatable -s 0.6\n\n# 80% of cluster resources\nuv run kr cluster resources -f allocatable -s 0.8\n</code></pre></p>"},{"location":"helm/science-platform/skaha/#uninstallation","title":"Uninstallation","text":"<p>To remove the Skaha application from your cluster:</p> <pre><code>helm --namespace skaha-system uninstall skaha-release\n</code></pre> <p>This command will delete all resources associated with the Skaha release.</p>"},{"location":"helm/science-platform/skaha/#license","title":"License","text":"<p>This project is licensed under the MIT License. For more information, refer to the LICENSE file in the repository.</p>"},{"location":"helm/science-platform/skaha/#values-reference","title":"Values Reference","text":""},{"location":"helm/science-platform/skaha/#skaha","title":"skaha","text":"<p>A Helm chart to install the Skaha web service of the CANFAR Science Platform</p> Chart AppVersion Type 1.4.0 1.2.0 application"},{"location":"helm/science-platform/skaha/#requirements","title":"Requirements","text":"Repository Name Version file://../utils utils ^0.1.0 oci://registry-1.docker.io/bitnamicharts redis ^18.19.0"},{"location":"helm/science-platform/skaha/#values","title":"Values","text":"Key Type Default Description deployment.hostname string <code>\"myhost.example.com\"</code> deployment.skaha.apiVersion string <code>\"v1\"</code> deployment.skaha.defaultQuotaGB string <code>\"10\"</code> deployment.skaha.identityManagerClass string <code>\"org.opencadc.auth.StandardIdentityManager\"</code> deployment.skaha.image string <code>\"images.opencadc.org/platform/skaha:1.1.7\"</code> deployment.skaha.imageCache.refreshSchedule string <code>\"*/30 * * * *\"</code> deployment.skaha.imagePullPolicy string <code>\"Always\"</code> deployment.skaha.init.image string <code>\"busybox:1.37.0\"</code> deployment.skaha.init.imagePullPolicy string <code>\"IfNotPresent\"</code> deployment.skaha.posixMapperCacheTTLSeconds string <code>\"86400\"</code> deployment.skaha.priorityClassName string <code>\"uber-user-preempt-high\"</code> deployment.skaha.registryHosts string <code>\"images.canfar.net\"</code> deployment.skaha.resources.limits.cpu string <code>\"2000m\"</code> deployment.skaha.resources.limits.memory string <code>\"3Gi\"</code> deployment.skaha.resources.requests.cpu string <code>\"1000m\"</code> deployment.skaha.resources.requests.memory string <code>\"2Gi\"</code> deployment.skaha.serviceAccountName string <code>\"skaha\"</code> deployment.skaha.sessions.expirySeconds string <code>\"345600\"</code> deployment.skaha.sessions.flexResourceRequests.headless.cpuCores string <code>\"1\"</code> deployment.skaha.sessions.flexResourceRequests.headless.memoryInGB string <code>\"2\"</code> deployment.skaha.sessions.imagePullPolicy string <code>\"Always\"</code> deployment.skaha.sessions.ingress.customResponseHeaders object <code>{}</code> deployment.skaha.sessions.ingress.tls object <code>{}</code> deployment.skaha.sessions.initContainerImage string <code>\"redis:8.2.2-bookworm\"</code> deployment.skaha.sessions.kueue object <code>{}</code> deployment.skaha.sessions.limitRange object <code>{}</code> deployment.skaha.sessions.maxCount string <code>\"5\"</code> deployment.skaha.sessions.maxEphemeralStorage string <code>\"200Gi\"</code> deployment.skaha.sessions.minEphemeralStorage string <code>\"20Gi\"</code> deployment.skaha.sessions.nodeLabelSelector string <code>nil</code> deployment.skaha.sessions.tolerations list <code>[]</code> deployment.skaha.sessions.userStorage.admin.auth string <code>nil</code> deployment.skaha.sessions.userStorage.homeDirectory string <code>\"home\"</code> deployment.skaha.sessions.userStorage.persistentVolumeClaimName string <code>\"skaha-workload-cavern-pvc\"</code> deployment.skaha.sessions.userStorage.projectsDirectory string <code>\"projects\"</code> deployment.skaha.sessions.userStorage.topLevelDirectory string <code>\"/cavern\"</code> experimentalFeatures.enabled bool <code>false</code> experimentalFeatures.sessionLimitRange object <code>{}</code> ingress.enabled bool <code>true</code> ingress.path string <code>\"/skaha\"</code> kubernetesClusterDomain string <code>\"cluster.local\"</code> podSecurityContext object <code>{}</code> redis.architecture string <code>\"standalone\"</code> redis.auth.enabled bool <code>false</code> redis.image.repository string <code>\"redis\"</code> redis.image.tag string <code>\"8.2.2-bookworm\"</code> redis.master.containerSecurityContext.allowPrivilegeEscalation bool <code>false</code> redis.master.containerSecurityContext.capabilities.drop[0] string <code>\"ALL\"</code> redis.master.containerSecurityContext.readOnlyRootFilesystem bool <code>true</code> redis.master.containerSecurityContext.runAsGroup int <code>1001</code> redis.master.containerSecurityContext.runAsNonRoot bool <code>true</code> redis.master.containerSecurityContext.runAsUser int <code>1001</code> redis.master.containerSecurityContext.seccompProfile.type string <code>\"RuntimeDefault\"</code> redis.master.persistence.enabled bool <code>false</code> replicaCount int <code>1</code> secrets string <code>nil</code> securityContext object <code>{}</code> service.port int <code>8080</code> skahaWorkload.namespace string <code>\"skaha-workload\"</code> tolerations list <code>[]</code>"},{"location":"helm/science-platform/storage-ui/","title":"Storage User Interface Helm Chart","text":""},{"location":"helm/science-platform/storage-ui/#dependencies","title":"Dependencies","text":"<ul> <li>An existing Kubernetes cluster (1.29+).</li> <li>An IVOA Registry (See the Current SKAO Registry)</li> <li>A working Cavern (User Storage) system</li> </ul>"},{"location":"helm/science-platform/storage-ui/#install","title":"Install","text":"<p>The Storage UI is a browser application to interface with IVOA VOSpace services, with a rich Javascript client and DOM manager.  It uses vanilla JavaScript and jQuery with DataTables to power the listing table, and is configurable for different OpenID Providers (OIdP).</p>"},{"location":"helm/science-platform/storage-ui/#minimum-helm-configuration","title":"Minimum Helm configuration","text":"<p>See the full set of options in the values.yaml.  The deployed Redirect URI (<code>redirect_uri</code>) is <code>/storage-ui/oidc-callback</code>, which handles receiving the <code>code</code> as part of the authorization code flow, and obtaining a token to put into a cookie.</p>"},{"location":"helm/science-platform/storage-ui/#run-with-configured-values","title":"Run with configured values","text":"<pre><code>helm repo add science-platform-client https://images.opencadc.org/chartrepo/client\nhelm repo update\n\nhelm install -n skaha-system --values my-storage-ui-local-values-file.yaml storage-ui science-platform-client/storageui\n\nRelease \"storage-ui\" has been installed. Happy Helming!\nNAME: storage-ui\nLAST DEPLOYED: Thu Jan 12 17:01:07 2024\nNAMESPACE: skaha-system\nSTATUS: deployed\nREVISION: 3\nTEST SUITE: None\n</code></pre>"},{"location":"helm/science-platform/storage-ui/#authentication-authorization","title":"Authentication &amp; Authorization","text":"<p>A&amp;A is handle by caching the Token Set server side and issuing a cookie to the browser to enable secure retrieval.  See the Application Authentication Documentation.</p>"},{"location":"helm/science-platform/storage-ui/#endpoints","title":"Endpoints","text":"<p>The system will be available at the <code>/storage</code> endpoint, (i.e. https://example.com/storage/list).  Authenticating to the system is optional.</p>"},{"location":"helm/science-platform/storage-ui/#configuration","title":"Configuration","text":"Parameter Description Default <code>kubernetesClusterDomain</code> DNS domain for the Kubernetes cluster. <code>cluster.local</code> <code>replicaCount</code> Number of Storage UI pod replicas. <code>1</code> <code>securityContext</code> Optional container-level security context. None (commented out) <code>applicationName</code> Application name used to rename the WAR file and endpoint. <code>storage</code> <code>deployment</code> Block containing Storage UI deployment configuration. N/A (object) <code>tolerations</code> Pod-level tolerations for the Storage UI workload. <code>[]</code> <code>secrets</code> Optional secrets (e.g., CA certificates). None (commented out) <code>redis</code> Redis configuration for token caching. N/A (object) <code>deployment.storageUI.image</code> Storage UI container image. <code>images.opencadc.org/client/storage-ui:1.4.1</code> <code>deployment.storageUI.imagePullPolicy</code> Image pull policy. <code>IfNotPresent</code> <code>deployment.storageUI.extraEnv</code> Optional extra environment variables (e.g., debug opts). None <code>deployment.storageUI.extraPorts</code> Optional additional ports exposed from the container. None <code>deployment.storageUI.resources.requests.memory</code> Memory requested by Storage UI. <code>\"500Mi\"</code> <code>deployment.storageUI.resources.requests.cpu</code> CPU requested by Storage UI. <code>\"500m\"</code> <code>deployment.storageUI.resources.limits.memory</code> Maximum memory allowed. <code>\"1Gi\"</code> <code>deployment.storageUI.resources.limits.cpu</code> Maximum CPU allowed. <code>\"750m\"</code> <code>deployment.storageUI.loggingGroups</code> Groups permitted to alter logging config. None (commented out) <code>deployment.storageUI.backend</code> Dictionary of VOSpace services described for UI usage. None (commented out) <code>deployment.storageUI.gmsID</code> Identifier (URI) for the GMS service. None <code>deployment.storageUI.oidc.uri</code> URL of the OpenID Provider (OIdP). None <code>deployment.storageUI.oidc.clientID</code> Client ID registered with the OpenID Provider. None <code>deployment.storageUI.oidc.clientSecret</code> Client Secret issued by the OIdP. None <code>deployment.storageUI.oidc.existingSecretName</code> Name of a Kubernetes secret containing the <code>clientSecret</code> key. None <code>deployment.storageUI.oidc.redirectURI</code> Redirect URI after OIDC authentication. None <code>deployment.storageUI.oidc.callbackURI</code> URI to redirect after the callback completes (usually <code>/storage/list</code>). None <code>deployment.storageUI.oidc.scope</code> OpenID scopes required for authentication. None <code>deployment.storageUI.registryURL</code> URL of IVOA Registry service. None <code>deployment.storageUI.nodeAffinity</code> Node affinity to influence scheduling. None <code>deployment.storageUI.identityManagerClass</code> Class handling authentication. <code>org.opencadc.auth.StandardIdentityManager</code> <code>deployment.storageUI.extraVolumeMounts</code> Optional extra volume mounts. None <code>deployment.storageUI.extraVolumes</code> Optional extra volumes. None <code>deployment.storageUI.themeName</code> UI theme (<code>src</code> or <code>canfar</code>). None"},{"location":"helm/science-platform/utils/","title":"Science Platform Helm Utility library (0.1.0)","text":"<p>A small Helm Library Chart to provide common utility functions to other Charts.</p>"},{"location":"helm/science-platform/utils/#install","title":"Install","text":"<p>Add to your Chart dependencies from within the <code>helm</code> folder:</p> <pre><code>  - name: \"utils\"\n    version: \"^0.1.0\"\n    repository: \"file://../utils\"\n</code></pre>"},{"location":"helm/science-platform/utils/#functions","title":"Functions","text":""},{"location":"helm/science-platform/utils/#getsecretkeyvalue","title":"getSecretKeyValue","text":"<p>The <code>getSecretKeyValue</code> function in the _get-secret-key-value.yaml file contains a function to read a Kubernetes Secret, and extract a value for the given <code>key</code>.</p>"},{"location":"helm/science-platform/utils/#example","title":"Example","text":"<pre><code>{{- $clientSecret := include \"getSecretKeyValue\" (list $existingSecretName \"clientSecret\" $namespace) -}}\n</code></pre>"},{"location":"operations/","title":"Operations","text":"<p>Operational guides and runbooks for managing CANFAR platform infrastructure, releases, and deployments.</p>"},{"location":"operations/#overview","title":"Overview","text":"<p>This section provides comprehensive documentation for platform operators managing CANFAR deployments. Whether you're releasing new versions, troubleshooting issues, or maintaining infrastructure, these guides will help you follow best practices and ensure reliable operations.</p> <ul> <li> CI/CD Pipelines GitHub Actions workflows</li> <li> Release Process Release playbook and procedures</li> </ul>"},{"location":"operations/#key-responsibilities","title":"Key Responsibilities","text":""},{"location":"operations/#release-management","title":"Release Management","text":"<ul> <li>Coordinate releases using Release Please automation</li> <li>Review and merge release PRs with proper approvals</li> <li>Monitor post-release workflows and verify deployments</li> <li>Manage hotfixes and rollback procedures when needed</li> </ul>"},{"location":"operations/#infrastructure-operations","title":"Infrastructure Operations","text":"<ul> <li>Deploy Helm charts and configuration overlays</li> <li>Manage environment-specific configurations (staging, production)</li> <li>Monitor platform health and respond to incidents</li> <li>Maintain secrets and access controls</li> </ul>"},{"location":"operations/#cicd-maintenance","title":"CI/CD Maintenance","text":"<ul> <li>Keep GitHub Actions workflows up to date</li> <li>Monitor workflow runs and troubleshoot failures</li> <li>Update documentation and configuration files</li> </ul>"},{"location":"operations/#tools-technologies","title":"Tools &amp; Technologies","text":"<p>The CANFAR deployment infrastructure relies on:</p> <ul> <li>Kubernetes - Container orchestration platform</li> <li>Helm - Package manager for Kubernetes applications</li> <li>GitHub Actions - CI/CD automation and workflows</li> <li>Release Please - Automated release management and changelog generation</li> <li>MkDocs Material - Documentation site generation</li> <li>uv - Python package and dependency management</li> </ul>"},{"location":"operations/#getting-help","title":"Getting Help","text":"<p>For operational support or questions:</p> <ul> <li>Check the relevant runbook in this documentation</li> <li>Review recent GitHub Actions workflow runs for error logs</li> <li>Contact the CADC operations team</li> <li>Consult the main CANFAR documentation</li> </ul>"},{"location":"operations/#best-practices","title":"Best Practices","text":"<ol> <li>Always follow the release checklist - Skip no steps to ensure consistent, reliable releases</li> <li>Test in staging first - Validate changes in staging before promoting to production</li> <li>Monitor post-deployment - Watch metrics and logs after every deployment</li> <li>Document incidents - Capture lessons learned and update runbooks</li> <li>Keep secrets secure - Rotate credentials regularly and limit access</li> <li>Maintain audit trails - All changes go through pull requests with proper reviews</li> </ol>"},{"location":"operations/ci-cd/","title":"CI/CD Pipelines","text":"<p>The deployments repository uses GitHub Actions to automate documentation and code quality tasks.</p>"},{"location":"operations/ci-cd/#documentation-deployment-docsyml","title":"Documentation Deployment (<code>docs.yml</code>)","text":"<p>Automatically builds and deploys the MkDocs documentation site to GitHub Pages.</p>"},{"location":"operations/ci-cd/#triggers","title":"Triggers","text":"<ul> <li>Pushes to <code>main</code> that modify <code>docs/**</code>, <code>mkdocs.yml</code>, or <code>pyproject.toml</code></li> <li>Manual dispatch with required reason field</li> </ul>"},{"location":"operations/ci-cd/#workflow-steps","title":"Workflow Steps","text":"<ol> <li>Checkout: Fetches full git history (required for git-revision-date plugin)</li> <li>Install uv: Sets up the uv package manager</li> <li>Setup Python: Installs Python using uv</li> <li>Install dependencies: Runs <code>uv sync</code> to install all dependencies from <code>pyproject.toml</code></li> <li>Deploy: Runs <code>uv run mkdocs gh-deploy --force</code> to build and publish to <code>gh-pages</code> branch</li> </ol>"},{"location":"operations/ci-cd/#requirements","title":"Requirements","text":"<ul> <li><code>contents: write</code> permission for pushing to <code>gh-pages</code> branch</li> <li>Dependencies managed in <code>pyproject.toml</code>:</li> <li><code>mkdocs-material</code> - Material theme for MkDocs</li> <li><code>mkdocs-git-revision-date-localized-plugin</code> - Git revision dates in docs</li> </ul>"},{"location":"operations/ci-cd/#pre-commit-checks-pre-commityml","title":"Pre-commit Checks (<code>pre-commit.yml</code>)","text":"<p>Runs pre-commit hooks on all files to ensure code quality and consistency.</p>"},{"location":"operations/ci-cd/#triggers_1","title":"Triggers","text":"<ul> <li>Pull requests to <code>main</code></li> <li>Manual dispatch</li> </ul>"},{"location":"operations/ci-cd/#what-it-checks","title":"What it checks","text":"<ul> <li>YAML syntax and formatting</li> <li>JSON formatting</li> <li>File permissions and naming</li> <li>Security scanning for hardcoded secrets</li> <li>Python code quality (if applicable)</li> </ul>"},{"location":"operations/release-process/","title":"Release Process","text":"<p>Documentation for releasing Helm charts and platform configuration for the CANFAR Science Platform.</p>"},{"location":"operations/release-process/#canfar-release-cycle","title":"CANFAR Release Cycle","text":"<p>The CANFAR Science Platform uses a version format <code>YYYY.Q</code> for quarterly releases:</p> <ul> <li>2025.1 - Q1 2025 release</li> <li>2025.2 - Q2 2025 release</li> </ul> <p>Between releases, hotfix patches may be released as needed for critical issues.</p>"},{"location":"operations/release-process/#helm-chart-releases","title":"Helm Chart Releases","text":"<p>Each Helm chart in this repository is versioned independently:</p> <ul> <li>Charts follow semantic versioning (<code>MAJOR.MINOR.PATCH</code>)</li> <li>Release Please automation manages changelog and version bumps</li> <li>Each chart has its own release cycle and <code>CHANGELOG.md</code></li> </ul>"},{"location":"operations/release-process/#branching-model","title":"Branching Model","text":"<ul> <li><code>main</code> is the integration branch - all work merges via pull requests</li> <li>Use conventional commits for automatic changelog generation</li> <li>Hotfixes branch from the latest release tag and merge back to <code>main</code></li> </ul>"},{"location":"operations/release-process/#release-workflow","title":"Release Workflow","text":""},{"location":"operations/release-process/#1-making-changes","title":"1. Making Changes","text":"<ul> <li>Create a pull request to <code>main</code></li> <li>Use conventional commit messages (e.g., <code>feat:</code>, <code>fix:</code>, <code>docs:</code>)</li> <li>Add appropriate labels for changelog categorization</li> <li>Ensure all CI checks pass</li> </ul>"},{"location":"operations/release-process/#2-release-please-automation","title":"2. Release Please Automation","text":"<p>Release Please automatically:</p> <ul> <li>Detects changes to Helm charts</li> <li>Determines version bump based on conventional commits</li> <li>Updates <code>CHANGELOG.md</code> and <code>Chart.yaml</code></li> <li>Creates a release PR for each affected chart</li> </ul>"},{"location":"operations/release-process/#3-review-and-merge","title":"3. Review and Merge","text":"<ul> <li>Review the generated changelog and version bump</li> <li>Verify Helm chart values and configuration</li> <li>Obtain required approvals</li> <li>Merge the release PR to create tags and GitHub releases</li> </ul>"},{"location":"operations/release-process/#4-deployment","title":"4. Deployment","text":"<p>After release PR is merged:</p> <ul> <li>Git tag is created automatically</li> <li>GitHub release is published with changelog</li> <li>Deploy to staging environment first</li> <li>Run validation and smoke tests</li> <li>Deploy to production after successful validation</li> </ul>"},{"location":"operations/release-process/#hotfix-process","title":"Hotfix Process","text":"<p>For critical issues requiring immediate patches:</p> <ol> <li>Create <code>hotfix/&lt;issue&gt;</code> branch from affected release tag</li> <li>Apply fix and create PR to <code>main</code></li> <li>Release Please generates patch release PR</li> <li>Follow standard review and merge process</li> <li>Deploy hotfix after testing</li> </ol>"},{"location":"operations/release-process/#pre-deployment-checklist","title":"Pre-deployment Checklist","text":"<p>Before deploying a Helm chart release:</p> <ul> <li>\u2705 Review CHANGELOG for all changes</li> <li>\u2705 Verify chart values match intended configuration</li> <li>\u2705 Check for breaking changes or migrations</li> <li>\u2705 Ensure dependent services are compatible</li> <li>\u2705 Prepare rollback plan</li> </ul>"},{"location":"operations/release-process/#rollback-strategy","title":"Rollback Strategy","text":"<p>If issues are detected after deployment:</p> <ol> <li>Roll back to previous Helm chart version</li> <li>Document issue and root cause</li> <li>Create hotfix branch to address problem</li> <li>Follow hotfix process for patch release</li> </ol>"}]}