---
# Resource Flavors
apiVersion: kueue.x-k8s.io/v1beta1
kind: ResourceFlavor
metadata:
  name: worker-flavor
spec:
  nodeLabels:
    skaha.opencadc.org/node-type: worker-node
---
# Resource Flavors
apiVersion: kueue.x-k8s.io/v1beta1
kind: ResourceFlavor
metadata:
  name: gpu-flavor
spec:
  nodeLabels:
    skaha.opencadc.org/node-type: gpu-node
---
# ClusterQueue
apiVersion: kueue.x-k8s.io/v1beta1
kind: ClusterQueue
metadata:
  name: skaha-cluster-queue
spec:
  namespaceSelector:
    matchExpressions:
      - key: kubernetes.io/metadata.name
        operator: In
        values: [ skaha-workload, canfar-b-workload ]
  queueingStrategy: BestEffortFIFO
  cohort: skaha-cohort
  resourceGroups:
    - coveredResources: ["cpu", "memory", "ephemeral-storage"]
      flavors:
        - name: "worker-flavor"
          # Cluster Hardware Configuration
          # Does not include
          #   - skaha.opencadc.org/node-type=service-node (p-nodes)
          #   - skaha.opencadc.org/node-type=gpu-node (p-nodes)
          #   - node-role.kubernetes.io/control-plane
          # Compute Nodes: 40 x [64 cores, 241Gi memory, 2048Gi ephemeral-storage]
          # nominalQuota = count * size - 2% overhead
          # borrowing / lending = size * 25%
          resources:
            - name: "cpu"
              nominalQuota:   "2189" # 80% of worker nodes minus 2%
              borrowingLimit: "420"
              lendingLimit:   "420"
            - name: "memory"
              nominalQuota:   "9063Gi" # 80% of worker nodes minus 2%
              borrowingLimit: "1640Gi"
              lendingLimit:   "1640Gi"
              # 40 nodes * 2048Gi = 81920Gi
            - name: "ephemeral-storage"
              nominalQuota:   "52636Gi"
              borrowingLimit: "13159Gi"
              lendingLimit:   "13159Gi"
    - coveredResources: ["cpu", "memory", "ephemeral-storage", "nvidia.com/gpu"]
      flavors:
        - name: "gpu-flavor"
          # Cluster Hardware Configuration
          # Does not include
          #   - skaha.opencadc.org/node-type=service-node (p-nodes)
          #   - skaha.opencadc.org/node-type=worker-node (p-nodes)
          #   - node-role.kubernetes.io/control-plane
          # Remaining GPU Node 01: 78 cores, 738Gi memory, 2950Gi ephemeral-storage, NVIDIA-A100-PCIE-40GB-MIG-3g.20gb]
          # Remaining GPU Node 02: 18 cores, 84Gi memory, 84Gi ephemeral-storage, Tesla-V100-PCIE-32GB]
          # nominalQuota = count * size - 2% overhead
          # borrowing / lending = size * 25%

          # Realistically the GPU 01 node will get used more, but smaller Jobs may get scheduled on 02.
          resources:
            - name: "cpu"
              nominalQuota:   "60" # 80% of GPU Node 01
            - name: "memory"
              nominalQuota:   "585Gi" # 80% of GPU Node 01
            - name: "ephemeral-storage"
              nominalQuota:   "2361Gi"
            - name: "nvidia.com/gpu"
              nominalQuota: "9"
  preemption:
    reclaimWithinCohort: LowerPriority
    borrowWithinCohort:
      policy: LowerPriority
      maxPriorityThreshold: 10000
    withinClusterQueue: LowerPriority
  stopPolicy: None
---
# WorkloadPriorityClass
apiVersion: kueue.x-k8s.io/v1beta1
kind: WorkloadPriorityClass
metadata:
  name: low
value: 10000
description: "Low Priority"
---
# WorkloadPriorityClass
apiVersion: kueue.x-k8s.io/v1beta1
kind: WorkloadPriorityClass
metadata:
  name: medium
value: 100000
description: "Medium Priority"
---
# WorkloadPriorityClass
apiVersion: kueue.x-k8s.io/v1beta1
kind: WorkloadPriorityClass
metadata:
  name: high
value: 1000000
description: "High Priority"
